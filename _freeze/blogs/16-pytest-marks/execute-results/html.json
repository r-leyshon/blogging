{
  "hash": "2018030beacc1c08f4469bbcf38daf1c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Custom Marks With Pytest in Plain English\"\nauthor: \"Rich Leyshon\"\ndate: \"July 22 2024\"\ndescription: \"Selectively running tests with marks\"\ncategories:\n    - Explanation\n    - pytest\n    - Unit tests\n    - marks\n    - custom marks\n    - markers\n    - pytest-in-plain-english\nimage: \"https://i.imgur.com/dCJBh9w.jpeg\"\nimage-alt: \"Planet composed of croissants\"\ntoc: true\ntoc-depth: 4\njupyter: \n  kernelspec:\n    name: \"mocking-env\"\n    language: \"python\"\n    display_name: \"blog-mocking-env\"\ncss: /www/13-pytest-parametrize/styles.css\n---\n\n\n\n\n<figure class=center >\n  <img class=\"shaded_box\" width=400px src=\"https://i.imgur.com/dCJBh9w.jpeg\"></img>\n  <figcaption>Planet composed of croissants.</figcaption>\n</figure>\n\n> \"The only flake I want is that of a croissant.\" Mariel Feldman.\n\n## Introduction\n\n`pytest` is a testing package for the python framework. It is broadly used to\nquality assure code logic. This article discusses custom marks, use cases and\npatterns for selecting and deselecting marked tests. This blog is the fifth and\nfinal in a series of blogs called\n[pytest in plain English](/blogs/index.qmd#category=pytest-in-plain-english),\nfavouring accessible language and simple examples to explain the more intricate\nfeatures of the `pytest` package.\n\nFor a wealth of documentation, guides and how-tos, please consult the\n<a href=\"https://docs.pytest.org/en/8.0.x/\" target=\"_blank\">`pytest` documentation</a>.\n\n### What are `pytest` Custom Marks?\n\nMarks are a way to conditionally run specific tests. There are a few marks that\ncome with the `pytest` package. To view these, run `pytest --markers` from the\ncommand line. This will print a list of the pre-registered marks available\nwithin the `pytest` package. However, it is extremely easy to register your own\nmarkers, allowing greater control over which tests get executed.\n\nThis article will cover:\n\n* Reasons for marking tests\n* Registering marks with `pytest`\n* Marking tests\n* Including or excluding markers from the command line\n\n:::{.callout collapse=\"true\"}\n\n### A Note on the Purpose (Click to expand)\n\nThis article intends to discuss clearly. It doesn't aim to be clever or\nimpressive. Its aim is to extend understanding without overwhelming the reader.\nThe code may not always be optimal, favouring a simplistic approach wherever\npossible.\n\n:::\n\n### Intended Audience\n\nProgrammers with a working knowledge of python and some familiarity with\n`pytest` and packaging. The type of programmer who has wondered about how to\nfollow best practice in testing python code.\n\n### What You'll Need:\n\n- [ ] Preferred python environment manager (eg `conda`)\n- [ ] `pip install pytest==8.1.1`\n- [ ] Git\n- [ ] GitHub account\n- [ ] Command line access\n\n### Preparation\n\nThis blog is accompanied by code in\n[this repository](https://github.com/r-leyshon/pytest-fiddly-examples). The\nmain branch provides a template with the minimum structure and requirements\nexpected to run a `pytest` suite. The repo branches contain the code used in\nthe examples of the following sections.\n\nFeel free to fork or clone the repo and checkout to the example branches as\nneeded.\n\nThe example code that accompanies this article is available in the\n[marks branch](https://github.com/r-leyshon/pytest-fiddly-examples/tree/marks)\nof the repo.\n\n## Overview\n\nOccasionally, we write tests that are a bit distinct to the rest of our test\nsuite. They could be integration tests, calling on elements of our code from\nmultiple modules. They could be end to end tests, executing a pipeline from\nstart to finish. Or they could be a flaky or brittle sort of test, a test that\nis prone to failure on specific operating systems, architectures or when\nexternal dependencies do not provide reliable inputs. \n\nThere are multiple ways to handle these kinds of tests, including mocking, as\ndiscussed in [my previous blog](/blogs/15-pytest-mocking.qmd). Mocking can\noften take a bit of time, and developers don't always have that precious\ncommodity. So instead, they may mark the test, ensuring that it doesn't get run\non continuous integration (CI) checks. This may involve flagging any flaky test\nas \"technical debt\" to be investigated and fixed later.\n\nIn fact, there are a number of reasons that we may want to selectively run\nelements of a test suite. Here is a selection of scenarios that could benefit\nfrom marking.\n\n:::{.callout-note collapse=\"true\"}\n\n### Flaky Tests: Common Causes\n\n| **Category**                | **Cause**                   | **Explanation**                                                         |\n|-----------------------------|-----------------------------|-------------------------------------------------------------------------|\n| External Dependencies       | Network                     | Network latency, outages, or Domain Name System (DNS) issues.           |\n|                             | Web APIs                    | Breaking changes, rate limits, or outages.                              |\n|                             | Databases                   | Concurrency issues, data changes, or connection problems.               |\n|                             | Timeouts                    | Hardcoded or too-short timeouts cause failures.                         |\n| Environment Dependencies    | Environment Variables       | Incorrectly set environment variables.                                  |\n|                             | File System                 | File locks, permissions, or missing files.                              |\n|                             | Resource Limits             | Insufficient CPU, memory, or disk space.                                |\n| State Dependencies          | Shared State                | Interference between tests sharing state.                               |\n|                             | Order Dependency            | Tests relying on execution order.                                       |\n| Test Data                   | Random Data                 | Different results on each run due to random data and seed not set.      |\n| Concurrency Issues          | Parallel Execution          | Tests not designed for parallel execution.                              |\n|                             | Locks                       | Deadlocks or timeouts involving locks or semaphores.                    |\n|                             | Race Conditions             | Tests depend on the order of execution of threads or processes.         |\n|                             | Async Operations            | Improperly awaited asynchronous code.                                   |\n| Hardware and System Issues  | Differences in Hardware     | Variations in performance across hardware or operating systems.         |\n|                             | System Load                 | Failures under high system load due to resource contention.             |\n| Non-deterministic Inputs    | Time                        | Variations in current time affecting test results.                      |\n|                             | User Input                  | Non-deterministic user input causing flaky behaviour.                   |\n|                             | Filepaths                   | CI runner filepaths may be hard to predict.                             |\n| Test Implementation Issues  | Assertions                  | Incorrect or overly strict assertions.                                  |\n|                             | Setup and Teardown          | Inconsistent state due to improper setup or teardown.                   |\n: {.light .hover .responsive-md}\n\n:::\n\nIn the case of integration tests, one approach may be to group them all\ntogether and have them execute within a dedicated CI workflow. This is common\npractice as developers may want to stay alert to problems with external\nresources that their code depends upon, while not failing 'core' CI checks\nabout changes to the source code. If your code relies on a web API; for\ninstance; you're probably less concerned about temporary outages in that\nservice. However, a breaking change to that service would require our source\ncode to be adapted. Once more, life is a compromise.\n\n> \"*Le mieux est l'ennemi du bien.*\" (The best is the enemy of the good),\nVoltaire\n\n## Custom Marks in `pytest`\n\nMarking allows us to have greater control over which of our tests are executed\nwhen we invoke `pytest`.  Marking is conveniently implemented in the following\nway (presuming you have already written your source and test code):\n\n1. Register a custom marker\n2. Assign the new marker name to the target test\n3. Invoke `pytest` with the `-m` (MARKEXPR) flag.\n\nThis section uses code available in the\n[marks branch](https://github.com/r-leyshon/pytest-fiddly-examples/tree/marks)\nof the GitHub repository.\n\n### Define the Source Code\n\nI have a motley crew of functions to consider. A sort of homage to Sergio\nLeone's 'The Good, The Bad & the Ugly', although I'll let you figure out which\nis which.\n\n<figure class=center >\n  <img class=\"shaded_box\" width=400px src=\"https://i.imgur.com/uO3DaTH.jpeg\"></img>\n  <figcaption>The Flaky, The Slow & The Needy</figcaption>\n</figure>\n\n\n#### The Flaky Function\n\nHere we define a function that will fail half the time. What a terrible test to\nhave. The root of this unpredictable behaviour should be diagnosed as a\npriority as a matter of sanity.\n\n::: {#aad116a8 .cell execution_count=1}\n``` {.python .cell-code}\nimport random\n\n\ndef croissant():\n    \"\"\"A very flaky function.\"\"\"\n    if round(random.uniform(0, 1)) == 1:\n        return True\n    else:\n        raise Exception(\"Flaky test detected!\")\n```\n:::\n\n\n#### The Slow Function\n\nThis function is going to be pretty slow. Slow test suites throttle our\nproductivity. Once it finishes waiting for a specified number of seconds, it\nwill return a string.\n\n::: {#53b9abe7 .cell execution_count=2}\n``` {.python .cell-code}\nimport time\nfrom typing import Union\n\n\ndef take_a_nap(how_many_seconds:Union[int, float]) -> str:\n    \"\"\"Mimic a costly function by just doing nothing for a specified time.\"\"\"\n    time.sleep(float(how_many_seconds))\n    return \"Rise and shine!\"\n```\n:::\n\n\n#### The Needy Function\n\nFinally, the needy function will have an external dependency on a website. This\ntest will simply check whether we get a HTTP status code of 200 (ok) when we\nrequest any URL. \n\n::: {#19317764 .cell execution_count=3}\n``` {.python .cell-code}\nimport requests\n\n\ndef check_site_available(url:str, timeout:int=5) -> bool:\n    \"\"\"Checks if a site is available.\"\"\"\n    try:\n        response = requests.get(url, timeout=timeout)\n        return True if response.status_code == 200 else False\n    except requests.RequestException:\n        return False\n```\n:::\n\n\n#### The Wrapper\n\nFinally, I'll introduce a wrapper that will act as an opportunity for an\nintegration test. This is a bit awkward, as none of the above functions are\nparticularly related to each other. \n\nThis function will execute the `check_site_available()` and `take_a_nap()`\ntogether. A pretty goofy example, I admit. Based on the status of the url\nrequest, a string will be returned.\n\n::: {#fc700305 .cell execution_count=4}\n``` {.python .cell-code}\nimport time\nfrom typing import Union\n\nimport requests\n\ndef goofy_wrapper(url:str, timeout:int=5) -> str:\n    \"\"\"Check a site is available, pause for no good reason before summarising\n    outcome with a string.\"\"\"\n    msg = f\"Napping for {timeout} seconds.\\n\"\n    msg = msg + take_a_nap(timeout)\n    if check_site_available(url):\n        msg = msg + \"\\nYour site is up!\"\n    else:\n        msg = msg + \"\\nYour site is down!\"\n\n    return msg\n```\n:::\n\n\n### Let's Get Testing\n\nInitially, I will define a test that does nothing other than pass. This will\nbe a placeholder, unmarked test. \n\n::: {#9f168d55 .cell execution_count=5}\n``` {.python .cell-code}\ndef test_nothing():\n    pass\n```\n:::\n\n\nNext, I import `croissant()` and assert that it returns `True`. As you may\nrecall from above, `croissant()` will do so ~50 % of the time.\n\n::: {#a3818387 .cell execution_count=6}\n``` {.python .cell-code}\nfrom example_pkg.do_something import (\n    croissant,\n    )\n\n\ndef test_nothing():\n    pass\n\n\ndef test_croissant():\n    assert croissant()\n```\n:::\n\n\nNow running `pytest -v` will print the test results, reporting test outcomes\nfor each test separately (`-v` means verbose).\n\n```{.abc}\n...% pytest -v\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 2 items                                                             \ntests/test_do_something.py::test_nothing PASSED                          [ 50%]\ntests/test_do_something.py::test_croissant PASSED                        [100%]\n============================== 2 passed in 0.05s ==============================\n\n```\n\nBut note that half the time, I will also get the following output:\n\n```{.abc}\n...% pytest -v\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 2 items                         \n\ntests/test_do_something.py::test_nothing PASSED                          [ 50%]\ntests/test_do_something.py::test_croissant FAILED                        [100%]\n\n================================== FAILURES ==================================\n_______________________________ test_croissant ________________________________\n    @pytest.mark.flaky\n    def test_croissant():\n>       assert croissant()\ntests/test_do_something.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n    def croissant():\n        \"\"\"A very flaky function.\"\"\"\n        if round(random.uniform(0, 1)) == 1:\n            return True\n        else:\n>           raise Exception(\"Flaky test detected!\")\nE           Exception: Flaky test detected!\n\nsrc/example_pkg/do_something.py:13: Exception\n============================short test summary info ===========================\nFAILED ...::test_croissant - Exception: Flaky test detected!\n========================= 1 failed, 1 passed in 0.07s =========================\n```\n\nTo prevent this flaky test from failing the test suite, we can choose to mark\nit as flaky, and optionally skip it when invoking `pytest`. To go about that,\nwe first need to register a new marker. To do that, let's update out project's\n`pyproject.toml` to include additional options for a `flaky` mark:\n\n```{.abc}\n# `pytest` configurations\n[tool.pytest.ini_options]\nmarkers = [\n    \"flaky: tests that can randomly fail through no change to the code\",\n]\n```\n\nNote that when registering a marker in this way, text after the colon is an\noptional mark description. Saving the document and running `pytest --markers`\nshould show that a new custom marker is available to our project:\n\n```{.abc}\n... % pytest --markers\n@pytest.mark.flaky: tests that can randomly fail through no change to the code\n...\n\n```\nNow that we have confirmed our marker is available for use, we can use it to\nmark `test_croissant()` as flaky:\n\n::: {#1382df3b .cell execution_count=7}\n``` {.python .cell-code}\nimport pytest\n\nfrom example_pkg.do_something import (\n    croissant,\n    )\n\n\ndef test_nothing():\n    pass\n\n\n@pytest.mark.flaky\ndef test_croissant():\n    assert croissant()\n```\n:::\n\n\nNote that we need to import `pytest` to our test module in order to use the\n`pytest.mark.<MARK_NAME>` decorator. \n\n#### Selecting a Single Mark\n\nNow that we have registered and marked a test as `flaky`, we can adapt our\n`pytest` call to execute tests with that mark only. The pattern we will use is:\n\n> `pytest -v -m \"<INSERT_MARK_NAME>\"`\n\n```{.abc}\n... % pytest -v -m \"flaky\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 2 items / 1 deselected / 1 selected                                 \n\ntests/test_do_something.py::test_croissant PASSED                        [100%]\n\n======================= 1 passed, 1 deselected in 0.05s =======================\n\n```\n\nNow we see that `test_croissant()` was executed, while the unmarked\n`test_nothing()` was not. \n\n#### Deselecting a Single Mark\n\nMore useful than selectively running a flaky test is to deselect it. In this\nway, it cannot fail our test suite. This is achieved with the following\npattern:\n\n> `pytest -v -m \"not <INSERT_MARK_NAME>\"`\n\n\n```{.abc}\n... % pytest -v -m \"not flaky\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 2 items / 1 deselected / 1 selected                                  \n\ntests/test_do_something.py::test_nothing PASSED                          [100%]\n\n======================= 1 passed, 1 deselected in 0.05s =======================\n\n```\n\nNote that this time, `test_flaky()` was not executed. \n\n#### Selecting Multiple Marks\n\nIn this section, we will introduce another, differently marked test to\nillustrate the syntax for running multiple marks. For this example, we'll test\n`take_a_nap()`:\n\n::: {#6cdcc693 .cell execution_count=8}\n``` {.python .cell-code}\nimport pytest\n\nfrom example_pkg.do_something import (\n    croissant,\n    take_a_nap,\n    )\n\n\ndef test_nothing():\n    pass\n\n\n@pytest.mark.flaky\ndef test_croissant():\n    assert croissant()\n\n\n@pytest.mark.slow\ndef test_take_a_nap():\n    out = take_a_nap(how_many_seconds=3)\n    assert isinstance(out, str), f\"a string was not returned: {type(out)}\"\n    assert out == \"Rise and shine!\", f\"unexpected string pattern: {out}\"\n```\n:::\n\n\nOur new test just makes some simple assertions about the string `take_a_nap()`\nreturns after snoozing. But notice what happens when running `pytest -v` now:\n\n```{.abc}\n... % pytest -v\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 3 items                                                             \n\ntests/test_do_something.py::test_nothing PASSED                          [ 33%]\ntests/test_do_something.py::test_croissant PASSED                        [ 66%]\ntests/test_do_something.py::test_take_a_nap PASSED                       [100%]\n\n============================== 3 passed in 3.07s ==============================\n\n``` \n\nThe test suite now takes in excess of 3 seconds to execute, as the test\nspecified for `take_a_nap()` to sleep for that period. Let's update our\n`pyproject.toml` and register a new mark:\n\n```{.abc}\n# `pytest` configurations\n[tool.pytest.ini_options]\nmarkers = [\n    \"flaky: tests that can randomly fail through no change to the code\",\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n]\n```\n\nNote that the nested speech marks within the description of the `slow` mark\nwere escaped. `pytest` would have complained that the toml file was not valid\nunless we ensured it was valid [toml syntax](https://toml.io/en/).\n\nIn order to run tests marked with either `flaky` or `slow`, we can use `or`:\n\n> `pytest -v -m \"<INSERT_MARK_1> or <INSERT_MARK_2>\"`\n\n```{.abc}\n... % pytest -v -m \"flaky or slow\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 3 items / 1 deselected / 2 selected                                 \n\ntests/test_do_something.py::test_croissant PASSED                        [ 50%]\ntests/test_do_something.py::test_take_a_nap PASSED                       [100%]\n\n======================= 2 passed, 1 deselected in 3.06s =======================\n\n```\n\nNote that anything not marked with `flaky` or `slow` (eg `test_nothing()`) was\nnot run. Also, `test_croissant()` failed 3 times in a row while I tried to get\na passing run. I didn't want the flaky exception to carry on presenting itself.\nWhile I may be sprinkling glitter, I do not want to misrepresent how\nfrustrating flaky tests can be!\n\n<div class=\"tenor-gif-embed center\" data-postid=\"24035134\" data-share-method=\"host\" data-aspect-ratio=\"1.22605\" data-width=\"50%\"><a href=\"https://tenor.com/view/glitter-gif-24035134\">Glitter GIF</a>from <a href=\"https://tenor.com/search/glitter-gifs\">Glitter GIFs</a></div> <script type=\"text/javascript\" async src=\"https://tenor.com/embed.js\"></script>\n\n#### Complex Selection Rules\n\nBy adding an additional mark, we can illustrate more complex selection and\ndeselection rules for invoking `pytest`. Let's write an integration test that\nchecks whether the domain for this blog site can be reached.\n\n::: {#2ea4995a .cell execution_count=9}\n``` {.python .cell-code}\nimport pytest\n\nfrom example_pkg.do_something import (\n    croissant,\n    take_a_nap,\n    check_site_available,\n    )\n\n\ndef test_nothing():\n    pass\n\n\n@pytest.mark.flaky\ndef test_croissant():\n    assert croissant()\n\n\n@pytest.mark.slow\ndef test_take_a_nap():\n    out = take_a_nap(how_many_seconds=3)\n    assert isinstance(out, str), f\"a string was not returned: {type(out)}\"\n    assert out == \"Rise and shine!\", f\"unexpected string pattern: {out}\"\n\n\n@pytest.mark.integration\ndef test_check_site_available():\n    url = \"https://thedatasavvycorner.com/\"\n    assert check_site_available(url), f\"site {url} is down...\"\n```\n:::\n\n\nNow updating our `pyproject.toml` like so:\n\n```{.abc}\n# `pytest` configurations\n[tool.pytest.ini_options]\nmarkers = [\n    \"flaky: tests that can randomly fail through no change to the code\",\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: tests that require external resources\",\n]\n```\n\nNow we can combine `and` and `not` statements when calling `pytest` to execute\njust the tests we need to. In the below, I choose to run the `slow` and\n`integration` tests while excluding that pesky `flaky` test.\n\n```{.abc}\n... % pytest -v -m \"slow or integration and not flaky\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 4 items / 2 deselected / 2 selected                                 \n\ntests/test_do_something.py::test_take_a_nap PASSED                       [ 50%]\ntests/test_do_something.py::test_check_site_available PASSED             [100%]\n\n======================= 2 passed, 2 deselected in 3.29s =======================\n```\n\nNote that both `test_nothing()` (unmarked) and `test_croissant()` (deselected)\nwere not run.\n\n#### Marks and Test Classes\n\nNote that so far, we have applied marks to test functions only. But we can also\napply marks to an entire test class, or even target specific test modules. For\nthis section, I will introduce the wrapper function introduced earlier and use\na test class to group its tests together. I will mark those tests with 2 new\nmarks, `classy` and `subclassy`.\n\n```{.abc}\n# `pytest` configurations\n[tool.pytest.ini_options]\nmarkers = [\n    \"flaky: tests that can randomly fail through no change to the code\",\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: tests that require external resources\",\n    \"classy: tests arranged in a class\",\n    \"subclassy: test methods\",\n]\n```\n\nUpdating our test module to include `test_goofy_wrapper()`:\n\n::: {#68e07188 .cell execution_count=10}\n``` {.python .cell-code}\nimport pytest\n\nfrom example_pkg.do_something import (\n    croissant,\n    take_a_nap,\n    check_site_available,\n    goofy_wrapper\n    )\n\n\ndef test_nothing():\n    pass\n\n\n@pytest.mark.flaky\ndef test_croissant():\n    assert croissant()\n\n\n@pytest.mark.slow\ndef test_take_a_nap():\n    out = take_a_nap(how_many_seconds=3)\n    assert isinstance(out, str), f\"a string was not returned: {type(out)}\"\n    assert out == \"Rise and shine!\", f\"unexpected string pattern: {out}\"\n\n\n@pytest.mark.integration\ndef test_check_site_available():\n    url = \"https://thedatasavvycorner.com/\"\n    assert check_site_available(url), f\"site {url} is down...\"\n\n\n@pytest.mark.classy\nclass TestGoofyWrapper:\n    @pytest.mark.subclassy\n    def test_goofy_wrapper_url_exists(self):\n        assert goofy_wrapper(\n            \"https://thedatasavvycorner.com/\", 1\n            ).endswith(\"Your site is up!\"), \"The site wasn't up.\"\n    @pytest.mark.subclassy\n    def test_goofy_wrapper_url_does_not_exist(self):\n        assert goofy_wrapper(\n            \"https://thegoofycorner.com/\", 1\n            ).endswith(\"Your site is down!\"), \"The site wasn't down.\"\n```\n:::\n\n\nNote that targeting either the `classy` or `subclassy` mark results in the\nsame output - all tests within this test class are executed:\n\n```{.abc}\n... % pytest -v -m \"classy\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 6 items / 4 deselected / 2 selected                                 \n\nTestGoofyWrapper::test_goofy_wrapper_url_exists PASSED                   [ 50%]\nTestGoofyWrapper::test_goofy_wrapper_url_does_not_exist PASSED           [100%]\n\n======================= 2 passed, 4 deselected in 2.30s =======================\n\n```\n\nNobody created the domain `https://thegoofycorner.com/` yet, such a shame.\n\n#### Tests with Multiple Marks\n\nNote that we can use multiple marks with any test or test class. Let's update\n`TestGoofyWrapper` to be marked as `integration` & `slow`:\n\n::: {#bd3fa839 .cell execution_count=11}\n``` {.python .cell-code}\n@pytest.mark.slow\n@pytest.mark.integration\n@pytest.mark.classy\nclass TestGoofyWrapper:\n    @pytest.mark.subclassy\n    def test_goofy_wrapper_url_exists(self):\n        assert goofy_wrapper(\n            \"https://thedatasavvycorner.com/\", 1\n            ).endswith(\"Your site is up!\"),\"The site wasn't up.\"\n    @pytest.mark.subclassy\n    def test_goofy_wrapper_url_does_not_exist(self):\n        assert goofy_wrapper(\n            \"https://thegoofycorner.com/\", 1\n            ).endswith(\"Your site is down!\"), \"The site wasn't down.\"\n```\n:::\n\n\nThis test class can now be exclusively targeted by specifying multiple marks\nwith `and`:\n\n>  `pytest -v -m \"<INSERT_MARK_1> and <INSERT_MARK2>... and <INSERT_MARK_N>\"`\n\n```{.abc}\n\n... % pytest -v -m \"integration and slow\"   \n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- /...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 6 items / 4 deselected / 2 selected                                 \n\nTestGoofyWrapper::test_goofy_wrapper_url_exists PASSED                   [ 50%]\nTestGoofyWrapper::test_goofy_wrapper_url_does_not_exist PASSED           [100%]\n\n======================= 2 passed, 4 deselected in 2.30s =======================\n\n```\n\nNote that even though there are other tests marked with `integration` and\n`slow` separately, they are excluded on the basis that `and` expects them to be\nmarked with both.\n\n#### Deselecting All Marks\n\nNow that we have introduced multiple custom markers to our test suite, what if\nwe want to exclude all of these marked tests, just running the 'core' test\nsuite? Unfortunately, there is not a way to specify 'unmarked' tests. There is\nan old `pytest` plugin called\n[`pytest-unmarked`](https://pypi.org/project/pytest-unmarked/) that allowed\nthis functionality. Unfortunately, this plugin is not being actively maintained\nand is not compatible with `pytest` v8.0.0+. You could introduce a 'standard'\nor 'core' marker, but you'd need to remember to mark every unmarked test within\nyour test suite with it.\n\nAlternatively, what we can do is exclude each of the marks that have been\nregistered. There are 2 patterns for achieving this:\n\n> 1. `pytest -v -m \"not <INSERT_MARK_1> ... or not <INSERT_MARK_N>\"`\n> 2. `pytest -v -m \"not (<INSERT_MARK_1> ... or <INSERT_MARK_N>)\"`\n\n```{.abc}\n... % pytest -v -m \"not (flaky or slow or integration)\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 -- ...\ncachedir: .pytest_cache\nrootdir: /...\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 6 items / 5 deselected / 1 selected                                 \n\ntests/test_do_something.py::test_nothing PASSED                          [100%]\n\n======================= 1 passed, 5 deselected in 0.05s =======================\n\n```\nNote that using `or` has greedily excluded any test marked with at least one of\nthe specified marks. \n\n## Summary\n\nRegistering marks with `pytest` is very easy and is useful for controlling\nwhich tests are executed. We have illustrated:\n\n* registering marks\n* marking tests and test classes\n* the use of the `pytest -m` flag\n* selection of multiple marks\n* deselection of multiple marks\n\nOverall, this feature of `pytest` is simple and intuitive. There are more\noptions for marking tests. I recommend reading the\n[`pytest` custom markers](https://docs.pytest.org/en/7.1.x/example/markers.html)\nexamples for more information.\n\nAs mentioned earlier, this is the final in the\n[pytest in plain English](/blogs/index.qmd#category=pytest-in-plain-english)\nseries. I will be taking a break from blogging about testing for a while. But\ncolleagues have asked about articles on property-based testing and some of the\nmore useful `pytest` plug-ins. I plan to cover these topics at a later date.\n\nIf you spot an error with this article, or have a suggested improvement then\nfeel free to\n[raise an issue on GitHub](https://github.com/r-leyshon/blogging/issues).  \n\nHappy testing!\n\n## Acknowledgements\n\nTo past and present colleagues who have helped to discuss pros and cons,\nestablishing practice and firming-up some opinions. Particularly:\n\n* Ethan\n* Sergio\n\n<p id=fin><i>fin!</i></p>\n\n",
    "supporting": [
      "16-pytest-marks_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}