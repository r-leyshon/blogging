{
  "hash": "9657fa177dafe6f3c68f8500b225a16c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Dark Data\nsubtitle: Why what you don't know matters\nauthor: Rich Leyshon\ndate: July 01 2024\ndescription: A perfect compendium of gotchas for the entry data scientist.\ncategories:\n  - Non-fiction\n  - Data\n  - Science\n  - Mathematics\n  - Statistics\nimage: 'https://i.imgur.com/qVgnzFV.jpeg'\nimage-alt: An image of a black hole within a field of binary digits.\ntoc: true\ntoc-depth: 4\n---\n\n<figure class=center>\n <img class=shaded_box src=https://i.imgur.com/qVgnzFV.jpeg alt=\"An image of a black hole within a field of binary digits.\" width=400px>\n</figure>\n\n## Book Summary\n\n\"Dark Data: Why What You Don't Know Matters\" by David J. Hand delves into the\ncrucial concept of dark data â€” information that is unseen, uncollected, or\nunanalysed but significantly impacts decision-making and understanding. Hand\ncategorises dark data into 15 types, including self-selection bias and\ntime-dependent missing data, illustrating the dangers of ignoring such data\nthrough historical and contemporary examples like FDR's reelection polls and\nthe Challenger shuttle disaster. The book highlights how unrecognized dark data\ncan lead to skewed understandings, incorrect conclusions, and flawed actions.\n\n> \"The first step must always be to be aware there might be dark data. Indeed,\nyour default assumption should be that the data are incomplete or inaccurate.\n*That is the most important message of this book: be suspicious about the data*\n- at least until it is proved they are adequate and accurate.\"\n[@darkdata, p.293]\n\nHand emphasizes the importance of recognising and mitigating dark data,\nteaching readers to be vigilant about the issues posed by unknown information.\nHe also explores how dark data can be strategically utilised. The book\naddresses various statistical methods and concepts, underscoring that even in\nthe age of big data, the data available is never complete. Through practical\nguidance, Hand aims to help readers make better decisions in a world where\nmissing data is inevitable, stressing the significance of understanding what is\nnot known.\n\n## On the Author\n\nDavid J. Hand is a distinguished British statistician born in Peterborough,\nEngland. He received his BA from the University of Oxford and his PhD from the\nUniversity of Southampton. Hand's academic career includes significant roles\nsuch as serving as a professor of statistics at the Open University from 1988\nto 1999 and later becoming an Emeritus Professor of Mathematics at Imperial\nCollege London, where he also worked as a Senior Research Investigator. His\nresearch interests are broad and include multivariate statistics,\nclassification methods, pattern recognition, computational statistics, and the\nfoundations of statistics, with a keen focus on data mining, data science, and\nbig data.\n\n## 3 Takeaways\n\n### On Human Bias\n\n> \"...At least until the scientific revolution... advances in understanding\nwere retarded by a (typically subconscious) reluctance to collect data which\nmight discprove a theory... advances were held back by an unwillingness to make\ndark data visible...\n\nMy favourite historical example of someone who spotted this problem is given by\nthe seventeenth-century philosopher, Francis Bacon, who wrote: \"Human\nunderstanding when it has once adpopted an opinion... draws all things else to\nsupport and agree with it. And though there be a great number and weight of\ninstances to be found on the other side, yet these it either neglects and\ndespises, or else by some distinction sets aside and rejects.\"\n\" [@darkdata, pp. 167-168]\n\nThis human bias may be one of the fundamental flaws in human psychology. It's\nsomething that people consider exists in everyone apart from themselves! My\nfavourite (disputed) quote which has become a widely used example of\nconfirmation bias has been attributed to the American film critic Pauline Kael,\non the topic of Nixon's 1972 U.S. election victory:\n\n> \"I can't believe Nixon won. I don't know anyone who voted for him.\"\n\nI have known highly intelligent, capable people who'se default position on\nalternative positions is not just one of scepticism, but one of objection. And\ndoubtlessly there have also been times when I have fallen into this trap\nmyself. The issue with discounting, excluding and mocking counter opinions is\nthat; on occasion; they are falsifiably correct! Widely accepted tautology can,\nhas been and will be shattered by those willing to think outside of broadly\naccepted opinion. \n\nMy favourite 2 examples of this:\n\n1. A young James Dyson's rejection when pitching his innovative bagless vacuum\ntechnology to Hoover @dyson-vindication. Hoover did not see the potential in\nDyson's early prototype and disliked the fact that they would not be able to\nmarket their profitable vacuum bags and filters with such a model.\n2. Did you know that the concept of continental drift was highly disputed by \ngeologists until the mid twentieth century? Bill Bryson @short-history\ndescribes this as dogmatic resistance of the scientific community to truth. It\nhappened that Alfred Wegener, who proposed the idea in World War 2 was not a\ntrained geologist. Even in the mid 1950s, the concept had not been adopted and\nnotable Harvard University professor Charles Hapgood @deny-drift was vehemently\nopposed to the concept.\n\n### Publication Bias\n\nHand rightfully has quite a bit to say about the reproducibility crisis. The\nauthor explores reproducibility, publication bias and fraud in the scientific\nliterature. Although these issues may or may not be intentional, they have the\npotential to frustrate progress and prolong suffering, as in the case of the\nrecent retraction @nature-retract of an influential study on Alzheimer's\ndisease published in Nature in 2006 @nature-amyloid. This paper has been\ncited more than 2,000 times and used brain scans of patients that have been\nshown to have been falsified.\n\nHand goes on to talk about the lesser known practice in academia known as\nharking:\n\n> \"Yet another cause of mistaken results is the pernicious practice called\n\"HARKing\", or *H*ypothesizing *A*fter the *R*esult is known...\n\n> HARKing can be alleviated by requiring researchers to state their hypotheses\nbefore they collect any data. Some scientific journals are considering moving\nin this direction, guaranteeing publication of a paper regardless of how the \nresults come out...\" [@darkdata, pp.198-199]\n\nI thoroughly agree with David Hand. Wikipedia @wiki-pub-bias quotes a study\nstating that papers with significant findings are 3x more likely to be\npublished than those with null findings. And as the number of publications is\nbroadly interpreted as a measure of success for academic institutions and \nindividuals alike, a systemic pressure for fraudulent practice emerges. I would\ngo even further and suggest that the barriers to journals demanding full\nreproducibility of analysis - including open and versioned code and data - is\nnow lower than ever. In 2024, every respectable journal can require its\ncontributors to use open source tools to increase transparency and prove\nreproducibility. So why has this practice not been established? I presume there\nto be inertia in adopting newer tooling. A legacy uplift issue that would\nhonestly present reasonable overhead for a significant gain. After all, most \npeople who undertake their statistical analysis in legacy or proprietary\nsoftware have a thorough knowledge framework that would greatly assist them in\ntransitioning to open source languages.\n\n### Data Missingness\n\nHand adapts the American Statistician Donald Rubin's description of the causes\nof missing data into the following:\n\n* Unseen Data Dependent (UDD): The cause of the missingness is related to the\nvalue of the datum. No reason for this missingness can be discerned from the\nextant data.\n* Seen Data Dependent (SDD): A clue to the reason for the missing datum is\navailable within the extant data. \n* Not Data Dependent (NDD): The reason for the missingess is unrelated to the\nextant data.\n\nHand's explanation of Complete Case Analysis is an excellent warning to all\nthose new to data science. The very common practice of dropping or imputing\nmissing values from the data can be a very risky business. What if there were\nan underlying pattern in the missingness? Hand employs a perfectly clear,\nsimple tabular example that have no complete cases, meaning that dropping\nmissing observations would result in no records. \n\n> \"...if the dark data were not NDD, then even a small reduction in sample size\ncould mean we were left with a distorted data set.\" [@darkdata, p.237]\n\nDistorting a dataset could be caused by either dropping records or imputing\nvalues for missing observations. There are far too many machine learning\ntutorials online that carelessly instruct learners to do so, in order to be\nable to use the algorithm of choice. Mindlessly erasing records with missing\nobservations would generally mean reversion to the mean but has the potential\nto propagate algorithmic harm. Could the missing records represent people at\nthe fringes of society, whose data are often challenging to capture? \n\nTreating these records should involve a good deal of investigation first. Is\nthere a hint in the present data that could explain the missingness? To help\npython developers assess missingness in a dataset, I highly encourage people to\nmake use of the [`missingno` package][missingno]. If you write in R, then the\nsame functionality is available with [`naniar`][naniar].\n\nThe below visual gives an overview of populated records in a data table about\nroad traffic collisions. Dark bands are populated records. White bands are\nmissing records.\n\n::: {#9d56fb02 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport missingno as msno\n\ncollisions = pd.read_csv(\"https://raw.githubusercontent.com/ResidentMario/missingno-data/master/nyc_collision_factors.csv\")\nmsno.matrix(collisions.sample(250))\n```\n\n::: {.cell-output .cell-output-display}\n![](04-dark-data_files/figure-html/cell-2-output-1.png){width=1963 height=1064}\n:::\n:::\n\n\nIn the above visual, you may see that certain columns demonstrate more\nmissingness than others. But that specific columns show patterns in missingness\nin comparison with that of other columns too. For example, in records where\n`ON STREET NAME` and `CROSS STREET NAME` are missing, you may find that\n`OFF STREET NAME` records are present. This simple visual along with many more\nuseful tools in the package are available to examine the idiosyncracies within\ndata.\n\n## In Summary\n\n...\n\n\n[missingno]: https://github.com/ResidentMario/missingno\n[naniar]: https://naniar.njtierney.com/\n\n",
    "supporting": [
      "04-dark-data_files"
    ],
    "filters": [],
    "includes": {}
  }
}