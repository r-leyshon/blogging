{
  "hash": "5ad7c90367521166aa567df6dd46f907",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Dark Data\"\nsubtitle: \"Why what you don't know matters\"\nauthor: \"Rich Leyshon\"\ndate: July 01 2024\ndescription: \"A perfect compendium of gotchas for the entry data scientist.\"\ncategories:        \n    - Non-fiction\n    - Data\n    - Science\n    - Mathematics\n    - Statistics\nimage: /./www/book-reviews/04-dark-data/intro-img.jpg\nimage-alt: \"An image of a black hole within a field of binary digits.\"\ntoc: true\ntoc-depth: 4\njupyter: \n  kernelspec:\n    name: \"missingno-env\"\n    language: \"python\"\n    display_name: \"missingno-env\"\n---\n\n<img class=shaded_box src=/./www/book-reviews/04-dark-data/intro-img.jpg alt=\"An image of a black hole within a field of binary digits.\" style=\"display:block;margin-left:auto;margin-right:auto;width:40%;border:none;\">\n\n## Book Summary\n\n\"Dark Data: Why What You Don't Know Matters\" by David J. Hand delves into the\ncrucial concept of dark data â€” information that is unseen, uncollected, or\nunanalysed but significantly impacts decision-making and understanding. Hand\ncategorises dark data into 15 types, illustrating the dangers of ignoring such\ndata through historical and contemporary examples like FDR's reelection polls\nand the Challenger shuttle disaster. The book highlights how unrecognised dark\ndata can lead to skewed understandings, incorrect conclusions, and flawed\nactions.\n\n> \"The first step must always be to be aware there might be dark data. Indeed,\nyour default assumption should be that the data are incomplete or inaccurate.\n*That is the most important message of this book: be suspicious about the data*\n- at least until it is proved they are adequate and accurate.\"\n[@darkdata, p.293]\n\nHand emphasises the importance of recognising and mitigating dark data,\nteaching readers to be vigilant about the issues posed by unknown information.\nHe also explores how dark data can be strategically utilised. The book\naddresses various statistical methods and concepts, underscoring that even in\nthe age of big data, the data available is never complete. Through practical\nguidance, Hand aims to help readers make better decisions in a world where\nmissing data is inevitable, stressing the significance of understanding what is\nnot known.\n\n## On the Author\n\nDavid J. Hand is a distinguished British statistician born in Peterborough,\nEngland. He received his BA from the University of Oxford and his PhD from the\nUniversity of Southampton. Hand's academic career includes significant roles\nsuch as serving as a professor of statistics at the Open University from 1988\nto 1999 and later becoming an Emeritus Professor of Mathematics at Imperial\nCollege London, where he also worked as a Senior Research Investigator. His\nresearch interests are broad and include multivariate statistics,\nclassification methods, pattern recognition, computational and foundational\nstatistics, with a keen focus on data mining, data science, and big data.\n\n## Three Takeaway Ideas\n\n### On Human Bias\n\n> \"...At least until the scientific revolution... advances in understanding\nwere retarded by a (typically subconscious) reluctance to collect data which\nmight disprove a theory... advances were held back by an unwillingness to make\ndark data visible...\n\n> My favourite historical example of someone who spotted this problem is given by\nthe seventeenth-century philosopher, Francis Bacon, who wrote: \"Human\nunderstanding when it has once adopted an opinion... draws all things else to\nsupport and agree with it. And though there be a great number and weight of\ninstances to be found on the other side, yet these it either neglects and\ndespises, or else by some distinction sets aside and rejects.\"\n\" [@darkdata, pp. 167-168]\n\nThis human bias may be one of the fundamental flaws in human psychology. It's\nsomething that people consider exists in everyone apart from themselves! My\nfavourite (disputed) quote which has become a widely used example of\nconfirmation bias has been attributed to the American film critic Pauline Kael,\non the topic of Nixon's 1972 U.S. election victory:\n\n> \"I can't believe Nixon won. I don't know anyone who voted for him.\"\n\nI have known highly intelligent, capable people whose default position on\nalternative opinions is not just one of scepticism, but one of objection. And\ndoubtlessly there have also been times when I have fallen into this trap\nmyself. The issue with discounting, excluding and mocking counter opinions is\nthat; on occasion; they are falsifiable and supported by the evidence! Widely\naccepted dogma can, has been and will be shattered by those willing to think\noutside of broadly accepted opinion. \n\nMy favourite 2 examples of this:\n\n1. A young James Dyson's rejection when pitching his innovative bagless vacuum\ntechnology to Hoover @dyson-vindication. Hoover did not see the potential in\nDyson's early prototype and disliked the fact that they would not be able to\nmarket their profitable vacuum bags and filters with such a model.\n2. Did you know that the concept of continental drift was highly disputed by \ngeologists until the mid twentieth century? Bill Bryson @short-history\ndescribes this as dogmatic resistance of the scientific community to truth. It\nhappened that Alfred Wegener, who proposed the idea in World War 2 was not a\ntrained geologist and therefore widely disputed. Even in the mid 1950s, the\nconcept had not been adopted and notable Harvard University professor Charles\nHapgood @deny-drift was vehemently opposed to the concept. The fact that a\nconcept this profound was being disputed in academic circles just thirty years\nbefore my birth shattered some preconceptions that I had about our\nunderstanding of our world. As a child in the '90s, and having a predisposition\ntowards studying science, I had a complete ignorance that this topic had been\nso recently contentious in academic circles. My intuition is that many others\nare similarly ignorant of this.\n\n### Publication Bias\n\nHand rightfully has quite a bit to say about the reproducibility crisis in\nacademic literature. The author explores reproducibility, publication bias and\nfraud in this domain. Although these issues may or may not be\nintentional, they have the potential to frustrate progress and prolong\nsuffering, as exemplified in the case of the recent retraction @nature-retract of an\ninfluential study on Alzheimer's disease published in Nature in 2006\n@nature-amyloid. This paper has been cited more than 2,000 times and used brain\nscans of patients that have been shown to have been falsified.\n\nHand goes on to talk about a lesser known practice in academia known as\nharking:\n\n> \"Yet another cause of mistaken results is the pernicious practice called\n\"HARKing\", or *H*ypothesizing *A*fter the *R*esult is known...\n\n> HARKing can be alleviated by requiring researchers to state their hypotheses\nbefore they collect any data. Some scientific journals are considering moving\nin this direction, guaranteeing publication of a paper regardless of how the \nresults come out...\" [@darkdata, pp.198-199]\n\nI thoroughly agree with this suggestion. Wikipedia @wiki-pub-bias quotes a\nstudy stating that papers with significant findings are 3x more likely to be\npublished than those with null findings. And as the number of publications is\nbroadly interpreted as a measure of success for academic institutions and \nindividuals alike, a systemic pressure for fraudulent practice emerges. I would\ngo even further and suggest that the barriers to journals demanding full\nreproducibility of analysis - including open and versioned code and data - is\nnow lower than ever. In 2024, every respectable journal can require its\ncontributors to use open source software to increase transparency and prove\nreproducibility. So why has this practice not been established? I presume there\nto be inertia in adopting newer tooling. A legacy uplift issue that would\npresent modest overhead for significant gain. After all, most  people who\nundertake their statistical analysis in legacy or proprietary software have a\nthorough knowledge framework that would greatly assist them in transitioning to\nopen source languages.\n\n### Data Missingness\n\nHand adapts the American Statistician Donald Rubin's description of the causes\nof missing data into the following:\n\n* Unseen Data Dependent (UDD): The cause of the missingness is related to the\nvalue of the missing datum. No reason for this missingness can be discerned\nfrom the extant data.\n* Seen Data Dependent (SDD): A clue to the reason for the missing datum is\navailable within the extant data. \n* Not Data Dependent (NDD): The reason for the missingness is unrelated to the\nextant data.\n\nHand's explanation of Complete Case Analysis is an excellent warning to all\nthose new to data science. The very common practice of dropping or imputing\nmissing values from the data can be a very risky business. What if there were\nan underlying pattern in the missingness? Hand employs a perfectly clear,\nsimple tabular example that have no complete cases, meaning that dropping\nmissing observations would result in no records. \n\n> \"...if the dark data were not NDD, then even a small reduction in sample size\ncould mean we were left with a distorted data set.\" [@darkdata, p.237]\n\nDistorting a dataset could be caused by either dropping records or imputing\nvalues for missing observations. There are far too many machine learning\ntutorials online that carelessly instruct learners to do so, in order to be\nable to use the algorithm of choice. Mindlessly erasing or imputing records\nwith missing observations would generally mean reversion to the mean but has\nthe potential to propagate algorithmic harm. Could the missing records\nrepresent people at the fringes of society, whose data are often challenging to\ncapture? I worry that this practice is far too widespread within the field of\ndata science, as many of the popular machine learning algorithms will not\ntolerate the inconvenient truth of dark data.\n\nTreating these records should involve a good deal of investigation. Is there a\nhint in the present data that could explain the missingness? Could correlations\nwith other dimensions in the data mean a more appropriate imputation value based\non a sub-group average, rather than the population average?  To help python\ndevelopers assess missingness in a dataset, I highly encourage people to make\nuse of the [`missingno` package][missingno]. If you write in R, then the\nequivalent functionality is available with [`naniar`][naniar].\n\nThe below visual gives an overview of populated records in a data table about\nroad traffic collisions. Dark bands are populated records. White bands are\nmissing records.\n\n::: {#9ad97741 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport missingno as msno\n\ncollisions = pd.read_csv(\"https://raw.githubusercontent.com/ResidentMario/missingno-data/master/nyc_collision_factors.csv\")\nmsno.matrix(collisions.sample(250))\n```\n\n::: {.cell-output .cell-output-display}\n![](04-dark-data_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nIn the above visual, you may see that certain columns demonstrate more\nmissingness than others. But that specific columns show patterns in missingness\nin comparison with that of other columns too. For example, in records where\n`ON STREET NAME` and `CROSS STREET NAME` are missing, you may find that\n`OFF STREET NAME` records are present. This simple visual along with many more\nuseful tools in the package are available to examine the idiosyncrasies within\ndata.\n\n## In Summary\n\nIn conclusion, David J. Hand offers an insightful exploration into the realm of\nunseen, uncollected, or unanalysed data that profoundly impacts decision-making\nand understanding. Through detailed categorisation and vivid historical and\ncontemporary examples, Hand underscores the critical need to recognise and\naddress the hidden gaps in our data. His comprehensive analysis highlights how\nignorance of dark data can lead to flawed conclusions and poor outcomes, while\nalso presenting strategies to mitigate these risks. By encouraging a vigilant\nand sceptical approach to data analysis, Hand empowers readers to navigate the\ncomplexities of incomplete information, ultimately fostering better\ndecision-making in an era where data is ever-present but often imperfect. This\nbook is a valuable resource for anyone looking to deepen their understanding of\nthe unseen dimensions of data and its far-reaching implications.\n\n[missingno]: https://github.com/ResidentMario/missingno\n[naniar]: https://naniar.njtierney.com/\n\n",
    "supporting": [
      "04-dark-data_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}