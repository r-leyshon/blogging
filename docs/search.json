[
  {
    "objectID": "music-reviews/index.html",
    "href": "music-reviews/index.html",
    "title": "Productivity Pulse",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWhen Worlds Collide\n\n\n3 min\n\n\nThomas Barrandon’s understated scifi masterpiece\n\n\n\nRich Leyshon\n\n\nMay 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriads\n\n\n5 min\n\n\nCode Elektro’s exploration of future Japan’s criminal underworld\n\n\n\nRich Leyshon\n\n\nApr 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonkey Kong Country Relaxing Piano (Instrumental)\n\n\n5 min\n\n\nBeautiful zen with a nostalgic twist.\n\n\n\nRich Leyshon\n\n\nJan 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZelda & Chill\n\n\n9 min\n\n\nA Lofi Homage to Nintendo’s Zelda Series, Composed by Mikel & GameChops\n\n\n\nRich Leyshon\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGhostrunner\n\n\n9 min\n\n\nA review of the soundtrack to the 2020 hit video game Ghostrunner, composed by Daniel Deluxe.\n\n\n\nRich Leyshon\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music-reviews/04-triads.html",
    "href": "music-reviews/04-triads.html",
    "title": "Triads",
    "section": "",
    "text": "Welcome to neon-lit streets, where the glint of laser-edged katana underlie the bustling cityscape. Where proud tradition is falling foul of organised exploitation. A realm where cybernetic dreams and dystopian nightmares converge in a symphony of pulsating synths and electrifying beats. Code Elektro’s masterpiece third album invites listeners on an immersive journey through a world where tradition meets technology, and the line between reality and virtuality blurs. With each track, the album paints a vivid portrait of a futuristic Tokyo, where shadowy figures roam the alleys and threat stalks the unwary.\n\n\n\nFor a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\nTriads is perfect for those moments when you crave an escape into a futuristic realm of imagination. Whether you’re embarking on a late-night drive through the city, diving into a cyberpunk novel, or simply seeking to enhance your creative flow, this album serves as the ideal soundtrack to accompany your journey into the unknown. Its pulsating rhythms and evocative melodies evoke a sense of adventure and intrigue, making it the perfect companion for any escapade into the realm of dystopian dreams.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature."
  },
  {
    "objectID": "music-reviews/04-triads.html#dysto-tokyo",
    "href": "music-reviews/04-triads.html#dysto-tokyo",
    "title": "Triads",
    "section": "",
    "text": "Welcome to neon-lit streets, where the glint of laser-edged katana underlie the bustling cityscape. Where proud tradition is falling foul of organised exploitation. A realm where cybernetic dreams and dystopian nightmares converge in a symphony of pulsating synths and electrifying beats. Code Elektro’s masterpiece third album invites listeners on an immersive journey through a world where tradition meets technology, and the line between reality and virtuality blurs. With each track, the album paints a vivid portrait of a futuristic Tokyo, where shadowy figures roam the alleys and threat stalks the unwary.\n\n\n\nFor a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\nTriads is perfect for those moments when you crave an escape into a futuristic realm of imagination. Whether you’re embarking on a late-night drive through the city, diving into a cyberpunk novel, or simply seeking to enhance your creative flow, this album serves as the ideal soundtrack to accompany your journey into the unknown. Its pulsating rhythms and evocative melodies evoke a sense of adventure and intrigue, making it the perfect companion for any escapade into the realm of dystopian dreams.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature."
  },
  {
    "objectID": "music-reviews/04-triads.html#the-tracks",
    "href": "music-reviews/04-triads.html#the-tracks",
    "title": "Triads",
    "section": "The Tracks",
    "text": "The Tracks\nBelow are the mean audio feature values for each track. The last row (green) presents a mean summary of the album.\n\n\n\n\n\n\ntrack_name\ndanceability\nenergy\nspeechiness\nacousticness\ninstrumentalness\nvalence\n\n\n\n\nRise of the Triads (Intro)\n0.31600\n0.4540000\n0.0471000\n0.6570000\n0.9000000\n0.1350\n\n\nShinobi\n0.46100\n0.7640000\n0.0344000\n0.0045100\n0.8940000\n0.1190\n\n\nNight Train\n0.51300\n0.5920000\n0.0378000\n0.2420000\n0.8990000\n0.1110\n\n\nChinese Dreams\n0.41100\n0.4140000\n0.0262000\n0.6480000\n0.9740000\n0.0732\n\n\nThe Monk\n0.47300\n0.9780000\n0.0427000\n0.0195000\n0.7320000\n0.4280\n\n\nIn the Shadows\n0.56600\n0.5280000\n0.0297000\n0.7830000\n0.8750000\n0.1900\n\n\nThe Wilderness\n0.48400\n0.7480000\n0.0344000\n0.0004190\n0.3020000\n0.3430\n\n\nInternational Karate\n0.57200\n0.7820000\n0.0357000\n0.0080500\n0.9510000\n0.2830\n\n\nTriads\n0.44000\n0.7550000\n0.0362000\n0.0101000\n0.8320000\n0.1610\n\n\nMission Control\n0.65500\n0.7330000\n0.0549000\n0.4410000\n0.8910000\n0.2560\n\n\nSilent Runner\n0.37300\n0.2570000\n0.0314000\n0.3730000\n0.8940000\n0.1780\n\n\nTokyo Dawn\n0.29500\n0.3680000\n0.0289000\n0.8160000\n0.9040000\n0.1840\n\n\nAlbum Mean\n0.46325\n0.6144167\n0.0366167\n0.3335482\n0.8373333\n0.2051\n\n\n\n\n\n\n\n\nThe ridge plot profile is remarkably similar to the ghostrunner ost, reviewed back in September 2023. Like ghostrunner, this albunm is low in speechiness and high in instrumentalness. In contrast, ghostrunner reported a broader distribution in valence than triads, which I find surprising. Triads is certainly a less menacing listen with some really beautiful melodies influenced by Japanese culture."
  },
  {
    "objectID": "music-reviews/04-triads.html#rise-of-the-triads-intro",
    "href": "music-reviews/04-triads.html#rise-of-the-triads-intro",
    "title": "Triads",
    "section": "Rise of the Triads (Intro)",
    "text": "Rise of the Triads (Intro)\nThis introductory track sets the stage for adventure. With pulsating synths and an aura of anticipation, it serves as the perfect prelude to the sonic journey ahead."
  },
  {
    "objectID": "music-reviews/04-triads.html#shinobi",
    "href": "music-reviews/04-triads.html#shinobi",
    "title": "Triads",
    "section": "Shinobi",
    "text": "Shinobi\n“Shinobi” channels the spirit of classic ninja films with its driving rhythms and mysterious melodies. Its electrifying energy evokes intrigue and images of stealthy warriors navigating through moonlit landscapes, ready to face any challenge."
  },
  {
    "objectID": "music-reviews/04-triads.html#night-train",
    "href": "music-reviews/04-triads.html#night-train",
    "title": "Triads",
    "section": "Night Train",
    "text": "Night Train\nMy favourite track. “Night Train” is an inspired prelude to action. An inspiring ride through the skies of a neon-lit cyberpunk city. A steady tempo and glittering percussive synth arrangement introduce the city’s grand scale accompanied by an alluring mix of danger and opportunity."
  },
  {
    "objectID": "music-reviews/04-triads.html#chinese-dreams",
    "href": "music-reviews/04-triads.html#chinese-dreams",
    "title": "Triads",
    "section": "Chinese Dreams",
    "text": "Chinese Dreams\nWith “Chinese Dreams,” Code Elektro delivers a mesmerizing blend of Eastern-inspired melodies and retro-futuristic synths. Its hypnotic rhythms and evocative instrumentation transport listeners to a realm where ancient traditions meet cutting-edge technology."
  },
  {
    "objectID": "music-reviews/04-triads.html#the-monk",
    "href": "music-reviews/04-triads.html#the-monk",
    "title": "Triads",
    "section": "The Monk",
    "text": "The Monk\n“The Monk” immerses listeners in a meditative soundscape, the onset of a surly tempest and a gong that could have been sampled straight from a ’70s martial arts flick. Contemplative melodies set to swelling rhythms create a sense of growing urgency."
  },
  {
    "objectID": "music-reviews/04-triads.html#in-the-shadows",
    "href": "music-reviews/04-triads.html#in-the-shadows",
    "title": "Triads",
    "section": "In the Shadows",
    "text": "In the Shadows\n“In the Shadows” conjures images of clandestine meetings and covert operations with its brooding atmosphere and enigmatic melodies. Its pulsating rhythms and suspenseful build-ups create a sense of tension and intrigue, keeping listeners on the edge of their seats. Something dark is afoot, and only a select few will ever know of what happened this day."
  },
  {
    "objectID": "music-reviews/04-triads.html#the-wilderness",
    "href": "music-reviews/04-triads.html#the-wilderness",
    "title": "Triads",
    "section": "The Wilderness",
    "text": "The Wilderness\n“The Wilderness” transports listeners to a landscapes where nature has been thoroughly hybridised with technology. Its expansive soundscapes and ethereal melodies evoke feelings of awe and wonder, then interspersed with an abundance of chiptune glitch and reverb, challenging assumptions and provoking wonder."
  },
  {
    "objectID": "music-reviews/04-triads.html#international-karate",
    "href": "music-reviews/04-triads.html#international-karate",
    "title": "Triads",
    "section": "International Karate",
    "text": "International Karate\nThis track is a steady movement with inspired homages to the sweet melodies of the East. Its minimalist approach provides a moment of introspection amidst the album’s high-energy tracks."
  },
  {
    "objectID": "music-reviews/04-triads.html#triads",
    "href": "music-reviews/04-triads.html#triads",
    "title": "Triads",
    "section": "Triads",
    "text": "Triads\nThe title track of the album is a tour de force of synth-driven bliss. “Triads” starts with a subtle ascent evoking impending action or imminent discovery. Then somewhere a door is kicked through and violence spills out onto the street, with a bassy reverb drawing the attention of all passers-by to the apparent chaos. Its pulsating rhythms and euphoric melodies leave a lasting impression."
  },
  {
    "objectID": "music-reviews/04-triads.html#mission-control",
    "href": "music-reviews/04-triads.html#mission-control",
    "title": "Triads",
    "section": "Mission Control",
    "text": "Mission Control\n“Mission Control” takes listeners on a cosmic voyage through space and time. Its propulsive rhythms evoke the spirit of space exploration and the promise of discovery, inviting listeners to join in on an epic adventure to the far reaches of the universe."
  },
  {
    "objectID": "music-reviews/04-triads.html#silent-runner",
    "href": "music-reviews/04-triads.html#silent-runner",
    "title": "Triads",
    "section": "Silent Runner",
    "text": "Silent Runner\n“Silent Runner” captivates listeners with its hauntingly beautiful melodies and hypnotic rhythms. A sweet soprano accompaniment counters a mysteriously brooding rhythm section. Its ethereal atmosphere and dreamlike quality create a sense of weightlessness, like drifting through the cosmos on a silent journey."
  },
  {
    "objectID": "music-reviews/04-triads.html#tokyo-dawn",
    "href": "music-reviews/04-triads.html#tokyo-dawn",
    "title": "Triads",
    "section": "Tokyo Dawn",
    "text": "Tokyo Dawn\nClosing out the album is “Tokyo Dawn,” a mesmerizing journey through to the dawn, bringing a sense of security and achievement. A sweet and contemplative outtro accompanying the hero through their jounrey home."
  },
  {
    "objectID": "music-reviews/04-triads.html#overall",
    "href": "music-reviews/04-triads.html#overall",
    "title": "Triads",
    "section": "Overall",
    "text": "Overall\nIn conclusion, “Triads” is a captivating odyssey through a cybernetic dreamscape, where each track offers a unique sonic experience that transports listeners to distant realms of imagination. Whether you’re a fan of synthwave or simply appreciate immersive music that takes you on a journey, “Triads” is an album that is sure to leave a lasting impression.\nFor more music to encourage your work efforts, check out Productivity Pulse."
  },
  {
    "objectID": "music-reviews/02-zelda-and-chill.html",
    "href": "music-reviews/02-zelda-and-chill.html",
    "title": "Zelda & Chill",
    "section": "",
    "text": "Nintendo’s Zelda series likely needs no introduction. In its fourth decade and one of the most successful adventure series in video games, Zelda is beloved for its sense of innovation.\nThe music of the games has often been at the core of establishing its identity and; at times; has played a key construct in the games’ development. The N64 batch of games adopted a core game mechanic where the ability to accurately play songs on an instrument called an ocarina triggered key plot events such as travelling through time. Perhaps due to this, many of the tracks here are from this period of Zelda’s history."
  },
  {
    "objectID": "music-reviews/02-zelda-and-chill.html#introduction",
    "href": "music-reviews/02-zelda-and-chill.html#introduction",
    "title": "Zelda & Chill",
    "section": "",
    "text": "Nintendo’s Zelda series likely needs no introduction. In its fourth decade and one of the most successful adventure series in video games, Zelda is beloved for its sense of innovation.\nThe music of the games has often been at the core of establishing its identity and; at times; has played a key construct in the games’ development. The N64 batch of games adopted a core game mechanic where the ability to accurately play songs on an instrument called an ocarina triggered key plot events such as travelling through time. Perhaps due to this, many of the tracks here are from this period of Zelda’s history."
  },
  {
    "objectID": "music-reviews/02-zelda-and-chill.html#songs-for-the-recovering-adventurers",
    "href": "music-reviews/02-zelda-and-chill.html#songs-for-the-recovering-adventurers",
    "title": "Zelda & Chill",
    "section": "Songs for the Recovering Adventurers",
    "text": "Songs for the Recovering Adventurers\nZelda & Chill is a love note to the iconic music produced by the Zelda developers. As a casual fan, I have enjoyed many of the games in the series and the album manages to prod the nostalgia sensor in my brain even after multiple playthroughs. But for those disinterested in video games, this album is great for drowning out the outside world and establishing the focus needed for deep work.\n\n\n Let’s take a look at the audio feature analysis for this album. For a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature.\n\n\n\nI find it surprising that there is such a wide distribution in the valence of the songs. I expected the album to be distributed centrally as the pace and tone of the music is generally understated. But there are a couple of tracks that strike a more melancholy tone, such as Dark World and Gerudo Valley. At the other end of the scale, the Legend of Zelda theme is likely the most positive on offer, helping to explain why a broad distribution of valence is observed. The album’s speechiness and instrumentalness score as you’d expect. Danceability is notably high, potentially as a result of the crunchy drum & bass lines present in all the songs. Acousticness is generally low throughout the tracks though with a broad distribution that is potentially influenced by recurring use of piano and string sections. Perhaps the most interesting feature for this album is energy, which describes the album as distinctly middle-of-the-road in terms of intensity. As a chill album you’d likely expect a lower energy but the strong rhythm section likely lifts the distribution of this feature."
  },
  {
    "objectID": "music-reviews/02-zelda-and-chill.html#the-tracks",
    "href": "music-reviews/02-zelda-and-chill.html#the-tracks",
    "title": "Zelda & Chill",
    "section": "The Tracks",
    "text": "The Tracks\nBelow are the mean audio feature values for each track. The last row (green) presents a mean summary of the album.\n\n\n\n\n\n\ntrack_name\ndanceability\nenergy\nspeechiness\nacousticness\ninstrumentalness\nvalence\n\n\n\n\nFairy Fountain\n0.6710000\n0.5250000\n0.0506000\n0.4040000\n0.924\n0.2830000\n\n\nDark World\n0.6700000\n0.3380000\n0.0569000\n0.4400000\n0.864\n0.5620000\n\n\nLost Woods\n0.7150000\n0.5440000\n0.0513000\n0.0895000\n0.935\n0.9080000\n\n\nSong of Storms\n0.6750000\n0.4580000\n0.0305000\n0.0181000\n0.889\n0.4790000\n\n\nMinuet of Forest\n0.7910000\n0.4820000\n0.0434000\n0.1750000\n0.857\n0.6980000\n\n\nGerudo Valley\n0.8430000\n0.4340000\n0.0844000\n0.4400000\n0.832\n0.1670000\n\n\nOath to Order\n0.7830000\n0.4640000\n0.1500000\n0.7480000\n0.805\n0.2200000\n\n\nDragon Roost Island\n0.7730000\n0.2450000\n0.0582000\n0.2530000\n0.932\n0.5530000\n\n\nKakariko Village\n0.7110000\n0.4330000\n0.1990000\n0.4520000\n0.905\n0.6260000\n\n\nBallad of the Goddess\n0.7590000\n0.4100000\n0.0595000\n0.0061500\n0.821\n0.3540000\n\n\nBreath of the Wild\n0.5410000\n0.4050000\n0.0418000\n0.0471000\n0.840\n0.0387000\n\n\nHateno Village\n0.7600000\n0.3470000\n0.0337000\n0.3430000\n0.689\n0.3940000\n\n\nLegend of Zelda\n0.5210000\n0.6270000\n0.0265000\n0.2510000\n0.847\n0.7260000\n\n\nOcarina of Time\n0.6440000\n0.4870000\n0.0419000\n0.1540000\n0.858\n0.2450000\n\n\nAlbum Mean\n0.7040714\n0.4427857\n0.0662643\n0.2729179\n0.857\n0.4466929\n\n\n\n\n\n\n\n\nFairy Fountain\n64-bit nostalgia ahead. While the Fairy Fountain has offered sanctuary to any daring adventurer donning the guise of Link throughout the entire Zelda series, this theme is synonymous with the N64’s Ocarina of Time. This rendition is a wonderful, crunchy production over a stripped-back ragtime tempo. Warm feelings of safe havens are immediately conjured from the outset, with that iconic dreamy harp intro. A strong start to the album that sets the mood for what’s to come.\nDark world\nFor those Zelda fans who can identify with A Link to the Past, this track may hold more significance. I have not played that game and checking out the original track, I’d say this version seems like a marked improvement. There’s a fascinating contrast in this song - the meaty bass forms the perfect foil to the high pitch tubular bell-effect hook.\nLost Woods\nAnother one for the Ocarina of Time fans. A slower tempo than theoriginal, hence the “Relax” in the album title. There’s a curious reverb effect at play that sounds a bit like a harpsichord fed through a tin can. After listening to quite a bit of lofi I am on board with this, but I could see that it may be an acquired taste for some. Against the light and airy melody, the low-tech production works really well.\nSong of Storms\nFamiliar to those wishing to summon rain in Ocarina of Time, this laid back rendition is mixed with a loop of the cooking theme from Breath of the Wild, replete with periodic pot whistles & cutlery clanking against china. The creative flair on show as the looped Song of Storms theme builds to a modest crescendo (it’s chill after all) makes this one of my album highlights.\nMinuet of Forest\nThis song was originally used in Ocarina of Time to warp characters back to the start of the lost woods play area - one of the more challenging areas in the early game. The artists have expertly built on what was a very brief movement in the original material, adding a bouncy low-end bass while keeping the track structured with a snappy drum section.\nGerudo Valley\nAnother Ocarina of Time offering here. The original Gerudo Valley theme was played on Spanish guitar in a flamenco-style with a high tempo. This rendition is nearly unrecognisable and undoubtedly brilliant. How the artist has isolated the key movements and set them against the slow, chuffing rhythm is quite amazing. The song is one of the more melancholy tunes and while lower in mood is undoubtedly beautiful.\nOath to Order\nThis song originally appeared in Majora’s Mask and was used as a plot construct in that game - apparently to summon giants. I have little association with Majora’s Mask and having listened to it, I fail to identify the musical connection with the source material. Someone with a greater familiarity would potentially find more to identify with. As a standalone piece, Oath to Order is a very warm-sounding, fuzzy sensation with a warbly theremin effect permeating throughout for a unique twist.\nDragon Roost Island\nIt’s great to have a theme from the Windwaker on the album, which in my opinion is one of the best Zelda games and is often overlooked for its cute exterior. The original track was distinctly latin with Spanish guitar, maracas and a forlorn panpipe high in the mix. As in Gerudo Valley, the artists have slowed the pace while emphasising the main movement in the original source material. The guitars have been replaced with warm, fuzzy piano for a more relaxed atmosphere.\nKakariko Village\nKakariko Village has made recurring appearances throughout the Zelda series of games and I have struggled to identify which version of the series soundtracks this song takes its inspiration from. Though the melody is very distinct and the track is another of my album highlights. I presume it must be from one of the games I am less familiar with. A light and plucky lick against a bright and bouncy bassline, there’s plenty to appreciate here.\nBallad of the Goddess\nAnother recurring song in the later games within the Zelda series, I find it a very enjoyable rendition despite my lack of familiarity with the source material. The song seems to originate from Skyward Sword, building upon the melody using sweet harp strings and later a dramatic orchestral accompaniment. In this version, the artist has retained the harp but has worked with piano for the main melody.\nBreath of the Wild\nTaking its inspiration from the main theme of Breath of the Wild which perhaps lacks a distinctive identity in my opinion, this song starts with a promising loop of one of its more recognisable movements. However, mid-way through the song it introduces some distinctly discordant keys that introduce a sort of dilapidated fairground motif to the song. This is one that I tend to skip for those reasons - I find it hard to concentrate on my work with discordant sound in my ears. I view the track as an experiment and not in keeping with the rest of the album.\nHateno Village\nAnother Breath of the Wild arrangement, Hateno Village is notably home to the Ancient Tech Lab - an important site for acquiring gear upgrades as the game progresses. In this rendition of the theme, the artists have retained some deep bass strings although the main melody is now expertly played on piano. Again, I feel that the original material is not the strongest explored in this album, while the reimagining on offer here is perhaps more interesting than the source material.\nLegend of Zelda\nMaybe the most iconic video theme of all time. This mix pays homage to the series’ 8-bit roots by experimenting with a chiptune style that does a great job of not getting skipped while I work - chiptune can be fun but not generally for concentration. One of the more moving songs on offer, this track builds to a crescendo replete with choral and piano arrangements, leaving the listener with a distinct sense of grandeur capturing the scale of the video games.\nOcarina of Time\nThe theme from the original N64 Zelda title is a fitting end to this fantastic homage to perhaps one of the greatest video game adventure series. Kids of my generation will recall this iconic track as it accompanied the Ocarina of Time intro. The intro that blew the minds of a generation of Zelda fans who got to see Link in 3D and on horseback in an amazing open world like never before. Great times."
  },
  {
    "objectID": "music-reviews/02-zelda-and-chill.html#highs-and-lows",
    "href": "music-reviews/02-zelda-and-chill.html#highs-and-lows",
    "title": "Zelda & Chill",
    "section": "Highs and Lows",
    "text": "Highs and Lows\nThe Breath of the Wild track is not to my taste and on writing this review, I’ve realised that I consider the soundtrack for this game to be somewhat inferior to the older games. No hate to Breath of the Wild, which is a masterpiece in modern gaming, but perhaps the soundtrack is less of a focal point and potentially appealing to a broader and more mature audience than the older games in the series.\nStandout tracks to my mind are Song of Storms, Kakariko Village and Legend of Zelda."
  },
  {
    "objectID": "music-reviews/02-zelda-and-chill.html#overall",
    "href": "music-reviews/02-zelda-and-chill.html#overall",
    "title": "Zelda & Chill",
    "section": "Overall",
    "text": "Overall\nThis album is a great choice for steadying the ship following a busy schedule. It establishes a quiet focus, allowing your brain’s operating system to reboot following a demanding presentation. I’d suggest this album as the first in a playlist dedicated to carving out an afternoon of focus time. Set your status to ‘do not disturb’ and eliminate your to-do list with the help of Link and his friends.\nFor more music to encourage your work efforts, check out Productivity Pulse."
  },
  {
    "objectID": "book-reviews/index.html",
    "href": "book-reviews/index.html",
    "title": "Bookmarked Insights",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDark Data\n\n\n1 min\n\n\n\nNon-fiction\n\n\nData\n\n\nScience\n\n\nMathematics\n\n\nStatistics\n\n\n\nReflecting on Neil Postman’s criticism of modern culture.\n\n\n\nRich Leyshon\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmusing Ourselves to Death\n\n\n33 min\n\n\n\nNon-fiction\n\n\nMedia\n\n\nCriticism\n\n\n\nReflecting on Neil Postman’s criticism of modern culture.\n\n\n\nRich Leyshon\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithms to Live By\n\n\n13 min\n\n\n\nMathematical Modelling\n\n\nComputer Science\n\n\n\nThoughts on Brian Christian & Tom Griffiths treatment of applied computer science.\n\n\n\nRich Leyshon\n\n\nSep 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Work\n\n\n9 min\n\n\n\nSelf-Help\n\n\nProductivity\n\n\nBusiness\n\n\nPsychology\n\n\nPhilosophy\n\n\n\nA review of Cal Newport’s much lauded productivity how to guide.\n\n\n\nRich Leyshon\n\n\nAug 30, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "book-reviews/03-amusing-ourselves-to-death.html",
    "href": "book-reviews/03-amusing-ourselves-to-death.html",
    "title": "Amusing Ourselves to Death",
    "section": "",
    "text": "Image credit: https://api.ndla.no/"
  },
  {
    "objectID": "book-reviews/03-amusing-ourselves-to-death.html#summary",
    "href": "book-reviews/03-amusing-ourselves-to-death.html#summary",
    "title": "Amusing Ourselves to Death",
    "section": "Summary",
    "text": "Summary\nAs of today’s date, Amusing Ourselves to Death (AOtD) scores an average of 4.15 / 5.00 over 29.5k ratings on goodreads.\nAOtD is a challenging read for a short book. It is heaped in cultural criticism and a pessimistic assessment of technological development. I would not recommend readily internalising the content without thoughtful evaluation. However, much of the critique is well-reasoned and although written in the 1980s, is possibly of greater relevance today than when first penned.\n\nOn the Author\nNeil Postman was a professor of communication at New York University. Well-known for his body of work in examining the study of knowledge-acquisition and pedagogy through the lens of societal decline. Postman endeavoured to influence public education curricula in the United States during his term as Chairman of the University’s Department for Culture and Communication."
  },
  {
    "objectID": "book-reviews/03-amusing-ourselves-to-death.html#an-overview-of-the-book",
    "href": "book-reviews/03-amusing-ourselves-to-death.html#an-overview-of-the-book",
    "title": "Amusing Ourselves to Death",
    "section": "An Overview of the Book",
    "text": "An Overview of the Book\nPostman’s critique is that the widespread adoption of television has subversively influenced culture, human behaviour and intellect in ways that could not be foreseen at the technology’s inception. Unaware of the broad repercussions of immersing ourselves in TV pop culture, the standards for entertainment in this medium of communication have permeated through society, from the boardroom to the classroom.\nPostman warns that American culture; seemingly unaware of how trivial and distracted it has become; is at risk of losing its moral compass. The main premise of his warning is that rather than an Orwellian dystopian future where society is controlled by a despotic surveillance state, a more realistic outcome would be characterised by Aldous Huxley’s Brave New World [1]. A future where a simpleton proletariat has been rendered ineffective by the pursuit of hedonism at the cost of all else.\nThe book is organised into 2 parts and 11 chapters. Part 1 establishes the scale of the apparent decline in literacy, logic and debate brought about by the decline of a largely print-based culture. Part 2 argues that aspects of TV as a medium impose limitations on the length of exposition and complexity of language that have widely influenced our lives. Postman exemplifies how this simplification of our public discourse has declined, with considerations from many domains.\n\nPart 1\nThe book begins with a treatment of Huxley’s warning in Brave New World. A future in which civilisation is threatened by an over-supply of information and a reduction of the peoples’ ability to identify that which has meaning. Postman suggests that Western culture had been en guarde for signs of Orwellian oppression but has not adopted a defence against the possibility that we may be oppressed by our own ignorance.\n\n1. The Medium Is the Metaphor\nPostman argues that the technological revolution brought about by mass adoption of television has unwittingly changed the fabric of our society. Using the analogy of the clock, Postman indicates that the mechanism is not merely a device used for measuring time, but that it “dissociates time from human events and thus nourishes the belief in an independent world of mathematically measurable sequences.” [2, p. 12].\nThis analogy does not help someone who is subject to the belief of quantifiable sequence in understanding his point. The thought of a world without clocks is an unimaginable dystopia. The positives that the clock has wrought greatly outweigh any possible drawbacks that I can conceive of. This may be the point entirely - just as the fish is likely unaware that it lives within water, the ability of the human mind to conceive of how it has been influenced by a technology that has been fully integrated through society may be similarly limited.\nPostman goes on to state that the clock has removed the concept of god from the meaning of time. Unable to place a god as the cause of time, I am lost as to how to meaningfully rationalise his analogy. I surmise that Postman’s point is that as a consequence of reducing the concept of time to the machinations of gears, removes any sense of reverence or magical thought. Although some would argue that the internals of an analogue clock are a thing of beauty and the trade of the watchmaker an art. Nonetheless, the reduction of time to a gadget now worn on the wrist may have effects beyond a greater ability to manage time, possibly influencing our perception of our place in the universe. The comparison with television’s replacement of print seems to mean that there are unintended consequences owing to the adoption of a novel technology that are perhaps poorly understood. In my opinion, this is a weak analogy, lacking in consequence. I don’t believe it helps the reader orient to Postman’s main point. The negative consequences of the clock; whatever they may be; must now be broadly accepted as part of human existence.\n\n\n2. Media as Epistemology\nIn this chapter, Postman considers the nature of truth, the acquisition of knowledge and how this has changed with the cultural trend toward visual media. The author posits the concept that all media are not equal in their ability to convey meaning and truth.\nAn interesting juxtaposition is explored - that in certain circumstances, the medium of spoken word is thought to be of higher merit than the written, such as in most modern courtrooms where verbal rather than written testimony is required. However, Postman is able to draw on his experience in academia to illustrate that the reverse is true in the assessment of a thesis, where a candidate had attempted to cite ephemeral evidence; a conversation or interview with an academic. In this example the result had been a witty retort from the review panel,\n“…we are sure you would prefer that this commission produce a written statement that you have passed your examination (should you do so) than for us merely to tell you that you have, and leave it at that. Our written statement would represent the”truth”. Our oral agreement would be only a rumour.” [2, p. 24].\nWho could argue with that? The contrast between written assessment and courtroom testimony’s truth value glazes over the context and purposes of these discourse formats. The courtroom favours verbal testimony as an unadulterated recount of the past and motive, yet quickly commits this testimony to the written word via courtroom stenographers. A raw, unfiltered account for a thesis would only serve as a draft and would require significant editing in order to test that the structure and content of the paper is robust.\nPostman goes on to examine truth as a cultural prejudice for certain media. From tribal counsel’s use of parables through Aristotle’s deductive reasoning about the physical world, Postman finally rounds to the modern world’s adherence to the quantifiable.\n“Many of our psychologists, sociologists, economists… will have numbers to tell them the truth or they will have nothing. Can you imagine, for example, a modern economist articulating truths about our standard of living by reciting a poem?” [2, p. 26]\nI wonder if numbers can be considered a medium. It occurs to me that numbers are a language that can be expressed in a variety of media. Postman could have explored instead the danger of an over-reliance on statistics as truth. The fact that statistics can be selectively deployed to make opposing arguments is of course, not a new one.\n\n\n\n\n\n\nNote\n\n\n\n“Lies, damned lies, and statistics.” attribution debated.\n\n\nThe use of numbers to oversimplify, diminish context and misdirect can also be a misuse of data visualisation. On the nature of bias in the chart,\n“…it makes the viewer believe that they can see everything, all at once, from an imaginary and impossible standpoint. But it’s also a trick because what appears to be everything, and what appears to be neutral, is always … a partial perspective.” [3, Ch. 3].\nAll of us must be guilty of accepting a viewpoint based on the convincing support of charts or numbers, without undertaking the necessary due diligence of reproducing the analysis and seriously examining the assumptions and potential underlying biases. The complexity of society and the volume of analysis we generate make it impossible not to do so. The presumption is that those people charged with the responsibility of evidence-based policy design are carrying out the due diligence on behalf of us all.\n\n\n3. Typographic America\nThe author uses this chapter to familiarise the reader with an abridged history of literacy from 17th century England and the United States through to modern day America. Thanks primarily to the efforts of the churches, literacy among the general populace was understood to have been significantly higher than in the modern day populace. But with a decline in the relevance of the printed word, literacy rates have steadily declined over the previous century.\n17th century people were conditioned to spend long hours of leisure time focussed upon reading a single book. This mental fortitude affected their ability and predilection for other feats of mental endurance. Debating halls were common across the country and accounts of debates extending long into the evenings were said to be common. The contrast between these discussions and the modern approach to televised debates must be striking. It is here that I have little to argue with Postman’s position. It is undoubtedly true that television does nothing to prepare viewers for bouts of prolonged concentration. If a topic cannot be reduced to chunks of ten second sound bites, it generally makes for poor television.\nWhile television is an enjoyable entertainment device, the demands of its format aim to keep the viewer perpetually stimulated and do not imbue its audience with mental fortitude. Postman’s position is that this lack of mental preparation against boredom leaves us struggling to achieve focus in many other contexts. These known limitations of television have given rise to the podcast in recent years, where a type of long format debate has returned to public fora.\n\n\n4. The Typographic Mind\nIn this chapter the author explores how a sustained media shift away from the printed word may have influenced our assessment of culture’s interpretation of truth. The author compares the image-obsessed standards of the televised news shows to a form of infantilism. Where assessment of truth is based upon the appearance of the newscaster rather than the content of the televised message. An industry-wide ad hominem has permeated through even those shows that are intended to be sober and informative. Postman suggests that the form of truth and intelligence valued by ‘the typographic’ mind (pre-television) would have been markedly different.\nAn interesting point is made about an unspoken contract between television and those appearing on it. The author argues that the dominant media in our society governs the behaviours of those wishing to exploit that as a tool. Politicians, scholars and religious figures are reduced to a fickle celebrity by the demands of a television presence - curate your image, reduce your campaign to sound bites, prioritise speed over reason. In considering the first 15 presidents of the United States, Postman states,\n“Public figures were known largely by their written words, for example, not by their looks or even their oratory… To think about those men was to think about what they had written, to judge them by their public positions, their arguments, their knowledge as codified in the printed word.” [2, p. 70].\nRecalling any of the notable icons of the twentieth century would entail instantly recalling their image rather than the content of their words or actions. Political campaigns have been won and lost at the hands of advertisement budgets, party political broadcasts and impactful slogans. It appears that we are in the midst of fast food politics, a generation of the slogan-oriented, averse to analysis of manifesto.\n\n\n5. The Peek-a-Boo World\nHere, the author establishes the “information-action ratio”, a concept that considers the relevance of a communication to the person receiving it.\n“…how often does it occur that information provided to you on morning radio or television, or in the morning newspaper, causes you to alter your plans for the day, or to take some action you would not otherwise have taken, or provides insight into some problem you are required to solve?” [2, p. 78]\nThe author goes on to qualify such exceptions to the low relevance information generally on offer by television, such as weather programming and stock movements. It is an interesting concept that I have pondered, finding exceptions here and there myself, such as programmes that help people to manage their finances, special interest programming such as gardening shows, and so on. I can only estimate how important shows such as Songs of Praise must be for those who are unable to participate in the communal observation of their religion in person. Undeniably, these shows would be exceptions that prove the rule - that the purpose of the television is primarily to entertain, even when it purports to inform.\nPostman’s criticism of what has become “The News of the Day”, which he states started with the invention of the telegraph and has been further transformed by a partnership between television and newspapers into the ‘human interest’ genre, is that the information is low in information-action ratio. With the telegraph, “news from nowhere, addressed to no one in particular, began to criss-cross the nation” [2, p. 78].\nThis commodification of contextless information, Postman argues, was bolstered by photography. Once papers adopted the photograph, “For countless Americans, seeing not reading, became the basis for believing” [2, p. 86]. A prescient point as we enter the age of the deep fake.\nThe author warns that our culture is bombarded with low-relevance information and a limited capacity for sorting it based on relevance. I have found the combination of a negative news loop and continual outrage on social media to have caused a shift in my personal behaviour away from those media. Yet I also note that at the time of writing this blog, the United Kingdom has experienced its largest sustained demonstration in history. Outbreak of war between Israel and Hamas has caused people across the country to consider their own position on the matter, arousing strong emotions that have led to these mass demonstrations. Similarly during the Covid-19 lockdowns, the greater part of the nation coalesced around the television to gain their news from the Covid press briefings. While the vast majority of television’s content may be almost irrelevant to those subject to it, its ability to influence on a mass scale may be magnified during crises.\nI will complete my review of Part 1 on the author’s warning that has troubled me the most. That televised media has become the dominant culture in a way that we can no longer separate the two.\n“Twenty years ago, the question, does television shape culture or merely reflect it? held considerable interest for many scholars and social critics. The question has largely disappeared as television has gradually become our culture.” [2, p. 91].\nAs fish may be largely unaware of the water around them, we are immersed in TV culture and are generally oblivious to how this has mediated our thoughts. One could argue that the relevance of television is in decline and that newer forms of media are to dominate the future. I would argue that the addictive properties of the mobile phone tend to be those that ape features of television, albeit a television that can follow you out of your living room, accompany you on your dog walk and keep you enthralled on your drive to work. It seems that the human predilection for entertainment can subjugate notions of personal safety and consideration for others.\n\n\n\nPart 2\n\n6. The Age of Show Business\nThis chapter best represents the core of Postman’s criticism. He elaborates on how entertainment has become ‘baked-in’ to television’s format, even in the more sober programming. As the author puts it, “…entertainment is the supra-ideology of all discourse on television” [2, p. 102]. Postman argues that this exaltation of entertainment has gone on to influence the wider culture.\n“Television is our culture’s principal mode of knowing about itself… how television stages the world becomes the model for how the world is properly to be staged…\nIn courtrooms, classrooms, operating rooms, boardrooms, churches and even airplanes, Americans no longer talk to each other, they entertain each other. They do not exchange ideas; they exchange images. They do not argue with propositions; they argue with good looks, celebrities and commercials.” [2, p. 108]\nIf I needed to reduce the book down to one quote, it would be the above. The values enshrined by television have been broadly adopted, influencing our societal norms. Postman rightly points to limitations in television’s format that limit reasoned conversation. In its pursuit of the stimulating, television generally does not permit hesitation. Uncertainty is not tolerated on modern TV. The act of thinking does not make for good viewing. Over time, our tolerance for thinking-time in real-life situations such as the boardroom or the job interview has been further eroded by standards propagated by TV. The iconic Dragon’s Den sixty second business pitch, replete with hype and corporate speak becoming the subconscious expectation for discourse in all situations.\nIn recent years, the division between television and the rest of the world has reduced further still, with the ‘reality TV’ genre aiming to commodify what may be represented as everyday life. Now there are TV shows where we can watch others watching TV shows. Instagram is the social media platform where people present an idealised impression of their lives on a dedicated catwalk, while on YouTube, people and families commodify their own lives, merging their leisure time with commercial opportunities in order to monetise every waking second. It is clear that television (and its close cousins) have become inextricably linked with human experience. It is likely that rather than causing the end of television culture, the internet has served television in such diverse formats that it is nearly inescapable.\n\n\n7. “Now… This”\n“For on television, nearly every half hour is a discrete event, separated in content, context and emotional texture from what precedes and follows it.” [2, p. 116]\nPostman applies this in what he refers to in its most embarrassing form to the news of the day shows. Updates may be broadly grouped as ‘politics’ or ‘sport’ but are scheduled to stimulate attention and emotion, in short - to entertain. Accompanied by attractive presenters, stirring music and interspersed with engaging commercials, Postman rightly points out that the viewer understands that even the most horrific article on the human impact of war should not upset the rest of their day. Quickly, the viewer’s palate will be cleansed in preparation for the next article in what has become a form of cultural voyeurism.\nPostman’s point about the fragmented context of a TV schedule may be applied at a finer scale in related visual formats. Many studies have investigated change in cinema and viewer attentiveness. Cornell University [4] have investigated decline in shot duration in a broad range of movies, finding a stable trend in decreasing shot duration. The interpretation of this finding is that cinema adapts to a society with less tolerance for boredom by ensuring that the executive function of cinema-goers is not taxed. Or in the words of Ridley Scott, on his latest movie Napoleon, he constantly watches for the “bum-ache factor” [5] in his movie-goers, which I find quite fascinating - Scott’s 40 year-old science fiction masterpiece Alien is well-known to revel in longer shot durations.\nThis trend in shot length can be anecdotally observed in television also, especially in children’s programming. Some shows that my children adore, I struggled to ‘keep up with’ - almost anything produced by Nickelodeon. The comparison to some of the older endearing shows of yesteryear such as ‘Watch With Mother’, ‘Thomas The Tank’ and ‘The Clangers’ is stark. As culture has adopted the norms of increased segmentation, a feedback loop whereby viewers no longer wish to endure longer format media may have driven a trend toward a reduced shot duration, greater emphasis on motion and greater stimulation brought about by frequent context switching.\nThe author’s warning to society is that the danger of a slew of information bereft of context, relevance and meaning has left people indifferent. How can a person internalise this glut of information without being seriously affected by the scenes of crime, war and human suffering? The answer would be in a societal intellectual compartmentalisation - cognitive dissonance at scale.\n“…it is far more likely that the Western democracies will dance and dream themselves into oblivion than march into it, single file and manacled… it is not necessary to conceal anything from a public insensible to contradiction and narcotized by technological diversions.” [2, pp. 128–129]\n\n\n8. Shuffle Off to Bethlehem\nIn this chapter, Postman explores how television has given a new platform to religion and how the norms of the television medium have influenced the content provided by religious programming. When AOtD was written in the 1980s, Postman observed that the most viewed religious content at the time had become focussed on dealing with the extremes of the human condition. Commercials showing people wracked with guilt or in desperate conditions and turning to the church for solace. Postman’s observation that extreme emotions translate to higher ratings was astute, predicting the trend for American TV evangelicals, of which the most viral have now become immortalised in memes rather than by the rapture that they tend to call for.\n\n&lt;a href=\"https://tenor.com/view/ken-copeland-kenneth-copeland-lil-kc-covid19-covid19preacher-gif-21109768\"&gt;Ken Copeland Kenneth Copeland GIF&lt;/a&gt;\nfrom\n&lt;a href=\"https://tenor.com/search/ken+copeland-gifs\"&gt;Ken Copeland GIFs&lt;/a&gt;\n\n\nWhich brings me to Postman’s next concern about religious show business - the unavoidable celebrity that the clerics attract. Coupled with the monetisation of modern religious viewing, a malaise of spiritual bankruptcy has been accepted in this most visible element of the modern church. Celebrity pastors become extremely wealthy while serving explosive rhetoric, using advertisement campaigns designed to prey upon misery, insecurity and vulnerability. Undoubtedly, at its worst, modern religious programming in the United States has become a caricature, a ridiculous spectacle of extremes. I note that the sort of viewing that I am selectively referring to here may not reflect all shows of this nature. In stark contrast, the BBC’s Songs of Praise could not be characterised as extreme in any way - largely due to the BBC’s public broadcasting purpose, it is not subject to the format pressures that monetisation (or arguably ratings) introduce.\nMuch of today’s religious viral content has great potential to misrepresent the defining features of its subject’s faith. To those passing viewers of no particular denomination, these shows often present a glimpse into a cast in the act of uncouth showmanship, rather than a community in the act of worship.\n\n\n9. Reach Out and Elect Someone\nA consistent thread in AotD is the change in political discourse between 1780s and 1980s America. Back in chapter 4, The Typographic Mind, Postman introduces the Lincoln-Douglas debates [6] in order to make two key points, the first being that the duration (7 hours in one sitting) and complexity of the discourse allowed a more nuanced evaluation of the positions of the candidates. The second point made was about the proposed capacity of the audience to sustain their comprehension of the complex subject matter for such an extended duration.\nIn chapter 9, Postman returns to this example in order to illustrate the decay in the public’s appetite for meaningful political discourse. Some of his points land well, with a stark contrast in recent years with the rise of populism and in particular the appalling and often provocative use of language during presidential debates appealing to the more base instincts of an audience. Yet something troubles me in the contrasting examples employed by Postman in order to make his point:\n\nHow can we say with any certainty that the attendees of the Lincoln-Douglas debates had comprehended the content?\nWhat evidence is there that the mental faculties of the Lincoln-Douglas debates’ audience were representative of the average American?\n\nThese two key questions cannot be answered by historical accounts alone. Without testing and surveying the audience immediately on exiting the debates, it would be incorrect to draw conclusions that could be generalised to a larger population. While it may be that societal and technological change may have reduced an average person’s attention span, I would not attempt to base my argument on anecdotal supposition.\nOn the subject of comparing political candidates,\n“…television makes impossible the determination of who is better than whom, if we mean by”better” such things as more capable in negotiation, more imaginative in executive skill, more knowledgeable about international affairs, more understanding of the interrelations of economic systems, and so on… For on television, the politician does not so much offer the audience an image of himself, as offer himself as an image of the audience.” [2, p. 155]\nI would agree that the limitations in television’s format constraints make for a poor vehicle for political awareness. We have seen how televised political debates become bouts of slogans and savagery. It means a great deal if a person is not allowed to take the necessary time to process and formulate a reasoned response. The rules of the game are then not in comprehension and logic. The most effective path to success would be in securing the audience’s appeal while simultaneously eroding public confidence in the competition.\nAnd on the point of how politics has; like many aspects of society; become obsessed with matters of image - where should we point the blame for this shallow obsession? The fact that Neil Kinnock falling over on Brighton Beach, Ed Miliband’s infamous bacon butty gurn or the media’s flare for capturing Theresa May’s most unflattering expressions likely sold more newspapers than any analysis of their respective parties’ manifestos ever did. In voting for a person instead of a party, are we not putting all of our political eggs in one basket?\n\n\n10. Teaching as an Amusing Activity\nOf all the different domains of society that Postman explores through his lens of discounted discourse, it was the topic of education that resonated the most with me. I should divulge that I have previously spent nearly ten years teaching high school science within the British education system.\nMy initial impression is that Postman is no fan of Sesame Street. As a child of the ‘80s, this is not a strong start. While not wholly critical of the Muppets-inspired preschool ’edutainment’ show in its potency as an entertaining pastime for preschoolers, Postman finds the precedent it sets for presenting television as a medium for education disagreeable.\nIn this chapter, the author sets out what I consider to be the most concerning element of his argument against television’s command of our culture - its ability to displace education with entertainment. This resonates with my experience in education during a period of technological advancement that must have supplanted television’s ability to interfere with intellectual advancement - the multimedia mobile phone. Postman’s technological scepticism in the 1980s may have been interpreted as antiquated at the time but to my mind this chapter is imbued with a prophetic quality.\n“…television has by its power to control the time, attention and cognitive habits of our youth gained the power to control their education. This is why I think it is accurate to call television a curriculum… whose purpose is to influence, teach, train or cultivate the mind and character of youth. Television, of course, does exactly that, and does it relentlessly. In so doing, it competes successfully with the school curriculum… it damn near obliterates it” [2, p. 169].\nThe crucial difference between the mobile phone and the traditional television that Postman opined in the ’80s is that the phone has now become a mobile television, among many other things. The need for traditional education to compete with this alternative curriculum has introduced a great deal of friction in the modern classroom, as if there were not enough already. But for a proportion of the population, the persistent availability of entertainment over education has devastating consequences. I know this because I have seen it. For many young people, education is rarely an enjoyable undertaking and preparing themselves requires discipline and endurance. Adding a constant opportunity for distraction to this situation will render a smaller proportion of our youth unable to learn anything other than the most shallow of content. One may argue that this has always been the case, which I would not refute. My position is that the proportion of people who are unable to access the National Curriculum has grown as a result of improper technological interference.\nPostman’s next point about television’s societal influence is yet more troubling. As he sets out in previous chapters, television not only presents certain standards for our intercourse, it establishes those norms for our use in society. In this context, television not only displaces education, it remodels it. On the topic of ‘dumbing down’ in educational curricula,\n“Mainly, they will have learned that learning is a form of entertainment or, more precisely, that anything worth learning can take the form of entertainment, and ought to. And they will not rebel if their English teacher asks them to learn the eight parts of speech through the medium of rock music. Or if their social studies teacher sings to them the facts about the War of 1812. Or if their physics comes to them on cookies and T-shirts. Indeed, they will expect it and thus will be well-prepared to receive their politics, their religion, their news and their commerce in the same delightful way” [2, p. 179].\nCurriculum-reform has faced a crisis in recent years - what should we teach children who now have the world’s information in their pocket? Education’s response to this development in the early part of the 20th century has been to strip the curriculum of content, to focus on the development of skills over knowledge. This has been coupled with prioritisation of skills in assessment frameworks. Our newfound ability to outsource accurate recall to devices has resulted in doctors that Google your symptoms, mechanics that Google engine components and data scientists that Google model parameters - I am compelled to disclose this last example, having successfully transitioned away from teaching to analysis in more recent years.\nHow problematic is all this change? Human recollection was never our species’ strong suit - just ask a legal professional about the fallibility of human memory. Better to outsource that to the machine, right? It is certainly convenient, and as someone who considers themselves a bit of a professional Googler, it’s undeniable that it is at times a tool for good. But is it a good idea to hand over responsibility for knowledge retrieval to the machine, or more accurately big corporations? What do we lose when we do this? What are the unseen biases and agendas in the content that is shown or censored? How are these intrinsic patterns affecting outcomes for people?\nThe shift from recall to a skills-based educational framework has assumed that the value of a human being is in synthesis - the ability to evaluate sources of information and to put it to work, composing novel content. As Large Language Models begin to reveal their potential in executing these higher-order intellectual functions, where do we go next? Technology’s advancement has the potential to disrupt our estimation of human value and priority. Will lack of practice in the creative endeavour reduce our species to manipulators of tools produced by the machine, or increase our capacity for creation - freeing us from time consuming, lower operations? I don’t know, but I can say that we will be discovering the answer to these questions in retrospect - realising the consequences; both desirable and otherwise; as we proceed through uncertainty.\n\n\n11. The Huxleyan Warning\nIn the final chapter, Postman reflects upon his argument against technology, examining the readiness of American people to adopt novel technology in spite of the potential disadvantages they may incur.\n“…a population that devoutly believes in the inevitability of progress… all Americans are Marxists, for we believe nothing if not that history is moving us toward some preordained paradise and that technology is the force behind that movement” [2, p. 183].\nThe reference to Marxism does not capture a meaningful assessment underlying the ideology. After all, Marx called for the people to intervene in the direction of their future based upon his assessment of the flaws in prevalent capitalist societies. This would not presume progress as the default position, although Marx did suggest that a system of rules limiting personal gain at the expense of others could result in a form of utopia if properly administered. I presume Postman’s comment to be a provocation of the intended audience, potentially born out of frustration at a society willing to roll the dice on progress through technology.\n“Americans will not shut down any part of their technological apparatus, and to suggest that they do so is to make no suggestion at all… Many civilized nations limit by law the amount of hours television may operate and thereby mitigate the role television plays in public life. But I believe that this is not a possibility in America” [2, p. 184].\nPostman makes an interesting point about directionality in technological innovation. When a novel product has non-trivial, immediate and tangible benefits you may expect it to be readily adopted. Once the economies of scale have been applied to said product and the reduction in financial cost has removed the significant barrier to accessing those benefits, I agree that our ability to reverse such widespread adoption is limited. Demand and profiteering form an economic valve that maintains the product evolution and consequent sales. The fact that the consequences of all this activity may be measured over a lifetime while the benefits may be observed within an instant is a dichotomy that the human mind appears poorly equipped to deal with. Technology may be our modern-day pandora’s box.\nFinally, Postman turns his attention to the computer, where he was both right and wrong,\n“Although I believe the computer to be a vastly overrated technology, I mention it here because, clearly, Americans have accorded it their customary mindless inattention; which means they will use it as they are told, without a whimper” [2, p. 187].\nHere, Postman proves that a person’s biases limit their understanding of potential. In the ’80s the position of computers in our society may have been debatable, although not to the likes of Bill Gates. It’s obvious from today’s perspective that the utility of the computer has been proven. People form interests around ideas and things that align with their values, developing foresight into potential applications as they encounter problems throughout their everyday life. Postman was wrong in his assessment of the utility of the computer, but in his subsequent statement,\n“…years from now, …it will be noticed that the massive collection and speed-of-light retrieval of data have been of great value to large-scale organizations but have solved very little of importance to most people and have created at least as many problems for them as they may have solved” [2, p. 187].\nIt is fascinating to observe this foresight from forty years ago. In the age of the multinational technology corporation, fortunes have been built upon meeting human needs. The consequent power this has produced has recently been accused of interfering; or being exploited to interfere; with democratic elections around the world. This has evolved into a power struggle for public influence between big corporations and authorities of various territories. Some twenty years after social media’s inception, governments have begun a campaign of control over social media platforms, opting to censor and police where they deem it necessary. Whether this represents infringement or protection of our rights is hotly contested. Could it be that Postman was again, partially right and wrong about the relative threats of Orwellian and Huxleyan dystopia? What if they are not mutually exclusive and what if technology enables the worst of both worlds?"
  },
  {
    "objectID": "book-reviews/03-amusing-ourselves-to-death.html#analysis-and-evaluation",
    "href": "book-reviews/03-amusing-ourselves-to-death.html#analysis-and-evaluation",
    "title": "Amusing Ourselves to Death",
    "section": "Analysis and Evaluation",
    "text": "Analysis and Evaluation\nIt would be tempting to dismiss Postman’s criticisms of television culture as an antiquated resistance to progress - this would be an ignorant treatment of his argument. Technological advancement has demonstrated its ability to transform our culture. As a species we are locked into a continual compromise of how to implement immediate benefits while avoiding the longer-term costs. When a technology’s consequences are transformational; as was the case with the printing press, the lightbulb, the nuclear bomb and the multimedia device; might it be best to proceed with caution, allowing time for a more meaningful cost-benefit analysis?\nIn the Imperative of Responsibility [7], Hans Jonas considers an alternative ethical framework, designed to hold those responsible of profiting from technological enterprise responsible for any remote and unforeseen consequences of its implementation. This principle is known as the Precautionary Principle and has garnered equal support and derision in academic literature. Authorities around the world have enacted the Precautionary Principle, establishing policy, regulation and; where identification of the causes of environmental degradation can be evidenced; to pursue justice.\nCritics of the Precautionary Principle would argue that it significantly raises the bar to human progress and that this presents an ethical quandary. Could you imagine an alternative reality where legislation prevented mass adoption of the light bulb pending a full review of the environmental and societal consequences? How would our world have been limited by the lack of access to this invention? And then again, how would our world have changed if we had paused for more information prior to rolling out thalidomide to expecting mothers?\nTo undertake an assessment of the full impact of Edison’s filament lamp on society would have been impossible at that time. It would have required an evaluation of innovations that had yet to exist and processes that were yet to be understood. This would include a multitude of missing jigsaw pieces, such as a national-scale power grid (that would only come to be some half a century after Edison perfected his initial design), an awareness of the greenhouse effect and how electricity generated by fossil fuels contribute to this (legitimacy still argued in some circles), the study of human circadian rhythm and how this would be disrupted by the invention, an evaluation of the impact of light pollution and so on. At the same time, the case for the technology would need to be evidenced in greater measure. Evidencing the benefits to productivity, safety, leisure and wellbeing that mass adoption of the light bulb would induce would have been a monumental undertaking. This is to say nothing of the consequential innovations that either required or were inspired by the filament lamp, or that in time we would improve it, producing brighter, more efficient devices that would mitigate much of that initial risk yet cause new environmental consequences related to the use of plastics and production and safe disposal of semiconductors.\nThe effort described above may have delayed the mass rollout of the lightbulb by half a century or more. Time therein where the world literally sits in the dark, awaiting the bureaucracy to arrive at a decision. Preparing the world for a transformational technology may be considered an intractable problem. The Collingridge dilemma states that until a technology has been embedded in a society, there is not enough information to provide an informed assessment of its risk, and then as a consequence of the widespread adoption, an inability to effectively regulate that technology. That doesn’t mean we get to wash our hands of the responsibility of doing this - not at all. A full and unbiased evaluation of the consequences of innovation that is proportionate to the current evidence of risk would be a vast improvement over a system accelerated by profit and disregard for all else. Political lobbying, the reproducibility crisis in many fields of research and our chronic inability to identify and prosecute fraudulent behaviour have resulted in an economy where the cost of hindering technological progress outweighs the consequences to its proliferation. To put it bluntly, we have a prenatal societal capacity to defer gratification.\nWith the scales tipped in favour of progress above all else, the ramifications for a society at the inception of generative artificial intelligence are clear. As artificial general intelligence now appears more feasible than ever, many people of influence are involved in a rich ethical debate.\n\n\n\nIn AI, the ratio of attention on hypothetical, future, forms of harm to actual, current, realized forms of harm seems out of whack. Many of the hypothetical forms of harm, like AI \"taking over\", are based on highly questionable hypotheses about what technology that does not…\n\n— Andrew NgDecember 18, 2023\n\n\n\n\nEarlier this week, POTUS Biden issued an executive order on artificial intelligence – a breakthrough technology that has the power to change the world in ways we’re only beginning to understand.I wanted to share some of the books, articles, and podcasts that have helped shape…\n\n— Barack ObamaNovember 3, 2023\n\n\n\nWhether or not the future risks of generative AI will be well-managed will be based upon the conversations that we have today. At this juncture in our history, the role of the sceptic may be more vital than ever. It is not enough to sway opinion by preaching calamity or pigeonholing criticism as antiquated techno-scepticism. Finding effective modes of discourse to help promulgate AI awareness is the route to ethically achieve the integration of this latest transformative technology. The only means to navigate this milestone must be to deepen our understanding of the technology and each other. To this end I would encourage the use of open source AI tools, to discuss the implications, benefits, limitations, biases and drawbacks. Educate yourself on the direction of the research and to attempt to stay abreast of the news in the sector, while avoiding tabloid sensationalism."
  },
  {
    "objectID": "book-reviews/01-deep-work.html",
    "href": "book-reviews/01-deep-work.html",
    "title": "Deep Work",
    "section": "",
    "text": "Fanal Forest, Madeira. Wikimedia Creative Commons."
  },
  {
    "objectID": "book-reviews/01-deep-work.html#introduction",
    "href": "book-reviews/01-deep-work.html#introduction",
    "title": "Deep Work",
    "section": "Introduction",
    "text": "Introduction\nAt the date of writing this review, Deep Work scores an average of 4.19 / 5.00 across over 141k ratings on goodreads. Also nominated by the same website for Best Nonfiction, 2016. Awarded gongs for best book and best seller in business & leadership by Amazon & the Wall Street Journal.\nThis book was recommended to me by a close colleague and I read it on holidays during some much-needed downtime in the Summer of 2023. I should note that I reflect on the relevance of the advice in this book to my own personal circumstances. To that end, it is worth noting that I currently work as a senior data scientist and that I happen to face many competing priorities in my day-to-day job.\nCal Newport is a celebrated author within the field of personal productivity. According to his website, Newport is an MIT Grad in Computer Science and a professor at Georgetown University [1]."
  },
  {
    "objectID": "book-reviews/01-deep-work.html#an-overview-of-the-book",
    "href": "book-reviews/01-deep-work.html#an-overview-of-the-book",
    "title": "Deep Work",
    "section": "An Overview of the Book",
    "text": "An Overview of the Book\nDeep Work is structured in 2 parts:\n\nPart 1: The Idea\nHere the author sets out his stall, establishing definitions of fundamental concepts, such as deep and shallow work and their relative value to the author’s definition of what he refers to as the ‘knowledge worker’. The author sets out noteworthy examples of individuals who (have) had a proven ability to establish deep thought throughout their illustrious careers. The author then sets out the case for why deep work is a rewarding endeavor and how its antithesis of a connected and responsive workday offers little benefit to the individual, the organisation or society. Yet , many modern organisations have unwittingly promoted and even entrenched a shallow work culture among their employees.\n\n\nPart 2: The Rules\nIn this section, the author offers suggested interventions to make within your workplace. Exemplifying strategies with personal experience and from those around him, Newport assists the reader in visualising a professional future where they can rule their schedule, rather than the reverse.\n\n\n\n\n\n\nNote\n\n\n\n“… remain unresponsive to the pinprick onslaught of small obligations that seem harmless in isolation but aggregate to serious injury to his deep work habit.” [2, p. 142]\n\n\nThe author identifies potential sticking points and attempts to treat them, offering suggested strategies for securing the support of colleagues, stakeholders and managers in the reader’s pursuit of deep work. Moderating his message, Newport offers words of advice against the pursuit of deep work to the exclusion of everything else, yet establishing set constraints within a schedule where shallow tasks and administrative commitments are not permitted to undo your daily objectives."
  },
  {
    "objectID": "book-reviews/01-deep-work.html#analysis-and-evaluation",
    "href": "book-reviews/01-deep-work.html#analysis-and-evaluation",
    "title": "Deep Work",
    "section": "Analysis and Evaluation",
    "text": "Analysis and Evaluation\nMuch of the content in this book resonated with me. Newport takes a contrarian view of many modern professional practices, such as structuring your day around group ceremonies like stand ups or scrums, maintaining open channels of communication via Email and instant messaging applications, and as mentioned above, social media channels. Many I.T. professionals will find their progress hampered by a menagerie of ceremonies, administrative tasks and meetings of ambiguous impact. Understanding how to categorise such meetings and structure your day around your deep work goals is what Newport hopes to espouse. This has been a consistent feature of my data-related roles throughout my career. Striking that balance of engaging with stakeholders versus undertaking the work can all too often result in a fractured day with little developmental progress.\nThat’s not to say that Newport promotes a hermetic approach to work. It’s clear to most that the tool, statistic or analysis that is being developed can only be effective if it is useful. Newport states that these ‘shallow’ tasks should be kept in check. They should be minimised and must earn your attention. Placing a premium on your time and attention is at the core of the book’s message.\n\n\n\n\n\n\nNote\n\n\n\n“…for decisions that involve large amounts of information and multiple vague, and perhaps even conflicting restraints, your unconscious mind is well suited to tackle the issue.” [2, p. 145]\n\n\n\nAbove, Newport summarises a high level takeaway from unconscious thought theory (UTT), which may present a theoretical basis for sleeping on a particularly challenging problem.\n\nA lot of what Newport writes has rung true in my experience. While not all jobs have the same requirements, my roles in data have all required a deep focus disposition. In fact, some of the strategies that the author suggests for optimising your day, I had independently arrived at. Approaches such as starting your day before the rest of the office has woken up and keeping Email responses process-related have both served me well in my career progression so far. But, there are some suggested strategies that I have not quite been convinced of. At one point, Newport discusses cognitive strategies for extending your capacity for deep concentration. The suggested approach is to remember the order of a shuffled deck of cards. Newport outlines a method for doing so and cites some studies that are indicative of the positive relationship between such activities and the mental fortitude required for deep focus. Personally, I would sacrifice too much time in achieving such a feat, whereas I would gain more by implementing the task at hand, gradually incrementing my exposure to deep concentration sessions. Newport does state that this sort of thinking misses the point entirely, and that investing effort in such a mental feat pays dividends in the longer term, though I have yet to be convinced of this claim.\n\n\n\n\n\n\nNote\n\n\n\n“…attention restoration theory (ART), which claims that spending time in nature can improve your ability to concentrate. This theory… is based on the concept of attention fatigue. To concentrate requires what ART calls directed attention. This resource is finite: If you exhaust it, you’ll struggle to concentrate.” [2, p. 147]\n\n\n\nI found this concept to be very interesting, and a little concerning. Managing burnout is something that I value. It seems that my pasttimes also require directed attention, and that perhaps I should pursue recreational activities that contrast with my professional interests. I have decided to note within my journal when I notice that I am lacking energy and to resolve to take a bike ride, swim or just a walk in order to efefctively disconnect and recharge.\n\nAt times throughout the book, I did find myself reflecting that some of the claims or advice were not as generalisable as the author had perceived. At one point for example, the Newport suggests diagnosing whether your workplace can support a culture of deep concentration and finding alternative employment if the answer was a clear enough ‘no’. This did strike me as something that many colleagues could only dream of, with the reality of bills to pay, children to feed and job security in an uncertain sector. Advising people to leave rather than finding alternative strategies to influence managerial culture does seem a bit thoughtless.\n\n\n\n\n\n\nNote\n\n\n\n“Professional E-mail Sorting: Do not reply to an e-mail message if any of the following applies:\n\nIt’s ambiguous or otherwise makes it hard for you to generate a reasonable response.\nIt’s not a question or proposal that interests you.\nNothing really good would happen if you did respond and nothing really bad would happen if you didn’t.” [2, p. 255]\n\n\n\n\nI find this advice to be a bit myopic. This is likely great advice for an accomplished academic with a strong internal locus of control. Junior entrants to a profession typically need to prove their utility to those with greater power around them - the decision makers that could be on their next promotion panel. Ignoring Emails that are a bit ambiguous could result in passing up an oportunity to help a colleague, forge a new professional relationship or prove your worth in the workplace. Rather than approaching the needs of others with derision, I would instead advise time-boxing a response and communicating your needs clearly to the other party. Something like - ‘I’m a bit pushed for time right now and am not familiar with the context, but if you can point me to a briefing note I can get back to you on Monday, once I’ve cleared off my backlog’.\n\nAnother feature of this book which that requires some scepticism is in the outcomes of studies used to underpin the attitudes and strategies promoted as good practice. It’s likely no news to you that the behavioural sciences has attracted a fair amount of criticism about the reproducibility of published findings over the last decade. Coupled with the widespread publication bias encountered in academic journals [3] and confirmation bias abound in the field of psychology, I would suggest the author reveal some of the research that they have undertaken in validating the claims of the studies. It is unlikely that the average reader will have the necessary time to carry out their own investigation of the findings, and doubt in the ability to evidence some of the claims within the book is introduced. I do not wish to sound like I am criticising the author for choosing studies that help to evidence their claims, as I would have criticised them for not doing so had that been the case. I would ask that the author publishes their attempts to prove the hypotheses considered.\n\n“Don’t trust everything you read in the psychology literature. In fact, two thirds of it should probably be distrusted.” [4]"
  },
  {
    "objectID": "book-reviews/01-deep-work.html#comparisons",
    "href": "book-reviews/01-deep-work.html#comparisons",
    "title": "Deep Work",
    "section": "Comparisons",
    "text": "Comparisons\nThere are some obvious comparisons with other productivity self-help books. One of the most obvious is in Stephen Covey’s The 7 Habits of Highly Effective People [5]. Both books make an effort to discuss the management of competing priorities and how to go about organising your working day around prioritised, impactful goals. Covey’s book dedicates more energy towards effectively triaging tasks and charting progress made against longer-term priorities than what is explored within Deep Work. In fact, this may be another presumption made by Newport - that the direction of progress is obvious to the reader and it is simply a matter of finding the undisturbed time to do the work. As many of us will encounter uncertainty and a fair amount of strategic thrashing in our working lives, this could prove to be a limiting assumption.\nBelow is an example of what is commonly referred to as a “Covey Quadrant”, a rubric for efficiently categorising tasks along dimensions of urgency and importance."
  },
  {
    "objectID": "book-reviews/01-deep-work.html#recommendation",
    "href": "book-reviews/01-deep-work.html#recommendation",
    "title": "Deep Work",
    "section": "Recommendation",
    "text": "Recommendation\nThere is a lot of wisdom to be gleaned from this book. Sure, some of it may be anecdotal and won’t be true in all cases. But there is an undeniable pattern in the examples provided, that sustained effort yields desirable outcomes. This is something that I have always found to be true and connects with my wider values. It is this message and the useful reminders and strategies that would form the basis of my recommendation for this book. This book is an opportunity to gain insight into how the elite of the achievers in society get things done. You may be able to make some of this work for you but don’t expect it to be easy, or perhaps even achievable in your current situation.\nTo put it bluntly, if you have ever struggled to quiet the noise at work and get things done, then this book is for you. (It’s not lost on me that that may qualify pretty much everyone who has ever worked in an office)."
  },
  {
    "objectID": "blogs/14-gh-actions-security.html",
    "href": "blogs/14-gh-actions-security.html",
    "title": "GitHub Actions Security",
    "section": "",
    "text": "An android locking a high security vault door."
  },
  {
    "objectID": "blogs/14-gh-actions-security.html#tldr",
    "href": "blogs/14-gh-actions-security.html#tldr",
    "title": "GitHub Actions Security",
    "section": "TL;DR",
    "text": "TL;DR\nIt’s more secure to reference GitHub Actions written by others by referring to the commit hash of the code than the version. Particularly if the action requires access to a secret credential. For example:\n\n\nupload-codecov.yml\n\nsteps:\n- uses: actions/checkout@main\n- uses: codecov/codecov-action@v4\n  with:\n    token: ${{ secrets.CODECOV_TOKEN }} # required\n\n…is more safely referenced like below:\n\n\nupload-codecov.yml\n\nsteps:\n- uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332 # sha for v4.1.7\n- uses: codecov/codecov-action@af2ee03a4e3e11499d866845a1e6c5a11f85cf4e # sha v4.5.0 \n  with:\n    token: ${{ secrets.CODECOV_TOKEN }} # required"
  },
  {
    "objectID": "blogs/14-gh-actions-security.html#background",
    "href": "blogs/14-gh-actions-security.html#background",
    "title": "GitHub Actions Security",
    "section": "Background",
    "text": "Background\n\nOpen source projects have been subject to a number of social engineering attacks recently.\nThis blog post describes risk in relying on GitHub Actions written by others."
  },
  {
    "objectID": "blogs/14-gh-actions-security.html#the-risk-in-plain-english",
    "href": "blogs/14-gh-actions-security.html#the-risk-in-plain-english",
    "title": "GitHub Actions Security",
    "section": "The Risk in Plain English",
    "text": "The Risk in Plain English\n\nDevelopers use GitHub Actions to conveniently automate various tasks. These Actions are often written and maintained by helpful members of the open-source community.\nSome of these Actions require access to secret credentials, for example to publish code to services in the cloud.\nMany of these Actions are widely adopted in the open source community, handling thousands of secrets every day. Developers consider the wide adoption of an Action when deciding whether to trust it.\nThere is a risk that bad actors could gain access to an Action’s code by placing pressure upon the package maintainers.\nOnce a bad actor has access to this code, they are able to adjust the code associated with the version reference, to do something nefarious, such as harvest secret credentials.\nUpdating workflow files to reference the Actions by commit hash guarantees that this code can be trusted to ‘do what it says on the tin’ in the future. Bad actors are unable to create new code that has the same commit reference code (known as SHA, short for Simple Hashing Algorithm)."
  },
  {
    "objectID": "blogs/14-gh-actions-security.html#updating-your-workflow-files",
    "href": "blogs/14-gh-actions-security.html#updating-your-workflow-files",
    "title": "GitHub Actions Security",
    "section": "Updating Your Workflow Files",
    "text": "Updating Your Workflow Files\n\n\n\n\n\n\nCaution\n\n\n\nThe layout of the GitHub user interface is liable to change.\n\n\n\nFind the Action that you wish to use on GitHub and click on its banner to navigate to the Action’s homepage.\n\n\n\n\nThe GitHub Actions marketplace\n\n\n\nEnsure that you have identified the correct Action by checking the author and the number of stars. Click on the link shown to navigate to the Action’s repository.\n\n\n\n\nThe Codecov Action homepage\n\n\n\nConsult the readme and releases section to identify the latest version of the Action. Ensure that the appropriate branch is selected from the branch selector widget.\n\n\n\n\nChecking the selected branch\n\n\n\nOnce you have selected the appropriate branch for the latest release, click on the commits section.\n\n\n\n\nClick on the commits\n\n\n\nFrom the list of commits, select the commit associated with the latest release and click the copy button to copy the full commit SHA reference to your clipboard.\n\n\n\n\nCopy the full commit sha\n\n\n\nReplace the version reference in your workflow file with your copied commit reference."
  },
  {
    "objectID": "blogs/14-gh-actions-security.html#likelihood-and-severity",
    "href": "blogs/14-gh-actions-security.html#likelihood-and-severity",
    "title": "GitHub Actions Security",
    "section": "Likelihood and Severity",
    "text": "Likelihood and Severity\nThe likelihood of this risk is debatable, but it is not zero.\nThe community of developers would likely pick up on such an event quickly and many of the targeted Action’s users would benefit from this awareness before any damage could be wrought. But why would you roll the dice? It may be equally possible that you and your colleagues may not realise this event has occurred before it was too late to intervene.\nThe severity of such an event would relate to the nature of the targeted Action. If said Action intercepted cloud service credentials, such as those required for deployment to GCP, AWS and the like, then the severity could be high. Whereas targeting widely used Actions such as the checkout Action would present differing severities for public or private repositories that use it.\nAs the effort needed to mitigate this risk is very small, I would encourage GitHub Actions users to consider updating all repositories that make use of such Actions for their continuous deployment workflows. In support of this suggested mitigation, here you can see that the well-known python package numpy have adopted this approach."
  },
  {
    "objectID": "blogs/14-gh-actions-security.html#acknowledgements",
    "href": "blogs/14-gh-actions-security.html#acknowledgements",
    "title": "GitHub Actions Security",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Mat for the conversation about recent bad actor efforts on various well-known open source repositories.\n\nfin!"
  },
  {
    "objectID": "blogs/12-pytest-tmp-path.html",
    "href": "blogs/12-pytest-tmp-path.html",
    "title": "Pytest With tmp_path in Plain English",
    "section": "",
    "text": "Creative commons license by Ralph"
  },
  {
    "objectID": "blogs/12-pytest-tmp-path.html#introduction",
    "href": "blogs/12-pytest-tmp-path.html#introduction",
    "title": "Pytest With tmp_path in Plain English",
    "section": "Introduction",
    "text": "Introduction\npytest is a testing package for the python framework. It is broadly used to quality assure code logic. This article discusses why and how we use pytest’s temporary fixtures tmp_path and tmp_path_factory. This blog is the second in a series of blogs called pytest in plain English, favouring accessible language and simple examples to explain the more intricate features of the pytest package.\nFor a wealth of documentation, guides and how-tos, please consult the pytest documentation.\n\n\n\n\n\n\nA Note on the Purpose (Click to expand)\n\n\n\n\n\nThis article intends to discuss clearly. It doesn’t aim to be clever or impressive. Its aim is to extend understanding without overwhelming the reader.\n\n\n\n\nIntended Audience\nProgrammers with a working knowledge of python and some familiarity with pytest and packaging. The type of programmer who has wondered about how to follow best practice in testing python code.\n\n\nWhat You’ll Need:\n\nPreferred python environment manager (eg conda)\npip install pytest==8.1.1\nGit\nGitHub account\nCommand line access\n\n\n\nPreparation\nThis blog is accompanied by code in this repository. The main branch provides a template with the minimum structure and requirements expected to run a pytest suite. The repo branches contain the code used in the examples of the following sections.\nFeel free to fork or clone the repo and checkout to the example branches as needed.\nThe example code that accompanies this article is available in the temp-fixtures branch of the repo."
  },
  {
    "objectID": "blogs/12-pytest-tmp-path.html#what-are-temporary-fixtures",
    "href": "blogs/12-pytest-tmp-path.html#what-are-temporary-fixtures",
    "title": "Pytest With tmp_path in Plain English",
    "section": "What Are Temporary Fixtures?",
    "text": "What Are Temporary Fixtures?\nIn the previous pytest in plain English article, we discussed how to write our own fixtures to serve data to our tests. But pytest comes with its own set of fixtures that are really useful in certain situations. In this article, we will consider those fixtures that are used to create temporary directories and files.\n\nWhy Do We Need Temporary Fixtures?\nIf the code you need to test carries out file operations, then there are a few considerations needed when writing our tests. It is best practice in testing to ensure the system state is unaffected by running the test suite. In the very worst cases I have encountered, running the test suite has resulted in timestamped csvs being written to disk every time pytest was run. As developers potentially run these tests hundreds of times while working on a code base, this thoughtless little side-effect quickly results in a messy file system.\nJust to clarify - I’m not saying it’s a bad idea to use timestamped file names. Or to have functions with these kinds of side effects - these features can be really useful. The problem is when the test suite creates junk on your disk that you weren’t aware of…\nBy using temporary fixtures, we are ensuring the tests are isolated from each other and behave in dependable ways. If you ever encounter a test suite that behaves differently on subsequent runs, then be suspicious of a messy test suite with file operations that have changed the state of the system. In order for us to reason about the state of the code, we need to be able to rely on the answers we get from the tests, known in test engineering speak as determinism.\n\n\nLet’s Compare the Available Temporary Fixtures\nThe 2 fixtures that we should be working with as of 2024 are tmp_path and tmp_path_factory. Both of these newer temporary fixtures return pathlib.Path objects and are included with the pytest package in order to encourage developers to use them. No need to import tempfile or any other dependency to get what you need, it’s all bundled up with your pytest installation.\ntmp_path is a function-scoped fixture. Meaning that if we use tmp_path in 2 unit tests, then we will be served with 2 separate temporary directories to work with. This should meet most developers’ needs. But if you’re doing something more complex with files, there are occasions where you may need a more persistent temporary directory. Perhaps a bunch of your functions need to work sequentially using files on disk and you need to test how all these units work together. This kind of scenario can arise if you are working on really large files where in-memory operations become too costly. This is where tmp_path_factory can be useful, as it is a session-scoped temporary structure. A tmp_path_factory structure will be created at the start of a test suite and will persist until teardown happens once the last test has been executed.\n\n\n\nFixture Name\nScope\nTeardown after each\n\n\n\n\ntmp_path\nfunction\ntest function\n\n\ntmp_path_factory\nsession\npytest session\n\n\n\n\n\nWhat About tmpdir?\nAh, the eagle-eyed among you may have noticed that the pytest package contains other fixtures that are relevant to temporary structures. Namely tmpdir and tmpdir_factory. These fixtures are older equivalents of the fixtures we discussed above. The main difference is that instead of returning pathlib.Path objects, they return py.path.local objects. These fixtures were written before pathlib had been adopted as the standardised approach to handling paths across multiple operating systems. The future of tmpdir and tmpdir_factory have been discussed for deprecation. These fixtures are being sunsetted and it is advised to port old test suites over to the new tmp_path fixture instead. The pytest team has provided a utility to help developers identify these issues in their old test suites.\nIn summary, don’t use tmpdir any more and consider converting old code if you used it in the past…"
  },
  {
    "objectID": "blogs/12-pytest-tmp-path.html#how-to-use-temporary-fixtures",
    "href": "blogs/12-pytest-tmp-path.html#how-to-use-temporary-fixtures",
    "title": "Pytest With tmp_path in Plain English",
    "section": "How to Use Temporary Fixtures",
    "text": "How to Use Temporary Fixtures\n\nWriting Source Code\nAs a reminder, the code for this section is located here.\nIn this deliberately silly example, let’s say we have a poem sitting on our disk in a text file. Thanks to chatGPT for the poem and MSFT Bing Copilot for the image, making this a trivial consideration. Or should I really thank the millions of people who wrote the content that these services trained on?\nSaving the text file in the chunk below to the ./tests/data/ folder is where you would typically save data for your tests.\n\n\n\ntests/data/jack-jill-2024.txt\n\nIn the realm of data, where Jack and Jill dwell,\nThey ventured forth, their tale to tell.\nBut amidst the bytes, a glitch they found,\nA challenge profound, in algorithms bound.\n\nTheir circuits whirred, their processors spun,\nAs they analyzed the glitch, one by one.\nYet despite their prowess, misfortune struck,\nA bug so elusive, like lightning struck.\n\nTheir systems faltered, errors abound,\nAs frustration grew with each rebound.\nBut Jack and Jill, with minds so keen,\nRefused to let the glitch remain unseen.\n\nWith perseverance strong and logic clear,\nThey traced the bug to its hidden sphere.\nAnd with precision fine and code refined,\nThey patched the glitch, their brilliance defined.\n\nIn the end, though misfortune came their way,\nJack and Jill triumphed, without delay.\nFor in the realm of AI, where challenges frown,\nTheir intellect prevailed, wearing victory's crown.\n\nSo let their tale inspire, in bytes and code,\nWhere challenges rise on the digital road.\nFor Jack and Jill, with their AI might,\nShowed that even in darkness, there's always light.\n\nLet’s imagine we need a program that can edit the text and write new versions of the poem to disk. Let’s go ahead and create a function that will read the poem from disk and replace any word that you’d like to change.\n\n\"\"\"Demonstrating tmp_path & tmp_path_factory with a simple txt file.\"\"\"\nfrom pathlib import Path\nfrom typing import Union\n\ndef _update_a_term(\n    txt_pth: Union[Path, str], target_pattern:str, replacement:str) -&gt; str:\n    \"\"\"Replace the target pattern in a body of text.\n\n    Parameters\n    ----------\n    txt_pth : Union[Path, str]\n        Path to a txt file.\n    target_pattern : str\n        The pattern to replace.\n    replacement : str\n        The replacement value.\n\n    Returns\n    -------\n    str\n        String with any occurrences of target_pattern replaced with specified\n        replacement value.\n\n    \"\"\"\n    with open(txt_pth, \"r\") as f:\n        txt = f.read()\n        f.close()\n    return txt.replace(target_pattern, replacement)\n\nNow we can try using the function to rename a character in the rhyme, by running the below code in a python shell.\n\nfrom pyprojroot import here\nrhyme = _update_a_term(\n  txt_pth=here(\"data/blogs/jack-jill-2024.txt\"),\n  target_pattern=\"Jill\",\n  replacement=\"Jock\")\nprint(rhyme[0:175])\n\nIn the realm of data, where Jack and Jock dwell,\nThey ventured forth, their tale to tell.\nBut amidst the bytes, a glitch they found,\nA challenge profound, in algorithms bound.\n\n\n\n\n\n\n\n\nWhy Use Underscores?\n\n\n\n\n\nYou may have noticed that the above function starts with an underscore. This convention means the function is not intended for use by the user. These internal functions would typically have less defensive checks than those you intend to expose to your users. It’s not an enforced thing but is considered good practice. It means “use at your own risk” as internals often have less documentation, may not be directly tested and could be less stable than functions in the api.\n\n\n\nGreat, next we need a little utility function that will take our text and write it to a file of our choosing.\n\ndef _write_string_to_txt(some_txt:str, out_pth:Union[Path, str]) -&gt; None:\n    \"\"\"Write some string to a text file.\n\n    Parameters\n    ----------\n    some_txt : str\n        The text to write to file.\n    out_pth : Union[Path, str]\n        The path to the file.\n    \n    Returns\n    -------\n    None\n\n    \"\"\"\n    with open(out_pth, \"w\") as f:\n        f.writelines(some_txt)\n        f.close()    \n\nFinally, we need a wrapper function that will use the above functions, allowing the user to read in the text file, replace a pattern and then write the new poem to file.\n\ndef update_poem(\n    poem_pth:Union[Path, str],\n    target_pattern:str,\n    replacement:str,\n    out_file:Union[Path, str]) -&gt; None:\n    \"\"\"Takes a txt file, replaces a pattern and writes to a new file.\n\n    Parameters\n    ----------\n    poem_pth : Union[Path, str]\n        Path to a txt file.\n    target_pattern : str\n        A pattern to update.\n    replacement : str\n        The replacement value.\n    out_file : Union[Path, str]\n        A file path to write to.\n\n    \"\"\"\n    txt = _update_a_term(poem_pth, target_pattern, replacement)\n    _write_string_to_txt(txt, out_file)\n\nHow do we know it works? We can use it and observe the output, as I did with _update_a_term() earlier, but this article is about testing. So let’s get to it.\n\n\nTesting the Source Code\nWe need to test update_poem() but it writes files to disk. We don’t want to litter our (and our colleagues’) disks with files every time pytest runs. Therefore we need to ensure the function’s out_file parameter is pointing at a temporary directory. In that way, we can rely on the temporary structure’s behaviour on teardown to remove these files when pytest finishes doing its business.\n\n\"\"\"Tests for update_poetry module.\"\"\"\nimport os\n\nimport pytest\n\nfrom example_pkg import update_poetry\n\ndef test_update_poem_writes_new_pattern_to_file(tmp_path):\n    \"\"\"Check that update_poem changes the poem pattern and writes to file.\"\"\"\n    new_poem_path = os.path.join(tmp_path, \"new_poem.txt\")\n    update_poetry.update_poem(\n        poem_pth=\"tests/data/jack-jill-2024.txt\",\n        target_pattern=\"glitch\",\n        replacement=\"bug\",\n        out_file=new_poem_path\n        )\n\nBefore I go ahead and add a bunch of assertions in, look at how easy it is to use tmp_path, blink and you’ll miss it. You simply reference it in the signature of the test where you wish to use it and then you are able to work with it like you would any other path object.\nSo far in this test function, I specified that I’d like to read the text from a file called jack-jill-2024.txt, replace the word “glitch” with “bug” wherever it occurs and then write this text to a file called new_poem.txt in a temporary directory.\nSome simple tests for this little function:\n\nDoes the file I asked for exist?\nAre the contents of that file as I expect?\n\nLet’s go ahead and add in those assertions.\n\n\"\"\"Tests for update_poetry module.\"\"\"\n\nimport os\n\nimport pytest\n\nfrom example_pkg import update_poetry\n\ndef test_update_poem_writes_new_pattern_to_file(tmp_path):\n    \"\"\"Check that update_poem changes the poem pattern and writes to file.\"\"\"\n    new_poem_path = os.path.join(tmp_path, \"new_poem.txt\")\n    update_poetry.update_poem(\n        poem_pth=\"tests/data/jack-jill-2024.txt\",\n        target_pattern=\"glitch\",\n        replacement=\"bug\",\n        out_file=new_poem_path\n        )\n    # Now for the assertions\n    assert os.path.exists(new_poem_path)\n    assert os.listdir(tmp_path) == [\"new_poem.txt\"]\n    # let's check what pattern was written - now we need to read in the\n    # contents of the new file.\n    with open(new_poem_path, \"r\") as f:\n        what_was_written = f.read()\n        f.close()\n    assert \"glitch\" not in what_was_written\n    assert \"bug\" in what_was_written\n\nRunning pytest results in the below output.\ncollected 1 item\n\ntests/test_update_poetry.py .                                            [100%]\n\n============================== 1 passed in 0.01s ==============================\nSo we prove that the function works how we hoped it would. But what if I want to work with the new_poem.txt file again in another test function? Let’s add another test to test_update_poetry.py and see what we get when we try to use tmp_path once more.\n\n\"\"\"Tests for update_poetry module.\"\"\"\n# import statements ...\n\n# def test_update_poem_writes_new_pattern_to_file(tmp_path): ...\n\ndef test_do_i_get_a_new_tmp_path(tmp_path):\n    \"\"\"Remind ourselves that tmp_path is function-scoped.\"\"\"\n    assert \"new_poem\" not in os.listdir(tmp_path)\n    assert os.listdir(tmp_path) == []\n\nAs is demonstrated when running pytest once more, tmp_path is function-scoped and we have now lost the new poem with the bugs instead of the glitches. Drat! What to do…\ncollected 2 items\n\ntests/test_update_poetry.py ..                                           [100%]\n\n============================== 2 passed in 0.01s ==============================\n\nAs mentioned earlier, pytest provides another fixture with more flexibility, called tmp_path_factory. As this fixture is session-scoped, we can have full control over this fixture’s scoping.\n\n\n\n\n\n\nFixture Scopes\n\n\n\n\n\nFor a refresher on the rules of scope referencing, please see the blog Pytest Fixtures in Plain English.\n\n\n\n\n\"\"\"Tests for update_poetry module.\"\"\"\n# import statements ...\n\n# def test_update_poem_writes_new_pattern_to_file(tmp_path): ...\n\n# def test_do_i_get_a_new_tmp_path(tmp_path): ...\n\n@pytest.fixture(scope=\"module\")\ndef _module_scoped_tmp(tmp_path_factory):\n    yield tmp_path_factory.mktemp(\"put_poetry_here\", numbered=False)\n\nNote that as tmp_path_factory is session-scoped, I’m free to reference it in another fixture with any scope. Here I define a module-scoped fixture, which means teardown of _module_scoped_tmp will occur once the final test in this test module completes. Now repeating the logic executed with tmp_path above, but this time with our new module-scoped temporary directory, we get a different outcome.\n\n\"\"\"Tests for update_poetry module.\"\"\"\n# import statements ...\n\n# def test_update_poem_writes_new_pattern_to_file(tmp_path): ...\n\n# def test_do_i_get_a_new_tmp_path(tmp_path): ...\n\n@pytest.fixture(scope=\"module\")\ndef _module_scoped_tmp(tmp_path_factory):\n    yield tmp_path_factory.mktemp(\"put_poetry_here\", numbered=False)\n\n\ndef test_module_scoped_tmp_exists(_module_scoped_tmp):\n    new_poem_path = os.path.join(_module_scoped_tmp, \"new_poem.txt\")\n    update_poetry.update_poem(\n        poem_pth=\"tests/data/jack-jill-2024.txt\",\n        target_pattern=\"glitch\",\n        replacement=\"bug\",\n        out_file=new_poem_path\n        )\n    assert os.path.exists(new_poem_path)\n    with open(new_poem_path, \"r\") as f:\n        what_was_written = f.read()\n        f.close()\n    assert \"glitch\" not in what_was_written\n    assert \"bug\" in what_was_written\n    assert os.listdir(_module_scoped_tmp) == [\"new_poem.txt\"]\n\n\ndef test_do_i_get_a_new_tmp_path_factory(_module_scoped_tmp):\n    assert not os.listdir(_module_scoped_tmp) == [] # not empty...\n    assert os.listdir(_module_scoped_tmp) == [\"new_poem.txt\"]\n    # module-scoped fixture still contains file made in previous test function\n    with open(os.path.join(_module_scoped_tmp, \"new_poem.txt\")) as f:\n        found_txt = f.read()\n        f.close()\n    assert \"glitch\" not in found_txt\n    assert \"bug\" in found_txt\n\nExecuting pytest one final time demonstrates that the same output file written to disk with test_module_scoped_tmp_exists() is subsequently available for further testing in test_do_i_get_a_new_tmp_path_factory().\ncollected 4 items\n\ntests/test_update_poetry.py ....                                         [100%]\n\n============================== 4 passed in 0.01s ==============================\nNote that the order that these 2 tests run in is now important. These tests are no longer isolated and trying to run the second test on its own with pytest -k \"test_do_i_get_a_new_tmp_path_factory\" would result in a failure. For this reason, it may be advisable to pop the test functions within a common test class, or even use pytest marks to mark them as integration tests (more on this in a future blog)."
  },
  {
    "objectID": "blogs/12-pytest-tmp-path.html#summary",
    "href": "blogs/12-pytest-tmp-path.html#summary",
    "title": "Pytest With tmp_path in Plain English",
    "section": "Summary",
    "text": "Summary\nThe reasons we use temporary fixtures and how to use them has been demonstrated with another silly (but hopefully relatable) little example. I have not gone into the wealth of methods available in these temporary fixtures, but they have many useful utilities. Maybe you’re working with a complex nested directory structure for example, the glob method would surely help with that.\nBelow are the public methods and attributes of tmp_path:\n['absolute', 'anchor', 'as_posix', 'as_uri', 'chmod', 'cwd', 'drive', 'exists',\n'expanduser', 'glob', 'group', 'hardlink_to', 'home', 'is_absolute',\n'is_block_device', 'is_char_device', 'is_dir', 'is_fifo', 'is_file',\n'is_junction', 'is_mount', 'is_relative_to', 'is_reserved', 'is_socket',\n'is_symlink', 'iterdir', 'joinpath', 'lchmod', 'lstat', 'match', 'mkdir',\n'name', 'open', 'owner', 'parent', 'parents', 'parts', 'read_bytes',\n'read_text', 'readlink', 'relative_to', 'rename', 'replace', 'resolve',\n'rglob', 'rmdir', 'root', 'samefile', 'stat', 'stem', 'suffix', 'suffixes',\n'symlink_to', 'touch', 'unlink', 'walk', 'with_name', 'with_segments',\n'with_stem', 'with_suffix', 'write_bytes', 'write_text'] \nIt is useful to read the pathlib.Path docs as both fixtures return this type and many of the methods above are inherited from these types. To read the tmp_path and tmp_path_factory implementation, I recommend reading the tmp docstrings on GitHub.\nIf you spot an error with this article, or have suggested improvement then feel free to raise an issue on GitHub.\nHappy testing!"
  },
  {
    "objectID": "blogs/12-pytest-tmp-path.html#acknowledgements",
    "href": "blogs/12-pytest-tmp-path.html#acknowledgements",
    "title": "Pytest With tmp_path in Plain English",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nTo past and present colleagues who have helped to discuss pros and cons, establishing practice and firming-up some opinions. Particularly:\n\nCharlie\nDan\nEdward\nIan\nMark\n\n\nfin!"
  },
  {
    "objectID": "blogs/10-pydeck-with-gpd.html",
    "href": "blogs/10-pydeck-with-gpd.html",
    "title": "Getting Pydeck to Play Nicely with GeoPandas.",
    "section": "",
    "text": "Creative commons license by Prompart"
  },
  {
    "objectID": "blogs/10-pydeck-with-gpd.html#introduction",
    "href": "blogs/10-pydeck-with-gpd.html#introduction",
    "title": "Getting Pydeck to Play Nicely with GeoPandas.",
    "section": "Introduction",
    "text": "Introduction\nPydeck is a python client for pydeck.gl, a powerful geospatial visualisation library. It’s a relatively new library and integrating it with the existing python geospatial ecosystem is currently a little tricky. This article demonstrates how to build pydeck ScatterplotLayer and GeoJsonLayer from geopandas GeoDataFrames.\n\nPydeck documentation\nDeck.gl documentation\n\n\n\n\n\n\n\nA Note on the Purpose\n\n\n\n\n\nThe content of this article was written using pydeck 0.8.0. Future releases may alter the package behaviour.\n\n\n\n\nIntended Audience\nPython practitioners familiar with virtual environments, requests and geospatial analysis with geopandas.\n\n\nThe Scenario\nYou have a geopandas GeoDataFrame with point or polygon geometries. You are attempting to build a pydeck visualisation but end up with empty basemap tiles.\n\n\nWhat You’ll Need:\n\nPreferred python environment manager (eg conda)\nPython package manager (eg pip)\nrequirements.txt:\n\n\n\nrequirements.txt\n\ngeopandas\npandas\npydeck\nrequests"
  },
  {
    "objectID": "blogs/10-pydeck-with-gpd.html#prepare-environment",
    "href": "blogs/10-pydeck-with-gpd.html#prepare-environment",
    "title": "Getting Pydeck to Play Nicely with GeoPandas.",
    "section": "Prepare Environment",
    "text": "Prepare Environment\n\nCreate a virtual environment.\nInstall the required dependencies.\nActivate the virtual environment.\nCreate a python script and import the dependencies.\n\n\nimport json\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport pydeck as pdk\nimport requests\nfrom sklearn import preprocessing"
  },
  {
    "objectID": "blogs/10-pydeck-with-gpd.html#build-a-scatterplotlayer",
    "href": "blogs/10-pydeck-with-gpd.html#build-a-scatterplotlayer",
    "title": "Getting Pydeck to Play Nicely with GeoPandas.",
    "section": "Build a ScatterplotLayer",
    "text": "Build a ScatterplotLayer\n\nIngest Data\nFor the point data, I will ingest all Welsh lower super output area 2021 population-weighted centroids from ONS Open Geography Portal.\nFor more on working with ONS Open Geography Portal, see Getting Data from ONS Open Geography Portal.\n\n\nShow the code\nENDPOINT = \"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/LLSOA_Dec_2021_PWC_for_England_and_Wales_2022/FeatureServer/0/query\"\nPARAMS = {\n    \"where\": \"LSOA21CD like 'W%'\",\n    \"f\": \"geoJSON\", \n    \"outFields\": \"*\",\n    \"outSR\": 4326,\n}\nresp = requests.get(ENDPOINT, params=PARAMS)\nif resp.ok:\n    content = resp.json()\nelse:\n    raise requests.RequestException(f\"HTTP {resp.status_code} : {resp.reason}\")\n\ncentroids = gpd.GeoDataFrame.from_features(\n    features=content[\"features\"], crs=content[\"crs\"][\"properties\"][\"name\"])\ncentroids.head()\n\n\n\n\n\n\n\n\n\n\ngeometry\nFID\nLSOA21CD\nGlobalID\n\n\n\n\n0\nPOINT (-3.27237 52.76593)\n68\nW01000461\n205abce3-aa73-408a-ab25-1c8364c12a63\n\n\n1\nPOINT (-3.21395 51.77614)\n92\nW01001459\n963a18d4-f225-4273-80aa-26c01236af11\n\n\n2\nPOINT (-3.20681 51.78261)\n133\nW01001456\n71e6c3f6-7e87-4cea-9256-97aa366cac43\n\n\n3\nPOINT (-2.68552 51.64398)\n194\nW01001585\ndf9d1950-64cd-42d4-a8fd-18c989951e27\n\n\n4\nPOINT (-3.09114 51.82767)\n203\nW01001563\nb2cc735d-c637-40dc-9871-cb8ac5bca9c3\n\n\n\n\n\n\n\n\nThe geometry column is not in a format that pydeck will accept. Adding a column with a list of long,lat values for each coordinate will do the trick.\n\ncentroids[\"pydeck_geometry\"] = [[c.x, c.y] for c in centroids[\"geometry\"]]\ncentroids.head()\n\n\n\n\n\n\n\n\n\ngeometry\nFID\nLSOA21CD\nGlobalID\npydeck_geometry\n\n\n\n\n0\nPOINT (-3.27237 52.76593)\n68\nW01000461\n205abce3-aa73-408a-ab25-1c8364c12a63\n[-3.27236528249052, 52.7659297296924]\n\n\n1\nPOINT (-3.21395 51.77614)\n92\nW01001459\n963a18d4-f225-4273-80aa-26c01236af11\n[-3.2139466137404, 51.7761398048545]\n\n\n2\nPOINT (-3.20681 51.78261)\n133\nW01001456\n71e6c3f6-7e87-4cea-9256-97aa366cac43\n[-3.20681397850058, 51.7826095824385]\n\n\n3\nPOINT (-2.68552 51.64398)\n194\nW01001585\ndf9d1950-64cd-42d4-a8fd-18c989951e27\n[-2.68552245106253, 51.6439814411607]\n\n\n4\nPOINT (-3.09114 51.82767)\n203\nW01001563\nb2cc735d-c637-40dc-9871-cb8ac5bca9c3\n[-3.09114462316771, 51.8276689661644]\n\n\n\n\n\n\n\n\n\n\nPydeck Visualisation\nWith the correct geometry format, the scatterplot is trivial.\n\n\n\n\n\n\nTip\n\n\n\nControl the map by click and dragging the map with your mouse. Hold shift + click and drag to yaw or pitch the map. Scroll in and out to alter the zoom.\n\n\n\nscatter = pdk.Layer(\n    \"ScatterplotLayer\",\n    centroids,\n    pickable=True,\n    stroked=True,\n    filled=True,\n    line_width_min_pixels=1,\n    get_position=\"pydeck_geometry\",\n    get_fill_color=[255, 140, 0],\n    get_line_color=[255, 140, 0],\n    radius_min_pixels=3,\n    opacity=0.1,\n)\n# Set the viewport location\nview_state = pdk.ViewState(\n    longitude=-2.84,\n    latitude=52.42,\n    zoom=6.5,\n    max_zoom=15,\n    pitch=0,\n    bearing=0,\n)\ntooltip = {\n    \"text\": \"LSOA21CD: {LSOA21CD}\"\n}\n# Render\nr = pdk.Deck(\n    layers=scatter, initial_view_state=view_state, tooltip=tooltip\n)\nr"
  },
  {
    "objectID": "blogs/10-pydeck-with-gpd.html#build-a-geojsonlayer",
    "href": "blogs/10-pydeck-with-gpd.html#build-a-geojsonlayer",
    "title": "Getting Pydeck to Play Nicely with GeoPandas.",
    "section": "Build a GeoJsonLayer",
    "text": "Build a GeoJsonLayer\nGeoJsonLayer is what tends to be used for presenting polygons with pydeck maps. The pydeck docs GeoJsonLayer example uses geoJSON data hosted on GitHub. But with a little effort, a Geopandas GeoDataFrame can be coerced to the necessary format.\n\nIngest Data\nTo demonstrate working with polygons, the Welsh super generalised 2023 local authority district boundaries will be ingested from ONS Open Geography Portal.\nAs elevation and polygon colour will be controlled by features of the data, sklearn.prepeocessing is used to scale the “Shape__Area” column.\n\n\nShow the code\nENDPOINT=\"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/Local_Authority_Districts_December_2023_Boundaries_UK_BSC/FeatureServer/0/query\"\nPARAMS[\"where\"] = \"LAD23CD like 'W%'\"\nresp = requests.get(ENDPOINT, params=PARAMS)\nif resp.ok:\n    content = resp.json()\nelse:\n    raise requests.RequestException(f\"HTTP {resp.status_code} : {resp.reason}\")\n\npolygons = gpd.GeoDataFrame.from_features(\n    features=content[\"features\"], crs=content[\"crs\"][\"properties\"][\"name\"])\n# feature engineering for pydeck viz\nmin_max_scaler = preprocessing.MinMaxScaler()\nx = polygons[\"Shape__Area\"].values.reshape(-1, 1)\nx_scaled = min_max_scaler.fit_transform(x)\npolygons[\"area_norm\"] = pd.Series(x_scaled.flatten())\npolygons.head()\n\n\n\n\n\n\n\n\n\n\ngeometry\nFID\nLAD23CD\nLAD23NM\nLAD23NMW\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Area\nShape__Length\nGlobalID\narea_norm\n\n\n\n\n0\nMULTIPOLYGON (((-4.02145 53.32145, -4.03186 53...\n340\nW06000001\nIsle of Anglesey\nYnys Môn\n245217\n378331\n-4.32298\n53.27931\n7.137720e+08\n226738.375131\ndd2ba2ec-78e0-4113-bfec-5bbf7828fb0b\n0.118905\n\n\n1\nMULTIPOLYGON (((-3.93116 52.55401, -3.93293 52...\n341\nW06000002\nGwynedd\nGwynedd\n280555\n334966\n-3.77715\n52.89883\n2.550513e+09\n466258.144553\n9436112f-3572-409b-8d93-cf21aaca60d5\n0.479954\n\n\n2\nPOLYGON ((-3.86356 53.34172, -3.84075 53.33698...\n342\nW06000003\nConwy\nConwy\n283293\n362563\n-3.74646\n53.14739\n1.131701e+09\n220424.522936\nd9ae4b38-3437-4e10-8556-03e3b529c5c5\n0.201058\n\n\n3\nPOLYGON ((-3.37625 53.32772, -3.38835 53.32323...\n343\nW06000004\nDenbighshire\nSir Ddinbych\n309843\n355416\n-3.34761\n53.08833\n8.374441e+08\n200171.915162\n22313345-a2d2-4a54-9edc-1f492cd7320e\n0.143215\n\n\n4\nMULTIPOLYGON (((-3.08242 53.25551, -3.08806 53...\n344\nW06000005\nFlintshire\nSir y Fflint\n321134\n369280\n-3.18248\n53.21471\n4.386527e+08\n146522.477076\n693217ec-5c94-4ec1-9d81-eed3f99f1777\n0.064825\n\n\n\n\n\n\n\n\nIn order to pass the content of this GeoDataFrame to pydeck, use the to_json method to format as a geoJSON string. Then use json.loads() to format that string as a dictionary.\n\n# format data for use in pydeck\njson_out = json.loads(polygons.to_json())\n# inspect the first authority\njson_out[\"features\"][0][\"properties\"]\n\n{'FID': 340,\n 'LAD23CD': 'W06000001',\n 'LAD23NM': 'Isle of Anglesey',\n 'LAD23NMW': 'Ynys Môn',\n 'BNG_E': 245217,\n 'BNG_N': 378331,\n 'LONG': -4.32298,\n 'LAT': 53.27931,\n 'Shape__Area': 713772027.9524,\n 'Shape__Length': 226738.375130659,\n 'GlobalID': 'dd2ba2ec-78e0-4113-bfec-5bbf7828fb0b',\n 'area_norm': 0.11890511689989425}\n\n\n\n\nPydeck Visualisation\nThis format can now be passed to pydeck. One ‘gotcha’ to be aware of, when using attributes in the json to control elevation or colour, the json properties must be explicitly referenced, eg \"properties.area_norm\".\nIn contrast, when using json attributes in the tooltip, you can refer to them directly, eg \"area_norm\".\n\nr = \"100\"\ng = \"(1 - properties.area_norm) * 255\"\nb = \"properties.area_norm * 255\"\nfill = f\"[{r},{g},{b}]\"\ngeojson = pdk.Layer(\n        \"GeoJsonLayer\",\n        json_out,\n        pickable=True,\n        opacity=1,\n        stroked=True,\n        filled=True,\n        extruded=True,\n        wireframe=True,\n        auto_highlight=True,\n        get_elevation=\"properties.area_norm * 200\",\n        elevation_scale=100,\n        get_fill_color=fill,\n    )\ntooltip = {\"text\": \"{LAD23NM}\\n{LAD23CD}\"}\nview_state = pdk.ViewState(\n    longitude=-2.84,\n    latitude=52.42,\n    zoom=6.5,\n    max_zoom=15,\n    pitch=100,\n    bearing=33,\n)\nr = pdk.Deck(\n    layers=geojson,\n    initial_view_state=view_state,\n    tooltip=tooltip,\n)\nr"
  },
  {
    "objectID": "blogs/10-pydeck-with-gpd.html#tips",
    "href": "blogs/10-pydeck-with-gpd.html#tips",
    "title": "Getting Pydeck to Play Nicely with GeoPandas.",
    "section": "Tips",
    "text": "Tips\n\npydeck does not raise when layer data are not formatted correctly. This can result in some lengthy render times only to discover you have an empty map. To combat this, work with the head or some small sample of your data until you have your map working."
  },
  {
    "objectID": "blogs/10-pydeck-with-gpd.html#conclusion",
    "href": "blogs/10-pydeck-with-gpd.html#conclusion",
    "title": "Getting Pydeck to Play Nicely with GeoPandas.",
    "section": "Conclusion",
    "text": "Conclusion\nThis article has recorded the state of play between pydeck and geopandas at the time of writing. Specifically, formatting:\n\ngeometry columns for pydeck ScatterplotLayer\na GeoDataFrame for use with pydeck GeoJsonLayer.\n\nI hope it saves someone some time bashing geopandas about.\n\nfin!"
  },
  {
    "objectID": "blogs/08-quarto-conda-env.html",
    "href": "blogs/08-quarto-conda-env.html",
    "title": "Conda Environments for Quarto Documents",
    "section": "",
    "text": "Creative commons license, created by Ralph"
  },
  {
    "objectID": "blogs/08-quarto-conda-env.html#introduction",
    "href": "blogs/08-quarto-conda-env.html#introduction",
    "title": "Conda Environments for Quarto Documents",
    "section": "Introduction",
    "text": "Introduction\nThis article sets out minimal instructions on rendering quarto documents that rely on specified conda virtual environments. This article collates information from:\n\nQuarto Documentation: Using Conda\nQuarto Documentation: Using Python\nnb_conda_kernels readme\n\nIt is presumed that you will be working within a git repository at times. If this is not the case, ignoring steps specifying git instructions should not affect your ability to successfully render the quarto documents.\n\n\n\n\n\n\nA Note on the Purpose\n\n\n\n\n\nThe purpose of this article is not to explore reasons for using conda enviroments or to compare the pros and cons of the many different options for managing python virtual environments. It aims to help the reader configure quarto documents to run with a specified conda environment while remaining minimal, opting to link to sources of further information where discussion may complement the content.\n\n\n\n\nIntended Audience\nPython practitioners familiar with conda environment management [1] who are less familiar working with quarto documents [2].\n\n\nThe Scenario\nYou are writing a quarto document that contains python code. You would like to use conda to manage your python dependencies. You are encountering problems in having quarto select the appropriate conda environment.\n\n\nWhat You’ll Need:\n\nConda or miniconda\nQuarto\nText editor eg VS Code\nPython package manager (eg pip)\nAccess to a command line interface (CLI) such as terminal / Bash.\nrequirements.txt:\n\n\n\nrequirements.txt\n\nnbclient\nnbformat\npalmerpenguins\n\n\ngit (optional)"
  },
  {
    "objectID": "blogs/08-quarto-conda-env.html#configuring-quarto-with-conda-env",
    "href": "blogs/08-quarto-conda-env.html#configuring-quarto-with-conda-env",
    "title": "Conda Environments for Quarto Documents",
    "section": "Configuring Quarto with Conda Env",
    "text": "Configuring Quarto with Conda Env\n\nProject Structure\n\nCreate a new project folder. Open a terminal and change directory to the new project.\nSave the requirements to a requirements.txt file.\nCreate a new quarto document in the project root. In VS Code, use\nFile  New File...  Quarto Document.\nWrite the following content to a python code chunk in the quarto file and save as penguins.qmd\n\n\n\npenguins.qmd\n\ndf = penguins.load_penguins().dropna()\ndf.head()\n\n\n\nConfigure the Environment\n\nCreate a new conda environment with the -p flag and give it a suitably descriptive name 1. Ensure that the environment is built with python 3.11 2.\n\n\n\nCLI\n\nconda create -p SOME_ENV_NAME python=3.11 -y\n\n\nActivate the environment.\n\n\n\nCLI\n\nconda activate ./SOME_ENV_NAME\n\n\nInstall the requirements file.\n\n\n\nCLI\n\npip install -r requirements.txt\n\n\nAdd a .gitignore file and include the name of the local environment directory created in step 4.\n\n\n\n.gitignore\n\nSOME_ENV_NAME/\n\n\n\nConfigure the Quarto Project\n\nCreate a _quarto.yml configuration file in the project root. In this file, we will specify that the quarto render command should render any qmd files and ignore any files found within your local environment. Add the following content:\n\n\n\n_quarto.yaml\n\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n    - \"!/./SOME_ENV_NAME/\"\n\n\nUse conda to install the nb_conda_kernels package. This is used to manage python jupyter kernels for notebooks.\n\n\n\nCLI\n\nconda install nb_conda_kernels\n\n\nCopy the path returned from the below command\n\n\n\nCLI\n\njupyter --config-dir\n\n\nCreate a jupyter_config.json in the jupyter config directory:\n\n\n\nCLI\n\ntouch &lt;INSERT_YOUR_CONFIG_DIR&gt;/jupyter_config.json\n\n\nWrite the below content to this file and save.\n\n\n\nCLI\n\necho -e \"{\\n  \"CondaKernelSpecManager\": {\\n    \"kernelspec_path\": \"--user\"\\n  }\\n}\" &gt;&gt; &lt;INSERT_YOUR_CONFIG_DIR&gt;/jupyter_config.json\n\n\nRun the below command to return a list of available kernels:\n\n\n\nCLI\n\npython -m nb_conda_kernels list\n\n\nCopy the name (not the path) for the environment that you created with the format conda-env-&lt;YOUR_ENV_NAME&gt;-py.\nOpen penguins.qmd. Adjust the YAML header so that it contains the following:\n\n\n\npenguins.qmd\n\njupyter: \n  kernelspec:\n    name: \"conda-env-&lt;YOUR_ENV_NAME&gt;-py\"\n    language: \"python\"\n    display_name: \"&lt;YOUR_ENV_NAME&gt;\"\n\n\nYou should now be able to render the quarto project, confirming that the target environment was activated in the CLI output. eg:\n\n\n\nCLI\n\nquarto render\n\nStarting &lt;YOUR_ENV_NAME&gt; kernel...Done\n\n\nTips\n\nWhen encountering issues with quarto render, it can be informative to examine the output of quarto check or quarto check jupyter in the CLI.\nAs there are many steps to configuring conda, it may be a good idea to create a dedicated conda environment for all of your quarto projects. Quarto attempts to select an appropriate kernel based upon the content of the first executable python code chunk in your quarto document. Usually, this chunk would contain the import statements. However, over time this would likely result in package conflicts over time.\nThe approach set out in this how-to would be a good fit for a website built with quarto, where the configuration steps can be performed only once in a parent website environment, and then specific, minimal environments created for each article requiring a python environment.\nAlternatively, consider using venv or poetry to manage python environments [3] for quarto projects."
  },
  {
    "objectID": "blogs/08-quarto-conda-env.html#troubleshooting",
    "href": "blogs/08-quarto-conda-env.html#troubleshooting",
    "title": "Conda Environments for Quarto Documents",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nYou’ve created a new environment and it is not discovered when running python -m nb_conda_kernels list:\n\nActivate your new environment\npip install ipykernel\nRun:\n\npython -m ipykernel install --user --name &lt;INSERT_ENV_NAME&gt; --display-name \"&lt;INSERT_DISPLAY_NAME&gt;\"\n\nDeactivate the new environment.\nRun python -m nb_conda_kernels list once more and the new env should appear.\nTaken from this SO thread"
  },
  {
    "objectID": "blogs/08-quarto-conda-env.html#conclusion",
    "href": "blogs/08-quarto-conda-env.html#conclusion",
    "title": "Conda Environments for Quarto Documents",
    "section": "Conclusion",
    "text": "Conclusion\nThis article has walked the reader through setting up a basic quarto project, creating a conda environment, and configuring a quarto document to render with a specified environment. For more help with quarto, consult the quarto-cli GitHub repository [4] and the RStudio Community [5] (soon to rebrand to the Posit Community).\n\nfin!"
  },
  {
    "objectID": "blogs/08-quarto-conda-env.html#footnotes",
    "href": "blogs/08-quarto-conda-env.html#footnotes",
    "title": "Conda Environments for Quarto Documents",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen creating conda environments, the use of generic names such as env will result in conda prepending the environment name with numbers to avoid conflicts. Use descriptive environment names in order to avoid this, eg penguins-env.↩︎\nnb_conda_kernels (a package required in a later step) does not currently work with python 3.12 or newer.↩︎"
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "",
    "text": "Wikimedia commons UK Map."
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html#introduction",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html#introduction",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial is for programmers familiar with Python and how to create virtual environments, but perhaps less familiar with the Python requests package or ArcGIS REST API [1].\nIf you’re in a rush and just need a snippet that will ingest every UK 2021 LSOA boundary available, here is a GitHub gist just for you."
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html#the-scenario",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html#the-scenario",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "The Scenario",
    "text": "The Scenario\nYou would like to use python to programmatically ingest data from the Office for National Statistics (ONS) Open Geography Portal. This tutorial aims to help you do this, working with the 2021 LSOA boundaries, the essential features and quirks of the ArcGIS REST API will be explored."
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html#what-youll-need",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html#what-youll-need",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "What you’ll need:",
    "text": "What you’ll need:\n\nA permissive firewall (whitelist the domain “https://geoportal.statistics.gov.uk/” if necessary)\nPython package manager (eg pip)\nPython environment manager (eg venv, poetry etc)\nPython requirements:\n\n\n\nrequirements.txt\n\nfolium\ngeopandas\nmapclassify\nmatplotlib\npandas\nrequests"
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html#tutorial",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html#tutorial",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "Tutorial",
    "text": "Tutorial\n\nSetting Things Up\n\nCreate a new directory with a requirements file as shown above.\nCreate a new virtual environment.\npip install -r requirements.txt\nCreate a file called get_data.py or whatever you would like to call it. The rest of the tutorial will work with this file.\nAdd the following lines to the top of get_data.py and run them, this ensures that you have the dependencies needed to run the rest of the code:\n\n\nimport requests\nimport geopandas as gpd\nimport pandas as pd\n\n\n\nFinding The Data Asset\nOne of the tricky parts of working with the GeoPortal is finding the resource that you need.\n\nAccess the ONS Open Geography Portal homepage [2].\nUsing the ribbon menu at the top of the page, navigate to:\nBoundaries  Census Boundaries  Lower Super Output Areas  2021 Boundaries.\nOnce you have clicked on this option, a page will open with items related to your selection. Click on the item called “Lower Layer Super Output Areas (2021) Boundaries EW BFC”\nThis will bring you to the data asset that you need. It should look like the webpage below.\n\n\n\n\n\n\nFinding the Endpoint\nNow that we have the correct data asset, let’s find the endpoint. This is the url that we will need to send our requests to, in order to receive the data that we need.\n\nClick on the “View Full Details” button.\nScroll down, under the menu “I want to…”, and expand the “View API Resources” menu.\nYou will see two urls labelled “GeoService” and “GeoJSON”. Click the copy button to the right of the url.\nPaste the url into your Python script.\nEdit the url string to remove everything to the right of the word ‘query’, including the question mark. Then assign it to a variable called ENDPOINT as below:\n\n\nENDPOINT = \"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/Lower_layer_Super_Output_Areas_2021_EW_BFC_V8/FeatureServer/0/query\"\n\nThis ENDPOINT is a url that we can use to flexibly ask for only the data or metadata, that we require.\n\n\nRequesting a Single Entry\nNow that we’re set up to make requests, we can use an example that brings back only a small slice of the database. To do this, we will need to specify some query parameters. These parameters will get added to our endpoint url and will be interpreted by ArcGIS to serve us only the data we ask for. In this example, I will ask for a single LSOA boundary only by specifying the LSOA code with an SQL clause. For more detail on the flexibility of ArcGIS API, please consult the documentation [1].\n\nDefine the below Python dictionary, noting that the syntax and data formats can be brittle - don’t forget to wrap the LSOA21CD in speech marks:\n\n\n# requesting a specific LSOA21CD\nparams = {\n    \"where\": \"LSOA21CD = 'W01002029'\", # SQL clauses can go here\n    \"outSR\": 4326, # CRS that you want\n    \"f\": \"geoJSON\", # response format\n    \"resultOffset\": 0, # parameter used for pagination later\n    \"outFields\": \"*\", # This will ensure all available fields are returned\n}\n\n\nNow I will define a function that will make the request and handle the response for us. Go ahead and define this function:\n\n\ndef request_to_gdf(url:str, query_params:dict) -&gt; gpd.GeoDataFrame:\n    \"\"\"Send a get request to ArcGIS API & Convert to GeoDataFrame.\n\n    Only works when asking for features and GeoJSON format.\n\n    Parameters\n    ----------\n    url : str\n        The url endpoint.\n    query_params : dict\n        A dictionary of query parameter : value pairs.\n\n    Returns\n    -------\n    requests.response\n        The response from ArcGIS API server. Useful for paginated requests\n        later.\n    gpd.GeoDataFrame\n        A GeoDataFrame of the requested geometries in the crs specified by the\n        response metadata.\n\n    Raises\n    ------\n    requests.exceptions.RequestException\n        The response was not ok.\n    \"\"\"\n    # this approach will only work with geoJSON\n    query_params[\"f\"] = \"geoJSON\"\n    # get the response\n    response = requests.get(url, params=query_params)\n    if response.ok:\n        # good response (hopefully, but be careful for JSONDecodeError)\n        content = response.json()\n        return (\n            response, # we'll need the response again later for pagination\n            gpd.GeoDataFrame.from_features(\n                content[\"features\"],\n                crs=content[\"crs\"][\"properties\"][\"name\"]\n                # safest to get crs from response\n                ))\n    else:\n        # cases where a traditional bad response may be returned\n        raise requests.RequestException(\n            f\"HTTP Code: {response.status_code}, Status: {response.reason}\"\n        )\n\nBriefly, this function is going to ensure the geoJSON format is asked for, as this is the neatest way to bash the response into a GeoDataFrame. It then queries ArcGIS API with the endpoint and parameter you specify. It checks if a status code 200 was returned (good response), if not an exception is raised with the HTTP code and status. Finally, if no error triggered an exception, the ArcGIS response and a GeoDataFrame format of the spatial feature is returned.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nBe careful when handling the response of ArcGIS API. Depending on the query you send, it is possible to return status code 200 responses that seem fine. But if the server was unable to make sense of your SQL query, it may result in a JSONDecodeError or even content with details of your error. It is important to handle the various error conditions if you plan to build something more robust than this tutorial and to be exacting with your query strings. For this reason, I would suggest using the params dictionary approach to introducing query parameters rather than attempting to manually format the url string.\n\n\n\n\nWith that function defined, we can go straight to a tabular data format, like below:\n\n\n_, gdf = request_to_gdf(ENDPOINT, params)\ngdf.head()\n\n\n\n\n\n\n\n\n\ngeometry\nFID\nLSOA21CD\nLSOA21NM\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Area\nShape__Length\nGlobalID\n\n\n\n\n0\nPOLYGON ((-3.06378 51.58946, -3.06200 51.58922...\n35661\nW01002029\nNewport 009G\n326721\n187882\n-3.05905\n51.58501\n315524.689297\n3127.139319\n716fb767-06e5-40c8-a0be-9c88b31f6cdf\n\n\n\n\n\n\n\n\n\nWe can use the GeoDataFrame .explore() method to quickly inspect the fruit of our efforts.\n\n\ngdf.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nReturn All LSOAs in a Local Authority\n\nWe probably need to work with more than just a single LSOA, but would prefer not to ingest all of them. Have a look at the available columns in the GeoDataFrame.\n\n\n\n\n\n\n\n\n\n\n\ngeometry\nFID\nLSOA21CD\nLSOA21NM\nBNG_E\nBNG_N\nLONG\nLAT\nShape__Area\nShape__Length\nGlobalID\n\n\n\n\n0\nPOLYGON ((-3.06378 51.58946, -3.06200 51.58922...\n35661\nW01002029\nNewport 009G\n326721\n187882\n-3.05905\n51.58501\n315524.689297\n3127.139319\n716fb767-06e5-40c8-a0be-9c88b31f6cdf\n\n\n\n\n\n\n\n\nThere is a pattern that we can exploit to request every LSOA in a local authority. Have a go at updating params[\"where\"] with an SQL query that can achieve this.\n\n\nShow the solution\nparams[\"where\"] = \"LSOA21NM like 'Newport%'\"\n\n\n\nPass the updated params dictionary to the request_to_gdf function and use the .explore() method to visualise the map. Confirm that the LSOAs returned match what you expected.\n\n\n\nShow the solution\n_, gdf = request_to_gdf(ENDPOINT, params)\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nHow Many Records Are There?\n\nUpdate the params dictionary by changing the value of where to '1=1'.\n\n\n\nShow the Solution\n# parameter to get max allowed data, this will get encoded to \"where=1%3D1\"\n# https://www.w3schools.com/tags/ref_urlencode.ASP\nparams[\"where\"] = \"1=1\"\n\n\nFor more on why to do this, consult the ArcGIS docs [1]. This is the way to state ‘where=true’, meaning get every record possible while respecting the maxRecordCount. maxRecordCount limits the number of records available for download to 2,000 records in most cases. This is ArcGIS’ method of limiting service demand while not requiring authentication. It also means we need to handle paginated responses.\n\nIt’s a good idea to confirm the number of records available within the database. Have a go at reading through the ArcGIS docs [1] to find the parameter responsible for returning counts only. Query the database for the number of records and store it as an integer called n_records.\n\n\n\nShow the Solution\n# how many LSOA boundaries should we expect in the full data?\nparams[\"returnCountOnly\"] = True\nresponse = requests.get(ENDPOINT, params=params)\nn_records = response.json()[\"properties\"][\"count\"]\nprint(f\"There are {n_records} LSOAs in total\")\n\n\nThere are 35672 LSOAs in total\n\n\n\n\nPaginated Requests\n\nNow we have the number of records, it’s important to go back to collecting geometries. Please update the params dictionary to allow that to happen.\n\n\n\nShow the Solution\n# lets now return to collecting geometries\ndel params[\"returnCountOnly\"] #alternatively set to False\n\n\n\nHave a go at requesting the first batch of LSOA boundaries. Count how many you get without attempting to paginate.\n\n\n\nShow the Solution\nresponse, gdf = request_to_gdf(ENDPOINT, params)\nprint(f\"There are only {len(gdf)} LSOAs on this page.\")\n\n\nThere are only 2000 LSOAs on this page.\n\n\n\nVisualise the first 100 rows of the GeoDataFrame you created in the previous step.\n\n\n\nShow the Solution\ngdf.head(100).explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nWe need a condition to check if there are more pages left in the database. See if you can find the target parameter by examining the response properties.\n\n\n\nShow the Solution\ncontent = response.json()\n# we need a conditional on whether more pages are available:\nmore_pages = content[\"properties\"][\"exceededTransferLimit\"]\nprint(f\"It is {more_pages}, that there are more pages of data to ingest...\")\n\n\nIt is True, that there are more pages of data to ingest...\n\n\n\nWe are nearly ready to ask for every available LSOA boundary. This will be an expensive request. Therefore to make things go a bit faster, let’s ask for only the default fields by removing params[\"outFields\"].\n\n\n\nShow the solution\ndel params[\"outFields\"]\n\n\n\nNow we need to add a new parameter to our params dictionary, with the key resultOffset. We need to send multiple queries to the server, incrementing the value of resultOffset by the number of records on each page in every consecutive request. This may take quite a while, depending on your connection. Add the code below to your python script and run it, then make yourself a cup of your chosen beverage.\n\n\noffset = len(gdf) # number of records to offset by\nall_lsoas = gdf # we can append our growing gdf of LSOA boundaries to this\nwhile more_pages:\n    params[\"resultOffset\"] += offset # increment the records to ingest\n    response, gdf = request_to_gdf(ENDPOINT, params)\n    content = response.json()\n    all_lsoas = pd.concat([all_lsoas, gdf])\n    try:\n        more_pages = content[\"properties\"][\"exceededTransferLimit\"]\n    except KeyError:\n        # rather than exceededTransferLimit = False, it disappears...\n        more_pages = False\n\nall_lsoas = all_lsoas.reset_index(drop=True)\n\nBe careful with the exceededTransferLimit parameter. Instead of being set to False on the last page (as the docs suggest it should) - it actually disappears instead, hence why I use the try:...except clause above. You can attempt to set this parameter explicitly, but I find this makes no difference.\n\nparams[\"returnExceededLimitFeatures\"] = \"true\"\n# or\nparams[\"returnExceededLimitFeatures\"] = True\n# both patterns result in the same behaviour as not setting it - the \n# [\"properties\"][\"exceededTransferLimit\"] key disappears from the final page's\n# response\n\n\nCheck whether the number of records ingested matches the number expected.\n\n\n\nShow the Solution\nall_done = len(all_lsoas) == n_records\nprint(f\"Does the row count match the expected number of records? {all_done}\")\n\n\nDoes the row count match the expected number of records? True\n\n\n\nFinally, visualise the last 100 records available within the GeoDataFrame.\n\n\n\nShow the Solution\nall_lsoas.tail(100).explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html#troubleshooting",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html#troubleshooting",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nOne tip I have for troubleshooting queries is to open up the web interface for the ENDPOINT, by pasting it into your web browser. You should get an interface like below:\n\n\n\nBy using the fields to test out your query parameters and clicking the “Query (GET)” button at the bottom of the page, you can get an indication of whether your query is valid. This is a good place to test out more complex SQL statements for the where parameter:"
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html#conclusion",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html#conclusion",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we have:\n\nDemonstrated how to find resources on ONS Open Geography Portal.\nFound the ArcGIS endpoint url of that resource.\nHad a brief read through the ArcGIS documentation.\nQueried the API for a single LSOA code.\nDiscussed a few of the quirks of this API.\nRetrieved the total number of records available.\nUsed paginated requests to retrieve every record in the database.\n\nA good next step towards a more robust ingestion method would be to consider adding a retry strategy to the requests [3]. For a great overview of the essentials of geographic data and tools, check out my colleague’s fantastic blog on geospatial need-to-knows [4].\nEvery web API has its own quirks, which is part of the joy of working with web data. I hope this was helpful and all the best with your geospatial data project!"
  },
  {
    "objectID": "blogs/06-working-with-ONS-open-geo-portal.html#special-thanks",
    "href": "blogs/06-working-with-ONS-open-geo-portal.html#special-thanks",
    "title": "Getting Data from ONS Open Geography Portal",
    "section": "Special Thanks…",
    "text": "Special Thanks…\n…to my colleague Edward, for working through this blog and providing me with really useful feedback.\n\nfin!"
  },
  {
    "objectID": "blogs/04-starfield-resources.html",
    "href": "blogs/04-starfield-resources.html",
    "title": "Maximizing Loot Gains in Starfield: A Data-Driven Approach",
    "section": "",
    "text": "Image credit https://www.trustedreviews.com/\n\n\nIt’s been a while since I’ve dedicated to a Bethesda video game. The last would have been Fallout 3 in 2009-ish. Since then a lot has changed for me. Kids, a job in data science and a lot less time to absorb myself in RPGs. But with the hype around Bethesda’s Starfield, I once again find myself creeping abandoned outposts, stealing junk. This time, in space rather than a post-apocalyptic alternative Earth.\nEmbarking on a galactic scavenger hunt in the realms of Bethesda’s Starfield is akin to being a spacefaring treasure hunter. Imagine prowling through forsaken outposts, seeking riches beyond measure amidst the cosmos. The challenge, however, lies in discerning the true gems from the cosmic clutter.\nIn this whirlwind of interstellar looting, the pivotal question arises: What celestial junk is truly worth hoarding in this game?\n Aiming to hoard everything may seem like a stellar strategy, but alas, even the boundless void of space has its limits. An excess of loot can weigh you down, impeding your maneuvers through the cosmos. To strike the perfect balance, we need to delve deeper, beyond just mass and value. We need to calculate the ratio of these two values, unveiling the true worth of each celestial find.\n\n\n\n\n\n\nNote\n\n\n\nCalculating the credit per gram ratio unlocks the secrets to discerning the relative value of all available items."
  },
  {
    "objectID": "blogs/04-starfield-resources.html#unleashing-your-inner-space-scavenger",
    "href": "blogs/04-starfield-resources.html#unleashing-your-inner-space-scavenger",
    "title": "Maximizing Loot Gains in Starfield: A Data-Driven Approach",
    "section": "",
    "text": "Image credit https://www.trustedreviews.com/\n\n\nIt’s been a while since I’ve dedicated to a Bethesda video game. The last would have been Fallout 3 in 2009-ish. Since then a lot has changed for me. Kids, a job in data science and a lot less time to absorb myself in RPGs. But with the hype around Bethesda’s Starfield, I once again find myself creeping abandoned outposts, stealing junk. This time, in space rather than a post-apocalyptic alternative Earth.\nEmbarking on a galactic scavenger hunt in the realms of Bethesda’s Starfield is akin to being a spacefaring treasure hunter. Imagine prowling through forsaken outposts, seeking riches beyond measure amidst the cosmos. The challenge, however, lies in discerning the true gems from the cosmic clutter.\nIn this whirlwind of interstellar looting, the pivotal question arises: What celestial junk is truly worth hoarding in this game?\n Aiming to hoard everything may seem like a stellar strategy, but alas, even the boundless void of space has its limits. An excess of loot can weigh you down, impeding your maneuvers through the cosmos. To strike the perfect balance, we need to delve deeper, beyond just mass and value. We need to calculate the ratio of these two values, unveiling the true worth of each celestial find.\n\n\n\n\n\n\nNote\n\n\n\nCalculating the credit per gram ratio unlocks the secrets to discerning the relative value of all available items."
  },
  {
    "objectID": "blogs/04-starfield-resources.html#unveiling-the-cosmic-equation",
    "href": "blogs/04-starfield-resources.html#unveiling-the-cosmic-equation",
    "title": "Maximizing Loot Gains in Starfield: A Data-Driven Approach",
    "section": "Unveiling the Cosmic Equation",
    "text": "Unveiling the Cosmic Equation\nJoin me as we venture into the depths of Starfield’s resource catalogue. We’ll decipher the mysteries of mass, value, and rarity to pinpoint the celestial treasures worth their weight in gold—or rather, in galactic credits.\n\nGathering Data in the Digital Cosmos\nTo navigate this cosmic quest, we turn to the wealth of information readily available on Starfield’s resources. Numerous websites offer tables detailing these cosmic artifacts. For this article, I selected https://inara.cz/starfield/database/, a site that helpfully separates the various Starfield collectibles into meaningful categories.\nFor those curious about the code behind the scenes, check it out on GitHub  (GitHub account required)."
  },
  {
    "objectID": "blogs/04-starfield-resources.html#the-cosmic-riches-mass-value-and-rarity",
    "href": "blogs/04-starfield-resources.html#the-cosmic-riches-mass-value-and-rarity",
    "title": "Maximizing Loot Gains in Starfield: A Data-Driven Approach",
    "section": "The Cosmic Riches: Mass, Value, and Rarity",
    "text": "The Cosmic Riches: Mass, Value, and Rarity\nBelow, I plot the available resources’ masses and values, colouring by rarity. An ordinary least squares (OLS) trend line across all item rarities has been plotted in white. This is the line of best fit that minimises the residuals - the differences between the observed values and trend line.\nWhat does the line mean for selecting resources to loot? Any point above that line is worth collecting. Anything below that line would probably be worth ditching if something higher in value or lower in mass became available. Hover over the points to get the resource names.\nNote that several resources with zero value and mass were removed from the data during processing. I’m generally not interested in things without mass as they pose no dilemma to my encumbered pockets.\n\n\n\n\n\nInsights from the Celestial Scatter\nHere’s what we’ve discovered:\n\nRare and exotic items offer around average payoff.\nTwo standouts, Caelumite and Quark-Degenerate Tissues, are definitely worth collecting.\nExotic and Unique resources tend to be low mass, all being under 3 grams.\nMost uncommon resources don’t offer substantial returns, except for select options like Helium, Silver, and Benzene. These are the lower mass options in this rarity class.\nVac Grade Explosives, a common item, stands out as a valuable choice.\nThe rare class of resources generally promises better returns, with notable outliers like Veryl Explosive and Vytinium Fuel Rod.\n\nAs I have no data on the different likelihoods of discovering these items (rarity drop rates), I cannot comment on which items are best to target for farming. Though https://inara.cz/starfield/database/ does offer in-game locations where these resources may be found."
  },
  {
    "objectID": "blogs/04-starfield-resources.html#value-per-gram-the-cosmic-currency",
    "href": "blogs/04-starfield-resources.html#value-per-gram-the-cosmic-currency",
    "title": "Maximizing Loot Gains in Starfield: A Data-Driven Approach",
    "section": "Value Per Gram: The Cosmic Currency",
    "text": "Value Per Gram: The Cosmic Currency\nLet’s break down the data further, showcasing each resource’s value-to-mass ratio, measured in credits per gram. Unveil the best options to fill your cosmic coffers."
  },
  {
    "objectID": "blogs/04-starfield-resources.html#value-per-gram",
    "href": "blogs/04-starfield-resources.html#value-per-gram",
    "title": "Maximizing Loot Gains in Starfield: A Data-Driven Approach",
    "section": "Value Per Gram",
    "text": "Value Per Gram\nBelow I present the data for each resource in tabular format. The ratio column means value / mass, in credits per gram. The resources are sorted by descending ratio by default, but you can change that by clicking on the arrows in each column.\n\n\n\n\nKey Takeaways\n\nCaelumite and Quark-Degenerate Tissues are high outliers, reaffirming their status as top choices.\nSurprisingly, Vac Grade Explosive emerges as a standout, despite being categorized as common.\n\n\n\n\n\n\n\nNote\n\n\n\nFill your boots up with Vac Grade Explosive. Though think about taking out good life insurance, first.\n\n\nSorting the ratio column in ascending order reveals the poor choices of resources to collect. At least in terms of value per gram. Fiber, Water, Nutrient, all of these things sound so wholesome. Perhaps there are alternative reasons for hoarding such items, such as crafting or survival. It’s not all about becoming the richest spaceperson in the universe, right? \nBy plotting the density of value to mass ratio for each rarity class, an interesting picture emerges. For this chart, I have filtered out those 2 high-end unique outliers, otherwise the traces are a bit small and hard to see.\n\n\n\n\nSome observations\n\nExotic and Unique rarities show a bimodal distribution (2 peaks in value to mass ratio).\nThe other rarities exhibit trimodal distributions.\nCommon and Rare classes have a clear outlier (Vac Grade Explosive and Veryl Explosive). This game places a high value on blowing things up.\nRare resources display a wider distribution in value per gram, making it a good compromise in availability, mass and value."
  },
  {
    "objectID": "blogs/04-starfield-resources.html#thank-you-cosmic-voyagers",
    "href": "blogs/04-starfield-resources.html#thank-you-cosmic-voyagers",
    "title": "Maximizing Loot Gains in Starfield: A Data-Driven Approach",
    "section": "Thank You, Cosmic Voyagers",
    "text": "Thank You, Cosmic Voyagers\nBy deploying simple Python know-how, we’ve unraveled the secrets of the digital cosmos, empowering you to make informed choices. Your virtual pockets, now filled with strategic loot, pave the way to riches in your interstellar escapades.\nBefore we sign off, a massive thank you to https://inara.cz/ for graciously sharing their data, making this cosmic venture possible. And kudos to the Python libraries — requests, beautifulsoup4, plotly, and ridgeplot — for bringing this interstellar exploration to life.\nIn the vast cosmic expanse of Starfield, live long and prosper."
  },
  {
    "objectID": "blogs/02-getting-started-pyshiny.html",
    "href": "blogs/02-getting-started-pyshiny.html",
    "title": "Let’s Build a Basic Python Shiny App",
    "section": "",
    "text": "Source: https://www.wallpaperflare.com/. Creative Commons License.\n\n\nThis tutorial is intended for those who are already familiar with Python, but may be less familiar with dashboarding and Shiny in Python. It may also be of interest to those who are well-versed in RShiny and would like to see how it has been implemented in Python.\nThis is a light-weight, introductory Python Shiny tutorial. No installation of software is required, other than a web-browser (which you must already have) and a willingness to experiment. We will use the shinylive service to display the application that we write and steadily add to a basic app, discussing some of the concepts as we go. Finally, let’s regroup and reflect on some coping techniques for when you begin writing your own Python Shiny apps.\nThis tutorial will not attempt to reproduce any of Posit’s documentation, which is rather excellent so please check that out. Also, if you would prefer a conceptual treatment of Python Shiny, please see my blog on The Current Stateof Python Shiny.\n\n\n\n\n\n\nHow to…\n\n\n\nFeel free to tinker with the code in the following example apps and then press play in the top-right hand corner of the console interface. Don’t worry - you won’t break anything important. To reset the code, simply refresh your web page.\nIf the app doesn’t launch, you’ll see some spinning grey hexagons that never go away . This is likely to be a problem with permissions in your browser. But you can click on the collapsible code block below the app windows and copy the code to an app.py file on your computer. If you have python and python shiny installed, you should be good to go.\n\n\n\n\nBelow is a really minimal app that doesn’t do very much at all. The python code is presented on the left. The interactive app is presented on the right. You can type into the app’s text field, and that’s about it for now.\n\n\n\n\n\nShow the code\n#1 Import modules to help us build our app\nfrom shiny import ui, App\n\n#^putting things above the app means they can be shared between your ui and server\n\napp_ui = ui.page_fluid(\n  #2 all of the elements you wish to show your user should go within the ui\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        )\n)\n\ndef server(input, output, session):\n  #3 this is where your app logic should go. So far, not much...\n    return None\n  \n# Finally - this bit packages up our ui and server. Super important - it must\n# be named `app`.\n\n\nYou’ll see that the code defines an app_ui object, which is a Shiny ui instance. Within that ui.page_fluid() function, we can specify all the elements of the app that we would like to present to our users.\n\n\n\n\n\n\nOn Users…\n\n\n\nThere are only two industries that call their customers “users”: illegal drugs and software – Edward Tufte\n\n\nSo far, only one simple ui element has been defined. The humble text input ui.input_text() which allows our users to place their own text into a text field.\nNotice that in Python, all the inputs begin with input.... There’s ui.input_text() as we’ve seen, but there’s lots more. ui.input_date(), ui.input_file() and ui.input_slider to name a few. This consistent syntax approach is a subtle improvement over RShiny and makes it so much easier to work with the vast array of widgets without having to remember them all. If you’re working in a modern editor such as Visual Studio Code, simply typing ui.input will remind you of all the options available to you. For those not working in a nice GUI like VSCode, a Shiny cheatsheet may be useful, though note that at the time of writing I could only find R-flavoured ones…\nAll ui input elements start with the same 2 arguments, id and label:\n\nid: The internal name of the input. What does that mean? Call it what you like. Whatever you choose, be aware that when you want to use the values from the text input to do something in the server, it’s this name that you will need to reference.\nlabel: A short piece of text that prompts your user to do something. This will be displayed in your ui above the input element.\n\n\n\n\nUnfortunately, so far our app doesn’t actually do much. Typing into the empty text field yields no result. That’s because right now, our server function simply returns None. Let’s resolve this.\n\n\n\n\n\nShow the code\n#1 update the import statement to include `render` module\nfrom shiny import ui, App, render\n\napp_ui = ui.page_fluid(\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    #2 Include a ui output element. This will show the calculations\n    # made in the server back to the user.\n    ui.output_text_verbatim(\"greeting\"),\n)\n\ndef server(input, output, session):\n    #3 Update the server with a function that handles the text response.\n    @output # use the output decorator to mark this function\n    @render.text # also need to ensure we use the correct render decorator\n    def greeting():\n        \"\"\"\n        This function will take the output of the ui.input_text() element,\n        format the string in a polite sentence and format it as an HTML\n        output for the ui to show.\n        \"\"\"\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\"\n\napp = App(app_ui, server)\n\n\nThere’s quite a lot going on in the above code chunk. Let’s start with the decorators @output & @render.text:\n\n@output: Any function marked with this decorator will have its returned value made available to the user interface. Notice that in the line ui.output_text_verbatim(\"greeting\") we are able to call on the values of the server’s greeting() function that we marked as an @output.\n@render.text: This tells Shiny what type of output to handle. Is it text, a chart (@render.plot) or something more fancy, like dynamically rendered ui (@render.ui). These output types all have their corresponding output functions to use in the ui. Here we called ui.output_text_verbatim().\nCalling the wrong ui-side function may not result in an error, but can have unexpected results, such as your text disappearing from your app. Keep an eye out for that if things aren’t working - are you using the correct combination of render in the server with output_... in the ui?\n\nDid you notice anything off-putting about the above code? Yes, too many comments but please indulge me. Functions in the server and ui are passing values back and forth. That can be a bit overwhelming to get your head around when you’re new to what’s known as ‘event-driven programming’. All that means is that the program needs to respond to some action taken by the user. The syntax in which you reference the functions is a bit inconsistent to my mind. Let’s take a closer look:\nIf I mark some function make_plot() in the server as an @output and then wish to call on its value within the ui, I need to use ui.output_plot(\"make_plot\"). Notice the lack of brackets following the function name \"make_plot\". Getting this wrong will result in a ValueError. Forgetting to wrap the function reference in a string will result in a NameError.\nNow in the other direction, perhaps we have a numeric input passing integer values from the user to the server. We’ll give the slider widget the id=\"int_slider\". Now when we want to use the value of this slider on the server-side, we use a different syntax:\ndef print_selection():\n    n = int_slider()\n    return f\"You chose {n}!\"\nNotice this time, we include brackets after our call to the widget id: n = int_slider(). Weird, right? Getting this wrong may result in unexpected behaviours. Keep an eye out for this. Also, wrapping server id references in speech marks results in unexpected behaviours, but not necessarily errors.\nIf I haven’t lost you yet, well done! Debugging applications can be a very frustrating process - part intuition earned from hours of Shiny debugging, part Stack Overflow and part coping mechanisms. I’ll cover some of those in the Tips section.\n\n\nTry modifying the app provided in the previous examples to repeat the greeting a number of times specified by the user.\n\n\n\n\n\n\nHints. Click to expand if needed.\n\n\n\n\n\n\nYou will need to include a UI input that will collect numbers from the user.\nUpdate the greeting() function to return multiples of the greeting string.\nExplore the other text output functions to avoid the message being truncated.\nIf you’re stuck, click on “Show the code” to see a solution.\n\n\n\n\n\n\n\n\n\nShow the code\nfrom shiny import ui, App, render\n\napp_ui = ui.page_fluid(\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    # 1 update the UI with a way of taking numbers from the user. Here I use a\n    # slider, but a numeric input or even radio buttons would also work.\n    ui.input_slider(\n        id=\"n_greetings\",\n        label=\"number of greetings\", value=1, min=1, max=10, step=1),\n\n    #2 Change to output_text instead of output_text_verbatim, which uses strict\n    # rules for word wrapping and would hide most of a long greeting.\n    ui.output_text(\"greeting\"),\n    ui.tags.br()\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def greeting():\n        \"\"\"\n        This function will take the output of the ui.input_text() element,\n        format the string in a polite sentence and format it as an HTML\n        output for the ui to show.\n        \"\"\"\n        # multiply the greeting string by the number of times the user asked\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\" * input.n_greetings()\n\napp = App(app_ui, server)\n\n\n\n\n\n\nOne final adjustment to this app. When you’re typing a name into the text input field, there’s a bit of a race going on. Can you type faster than the server can render the text? This may not be what you want. In fact, you may require a bunch of selections to be made prior to calculating anything in the server. We can use methods to interrupt and isolate elements of the server. In effect, we can tell any of our server functions to hang fire until a certain condition is met. In this example, we’ll try out perhaps the simplest way of achieving this, enter the ui.input_action_button().\n\n\n\n\n\nShow the code\n#1 Import the reactive module from Shiny\nfrom shiny import ui, render, App, reactive\n\napp_ui = ui.page_fluid(\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    #2 add an action button to the ui, much like we did with the text input\n    ui.input_action_button(id=\"go_button\", label=\"Click to run...\"),\n    # add some visual separation to the text output\n    ui.tags.br(),\n    ui.tags.br(),\n\n    ui.output_text_verbatim(\"greeting\"),\n)\n\ndef server(input, output, session):\n    #3 add an additional mark below the others\n    @output\n    @render.text\n    @reactive.event(input.go_button)\n    def greeting():\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\"\n\n# This is a Shiny.App object. It must be named `app`.\napp = App(app_ui, server)\n\n\nYou should now see that an input button has appeared and the sentence won’t get printed out until you press it.\nAlso notice that the inconsistency in how to refer to functions on the other side of the ui:server divide rears its head once more. All in the server, when we want to use the values returned by the text input, we use the syntax input.name_entry(). When we want to use the action button in the reactive decorator, we have to use input.go_button - no parenthesis! The docs describe this as when you need to access the returned value versus when you need to call the function. This does make sense but can introduce some cognitive conflict while you are working with Shiny. I hope by version 1.0 the development team can find a way to simplify things.\nI also included some visual separation between elements in the ui by using ui.tags.br(). If you know a little HTML, you may get excited at that. You can access all the typical HTML tags in this way:\n\n\n\n\n\nShow the code\nfrom shiny import ui, App, render\n\napp_ui = ui.page_fluid(\n    ui.tags.h1(\"A level 1 heading\"),\n    ui.tags.h2(\"A level 2 heading\"),\n    ui.tags.br(),\n    ui.tags.p(\"Some text \", ui.tags.strong(\"in bold...\"))\n\n)\n\ndef server(input, output, session):\n    return None\n\napp = App(app_ui, server)\n\n\n\n\nDo you know enough markdown syntax to convert the ui below from HTML tags into markdown? This will greatly simplify the code. You will need to use ui.markdown(\"\"\"some multiline markdown\"\"\") to achieve that.\n\n\n\n\n\n\nHints. Click to expand if needed.\n\n\n\n\n\n\nYou can use this markdown cheatsheet to help.\nIf you’re stuck, click on “Show the code” to see an example solution.\n\n\n\n\n\n\n\n\n\nShow the code\n#1 Import the reactive module from Shiny\nfrom shiny import ui, render, App, reactive\n\napp_ui = ui.page_fluid(\n\n    ui.markdown(\n        \"\"\"\n        # Hello Python Shiny!\n        This **simple application** will print you a greeting.\n\n        1. Enter your name\n        2. Click run\n\n        Please visit [some website](https://datasciencecampus.github.io/)\n        for more information\n        ***\n        \"\"\"),\n    \n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    #2 add an action button to the ui, much like we did with the text input\n    ui.input_action_button(id=\"go_button\", label=\"Click to run...\"),\n    # add some visual separation to the text output\n    ui.tags.br(),\n    ui.tags.br(),\n\n    ui.output_text_verbatim(\"greeting\"),\n)\n\ndef server(input, output, session):\n    #3 add an additional mark below the others\n    @output\n    @render.text\n    @reactive.event(input.go_button)\n    def greeting():\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\"\n\n# This is a Shiny.App object. It must be named `app`.\napp = App(app_ui, server)\n\n\nSo there you have it. A very basic application that can be used to print out some simple messages to your user. This is of course a trivial app in order to keep things basic for the purposes of this blog. If you’d like to investigate what’s possible with Shiny, I’d suggest taking a peek through the Posit docs examples and the Python Shiny gallery. In the next section I’ll go over some tips that may help with common pitfalls I’ve encountered while working in Shiny."
  },
  {
    "objectID": "blogs/02-getting-started-pyshiny.html#getting-to-grips-with-python-shiny.",
    "href": "blogs/02-getting-started-pyshiny.html#getting-to-grips-with-python-shiny.",
    "title": "Let’s Build a Basic Python Shiny App",
    "section": "",
    "text": "Source: https://www.wallpaperflare.com/. Creative Commons License.\n\n\nThis tutorial is intended for those who are already familiar with Python, but may be less familiar with dashboarding and Shiny in Python. It may also be of interest to those who are well-versed in RShiny and would like to see how it has been implemented in Python.\nThis is a light-weight, introductory Python Shiny tutorial. No installation of software is required, other than a web-browser (which you must already have) and a willingness to experiment. We will use the shinylive service to display the application that we write and steadily add to a basic app, discussing some of the concepts as we go. Finally, let’s regroup and reflect on some coping techniques for when you begin writing your own Python Shiny apps.\nThis tutorial will not attempt to reproduce any of Posit’s documentation, which is rather excellent so please check that out. Also, if you would prefer a conceptual treatment of Python Shiny, please see my blog on The Current Stateof Python Shiny.\n\n\n\n\n\n\nHow to…\n\n\n\nFeel free to tinker with the code in the following example apps and then press play in the top-right hand corner of the console interface. Don’t worry - you won’t break anything important. To reset the code, simply refresh your web page.\nIf the app doesn’t launch, you’ll see some spinning grey hexagons that never go away . This is likely to be a problem with permissions in your browser. But you can click on the collapsible code block below the app windows and copy the code to an app.py file on your computer. If you have python and python shiny installed, you should be good to go.\n\n\n\n\nBelow is a really minimal app that doesn’t do very much at all. The python code is presented on the left. The interactive app is presented on the right. You can type into the app’s text field, and that’s about it for now.\n\n\n\n\n\nShow the code\n#1 Import modules to help us build our app\nfrom shiny import ui, App\n\n#^putting things above the app means they can be shared between your ui and server\n\napp_ui = ui.page_fluid(\n  #2 all of the elements you wish to show your user should go within the ui\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        )\n)\n\ndef server(input, output, session):\n  #3 this is where your app logic should go. So far, not much...\n    return None\n  \n# Finally - this bit packages up our ui and server. Super important - it must\n# be named `app`.\n\n\nYou’ll see that the code defines an app_ui object, which is a Shiny ui instance. Within that ui.page_fluid() function, we can specify all the elements of the app that we would like to present to our users.\n\n\n\n\n\n\nOn Users…\n\n\n\nThere are only two industries that call their customers “users”: illegal drugs and software – Edward Tufte\n\n\nSo far, only one simple ui element has been defined. The humble text input ui.input_text() which allows our users to place their own text into a text field.\nNotice that in Python, all the inputs begin with input.... There’s ui.input_text() as we’ve seen, but there’s lots more. ui.input_date(), ui.input_file() and ui.input_slider to name a few. This consistent syntax approach is a subtle improvement over RShiny and makes it so much easier to work with the vast array of widgets without having to remember them all. If you’re working in a modern editor such as Visual Studio Code, simply typing ui.input will remind you of all the options available to you. For those not working in a nice GUI like VSCode, a Shiny cheatsheet may be useful, though note that at the time of writing I could only find R-flavoured ones…\nAll ui input elements start with the same 2 arguments, id and label:\n\nid: The internal name of the input. What does that mean? Call it what you like. Whatever you choose, be aware that when you want to use the values from the text input to do something in the server, it’s this name that you will need to reference.\nlabel: A short piece of text that prompts your user to do something. This will be displayed in your ui above the input element.\n\n\n\n\nUnfortunately, so far our app doesn’t actually do much. Typing into the empty text field yields no result. That’s because right now, our server function simply returns None. Let’s resolve this.\n\n\n\n\n\nShow the code\n#1 update the import statement to include `render` module\nfrom shiny import ui, App, render\n\napp_ui = ui.page_fluid(\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    #2 Include a ui output element. This will show the calculations\n    # made in the server back to the user.\n    ui.output_text_verbatim(\"greeting\"),\n)\n\ndef server(input, output, session):\n    #3 Update the server with a function that handles the text response.\n    @output # use the output decorator to mark this function\n    @render.text # also need to ensure we use the correct render decorator\n    def greeting():\n        \"\"\"\n        This function will take the output of the ui.input_text() element,\n        format the string in a polite sentence and format it as an HTML\n        output for the ui to show.\n        \"\"\"\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\"\n\napp = App(app_ui, server)\n\n\nThere’s quite a lot going on in the above code chunk. Let’s start with the decorators @output & @render.text:\n\n@output: Any function marked with this decorator will have its returned value made available to the user interface. Notice that in the line ui.output_text_verbatim(\"greeting\") we are able to call on the values of the server’s greeting() function that we marked as an @output.\n@render.text: This tells Shiny what type of output to handle. Is it text, a chart (@render.plot) or something more fancy, like dynamically rendered ui (@render.ui). These output types all have their corresponding output functions to use in the ui. Here we called ui.output_text_verbatim().\nCalling the wrong ui-side function may not result in an error, but can have unexpected results, such as your text disappearing from your app. Keep an eye out for that if things aren’t working - are you using the correct combination of render in the server with output_... in the ui?\n\nDid you notice anything off-putting about the above code? Yes, too many comments but please indulge me. Functions in the server and ui are passing values back and forth. That can be a bit overwhelming to get your head around when you’re new to what’s known as ‘event-driven programming’. All that means is that the program needs to respond to some action taken by the user. The syntax in which you reference the functions is a bit inconsistent to my mind. Let’s take a closer look:\nIf I mark some function make_plot() in the server as an @output and then wish to call on its value within the ui, I need to use ui.output_plot(\"make_plot\"). Notice the lack of brackets following the function name \"make_plot\". Getting this wrong will result in a ValueError. Forgetting to wrap the function reference in a string will result in a NameError.\nNow in the other direction, perhaps we have a numeric input passing integer values from the user to the server. We’ll give the slider widget the id=\"int_slider\". Now when we want to use the value of this slider on the server-side, we use a different syntax:\ndef print_selection():\n    n = int_slider()\n    return f\"You chose {n}!\"\nNotice this time, we include brackets after our call to the widget id: n = int_slider(). Weird, right? Getting this wrong may result in unexpected behaviours. Keep an eye out for this. Also, wrapping server id references in speech marks results in unexpected behaviours, but not necessarily errors.\nIf I haven’t lost you yet, well done! Debugging applications can be a very frustrating process - part intuition earned from hours of Shiny debugging, part Stack Overflow and part coping mechanisms. I’ll cover some of those in the Tips section.\n\n\nTry modifying the app provided in the previous examples to repeat the greeting a number of times specified by the user.\n\n\n\n\n\n\nHints. Click to expand if needed.\n\n\n\n\n\n\nYou will need to include a UI input that will collect numbers from the user.\nUpdate the greeting() function to return multiples of the greeting string.\nExplore the other text output functions to avoid the message being truncated.\nIf you’re stuck, click on “Show the code” to see a solution.\n\n\n\n\n\n\n\n\n\nShow the code\nfrom shiny import ui, App, render\n\napp_ui = ui.page_fluid(\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    # 1 update the UI with a way of taking numbers from the user. Here I use a\n    # slider, but a numeric input or even radio buttons would also work.\n    ui.input_slider(\n        id=\"n_greetings\",\n        label=\"number of greetings\", value=1, min=1, max=10, step=1),\n\n    #2 Change to output_text instead of output_text_verbatim, which uses strict\n    # rules for word wrapping and would hide most of a long greeting.\n    ui.output_text(\"greeting\"),\n    ui.tags.br()\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def greeting():\n        \"\"\"\n        This function will take the output of the ui.input_text() element,\n        format the string in a polite sentence and format it as an HTML\n        output for the ui to show.\n        \"\"\"\n        # multiply the greeting string by the number of times the user asked\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\" * input.n_greetings()\n\napp = App(app_ui, server)\n\n\n\n\n\n\nOne final adjustment to this app. When you’re typing a name into the text input field, there’s a bit of a race going on. Can you type faster than the server can render the text? This may not be what you want. In fact, you may require a bunch of selections to be made prior to calculating anything in the server. We can use methods to interrupt and isolate elements of the server. In effect, we can tell any of our server functions to hang fire until a certain condition is met. In this example, we’ll try out perhaps the simplest way of achieving this, enter the ui.input_action_button().\n\n\n\n\n\nShow the code\n#1 Import the reactive module from Shiny\nfrom shiny import ui, render, App, reactive\n\napp_ui = ui.page_fluid(\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    #2 add an action button to the ui, much like we did with the text input\n    ui.input_action_button(id=\"go_button\", label=\"Click to run...\"),\n    # add some visual separation to the text output\n    ui.tags.br(),\n    ui.tags.br(),\n\n    ui.output_text_verbatim(\"greeting\"),\n)\n\ndef server(input, output, session):\n    #3 add an additional mark below the others\n    @output\n    @render.text\n    @reactive.event(input.go_button)\n    def greeting():\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\"\n\n# This is a Shiny.App object. It must be named `app`.\napp = App(app_ui, server)\n\n\nYou should now see that an input button has appeared and the sentence won’t get printed out until you press it.\nAlso notice that the inconsistency in how to refer to functions on the other side of the ui:server divide rears its head once more. All in the server, when we want to use the values returned by the text input, we use the syntax input.name_entry(). When we want to use the action button in the reactive decorator, we have to use input.go_button - no parenthesis! The docs describe this as when you need to access the returned value versus when you need to call the function. This does make sense but can introduce some cognitive conflict while you are working with Shiny. I hope by version 1.0 the development team can find a way to simplify things.\nI also included some visual separation between elements in the ui by using ui.tags.br(). If you know a little HTML, you may get excited at that. You can access all the typical HTML tags in this way:\n\n\n\n\n\nShow the code\nfrom shiny import ui, App, render\n\napp_ui = ui.page_fluid(\n    ui.tags.h1(\"A level 1 heading\"),\n    ui.tags.h2(\"A level 2 heading\"),\n    ui.tags.br(),\n    ui.tags.p(\"Some text \", ui.tags.strong(\"in bold...\"))\n\n)\n\ndef server(input, output, session):\n    return None\n\napp = App(app_ui, server)\n\n\n\n\nDo you know enough markdown syntax to convert the ui below from HTML tags into markdown? This will greatly simplify the code. You will need to use ui.markdown(\"\"\"some multiline markdown\"\"\") to achieve that.\n\n\n\n\n\n\nHints. Click to expand if needed.\n\n\n\n\n\n\nYou can use this markdown cheatsheet to help.\nIf you’re stuck, click on “Show the code” to see an example solution.\n\n\n\n\n\n\n\n\n\nShow the code\n#1 Import the reactive module from Shiny\nfrom shiny import ui, render, App, reactive\n\napp_ui = ui.page_fluid(\n\n    ui.markdown(\n        \"\"\"\n        # Hello Python Shiny!\n        This **simple application** will print you a greeting.\n\n        1. Enter your name\n        2. Click run\n\n        Please visit [some website](https://datasciencecampus.github.io/)\n        for more information\n        ***\n        \"\"\"),\n    \n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    #2 add an action button to the ui, much like we did with the text input\n    ui.input_action_button(id=\"go_button\", label=\"Click to run...\"),\n    # add some visual separation to the text output\n    ui.tags.br(),\n    ui.tags.br(),\n\n    ui.output_text_verbatim(\"greeting\"),\n)\n\ndef server(input, output, session):\n    #3 add an additional mark below the others\n    @output\n    @render.text\n    @reactive.event(input.go_button)\n    def greeting():\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\"\n\n# This is a Shiny.App object. It must be named `app`.\napp = App(app_ui, server)\n\n\nSo there you have it. A very basic application that can be used to print out some simple messages to your user. This is of course a trivial app in order to keep things basic for the purposes of this blog. If you’d like to investigate what’s possible with Shiny, I’d suggest taking a peek through the Posit docs examples and the Python Shiny gallery. In the next section I’ll go over some tips that may help with common pitfalls I’ve encountered while working in Shiny."
  },
  {
    "objectID": "blogs/02-getting-started-pyshiny.html#tips.",
    "href": "blogs/02-getting-started-pyshiny.html#tips.",
    "title": "Let’s Build a Basic Python Shiny App",
    "section": "Tips.",
    "text": "Tips.\n\nShiny for Python VSCode Extension.\nThe Shiny for Python extension is a convenient way to launch Python Shiny apps. It adds a ‘Run Shiny App’ button to the VS Code interface, allowing for options to serve the app locally within a dedicated VS Code viewer window, or alternatively launch the app directly within your default web browser.\nIn order to run your application with this extension, you must ensure your app file is saved as app.py, otherwise the run button will not recognise that the currently selected document is a Shiny app.\n\n\nHeader Accessibility Adjustment.\nA big shoutout to Utah State University for making their fantastic suite of web accessibility-checking tools open source. These tools make checking the accessibility of your web products much easier. Simply visit the Web Accessibility Evaluation Tool (WAVE) and enter a url under “Web page address:” and press return. The site will launch a helpful overlay on top of your specified url, highlighting accessibility alerts, warnings and features. There is also a sidebar helpfully explaining why the various alerts are important and what can be done to resolve them.\nUnless you have managed to host a Shiny application on a service such as shinyapps.io, unfortunately you won’t have a url to pass to WAVE. Working locally on your machine, your locally hosted app interface will launch with a url like: http://localhost:… There is another way to use WAVE to check localhost sites. Using the WAVE browser extensions will allow you to launch the WAVE tool within any of your browser windows. This would allow you to run these checks locally on your machine while also ensuring that your app looks good on Chrome, Firefox or Edge. When checking basic Python Shiny apps for accessibility, one of the common accessibility errors you will encounter will be:\n\n\n\n\n\n\nLanguage missing or invalid!\n\n\n\nThe language of the document is not identified or a lang attribute value is invalid.\n\n\nThis means a screen reader may not detect the language of the website content and may not work properly. The ideal scenario would be that Shiny would ask you to specify the language of the application that you are working in at the point where you create the application. It’s a straightforward fix though. We can include some tags at the top of our ui that will update the web page content with the required information:\n\n\n\n\n\nShow the code\nfrom shiny import ui, render, App, reactive\n\napp_ui = ui.page_fluid(\n    # Add a header and specify the language and title.\n    ui.tags.header(ui.tags.html(lang=\"en\"), ui.tags.title(\"A Really Basic App\")),\n\n    ui.input_text(\n        id=\"name_entry\",\n        label=\"Please enter your name\",\n        placeholder=\"Your name here\"\n        ),\n    ui.input_action_button(id=\"go_button\", label=\"Click to run...\"),\n    ui.tags.br(),\n    ui.tags.br(),\n\n    ui.output_text_verbatim(\"greeting\"),\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    @reactive.event(input.go_button)\n    def greeting():\n        return f\"Hi {input.name_entry()}, welcome to Python Shiny.\"\n\napp = App(app_ui, server)\n\n\n\n\nMissing Commas.\nOne of the more frustrating aspects of debugging Shiny applications, particularly as the application grows, is that a single misplaced comma can completely break your application. The thing to look out for here is when you run your app and you get the unexpected SyntaxError: invalid syntax error. I recall this being a real headache when I was learning RShiny, so much so that I would leave a little comment after each comma reminding me of which element of the code was being closed within the preceding bracket.\nThe Python errors are really helpful. They not only point to a problem with the syntax, but they identify the line number in the app where the first instance of this issue was encountered.\n\n\nDebugging Errors.\nAt times it can be unclear why errors are being raised and it becomes important to investigate intermediate values that are being calculated in the server. Your usual python debugging tools may not work with Python Shiny. Shiny is event-driven and the reactive flow and order of the object definitions in your scripts are treated a bit differently. Without a tool like R’s Reactlog package, this is currently quite tricky to do in Python. The main coping mechanism available at the moment is to include some text outputs in the ui, paired with render text functions in the server. You can go ahead and use these elements to helpfully print out intermediate values such as DataFrame column names, data types etc and then comment them out when they’re no longer needed. Examining these intermediate values from within your ui is often the way to go when you can’t understand the root cause of your problem.\n\n\n\n\n\n\nMake a Reprex!\n\n\n\nOne more approach to consider is to try to isolate your problem.\n\n\nSometimes it can be hard to build a mental picture of all the moving elements under the hood in your server, and how they may be unexpectedly interacting with each other. The problem you’re encountering may be due to these complex interactions in your server. Simply commenting out the code not directly related to the issue you are experiencing helps to triangulate the source of the issue within your reactive model. This is also the first step towards producing a reprex - a reproducible example. These should be as minimal as possible and are very helpful to start getting to the root of a programming problem.\n\n\nSpecifying Working Directory.\nThis is more of a consideration for deployment to a service such as shinyapps.io than something you tend to encounter while learning the ropes. And you likely won’t encounter this issue if your app.py file is located in the top level of your project folder (also known as the project root). If your app is not in the root of the project folder, you may wish to include this snippet of code before you define your Shiny ui:\nos.chdir(os.path.dirname(os.path.realpath(__file__)))\nThis ensures that the working directory is set to that of the app.py file when it runs. If you encounter pandas errors complaining about FileExistsError when you deploy your app to shinyapps.io but not when you locally run your application, this may be the fix you need. Also something to consider if your app is styled correctly locally but not when you deploy. Potentially a relative path to a dedicated stylesheet has broken.\nOne more thing on deploying your app - if you do intend to host your app for others to use, I cannot emphasise this enough:\n\n\n\n\n\n\nDeploy Your App Early!\n\n\n\n\n\n\nDeployment of applications is not a straight forward, half an hour job. There are often inconsistencies to iron out between the environment you developed the app in and the server that will be running the remote app for your users. Deploy your app early when it is basic and you can catch these inconsistencies as you go. Or don’t and ignore the pile of technical debt your project is accruing. These are your choices."
  },
  {
    "objectID": "blogs/02-getting-started-pyshiny.html#in-review.",
    "href": "blogs/02-getting-started-pyshiny.html#in-review.",
    "title": "Let’s Build a Basic Python Shiny App",
    "section": "In Review.",
    "text": "In Review.\nWe have written a very basic application that is not much use beyond a basic tutorial. Although we have successfully demonstrated how to have the ui and server elements of a Shiny application talk to one another. We’ve captured dynamic inputs provided by the user and presented them back within the interface. And we have been able to pause the server execution until the user asks for a response. That’s not a bad start at all. But so much more than this can be achieved in Python Shiny. A good place to go for inspiration is the Posit Example Gallery. And if you’d like to understand a little more about how Python Shiny fits into the Python dashboarding toolkit, please check out my other blog on The Current State of Python Shiny.\nHappy dashboarding!"
  },
  {
    "objectID": "blogs/01-state-of-pyshiny.html",
    "href": "blogs/01-state-of-pyshiny.html",
    "title": "The State of Python Shiny",
    "section": "",
    "text": "DALL.E prompt: python logo golden Sparkling glittery bokeh bright.\n\n\nPython Shiny celebrates its first year anniversary on PyPi in July 2023. In case that needs further qualification, this is the Python-native implementation of the beloved RShiny dashboarding package that has been available in the R programming framework for over a decade now. Python Shiny has benefited from the experience that the developers at Posit have gained in building its more mature, R-flavoured sibling.\nPython Shiny has experienced what I would describe as an accelerated evolution and has the potential to become a front runner in production-grade applications. Mainly because it adheres to an evaluation strategy which is a bit ‘un-pythonic’. Shiny introduces dashboarding with lazy evaluation, meaning that elements of your code will only be evaluated when there is a need to do so. This is not always a great idea, particularly for standard scripting purposes. But it does make event-driven applications more efficient and scalable. More on Posit’s take on the USP of Python Shiny here.\nLet’s consider the following example in Python:\n\n# In Python\ntry:\n  def failing_print(param_a=\"Am I lazy?\", param_b=absolutely_not):\n    print(param_a)\nexcept NameError:\n  print(\"Python would not allow me to define this function.\")\n\nPython would not allow me to define this function.\n\n\nStrict (also known as eager) evaluation in Python has triggered an error. I’ve had to catch the error above with the try/except clause in order to render this website. Even though failing_print() never calls on param_b, Python checks that the default value of that parameter exists on definition of the function.\nLet us now compare the situation in R.\n\n# In R\npassing_print &lt;- function(param_a = \"Am I lazy?\", param_b = yes_you_are){\n  print(param_a)\n}\nprint(paste(\"Does `yes_you_are` exist? : \", exists(\"yes_you_are\")))\n\n[1] \"Does `yes_you_are` exist? :  FALSE\"\n\npassing_print()\n\n[1] \"Am I lazy?\"\n\n\nNo exception was raised although the value yes_you_are has not been defined.\nLazy evaluation in Python Shiny minimises the work the application needs to undertake. Values in the app backend will only be re-evaluated as they are needed and contrasts with more pythonic approaches to dashboarding, such as the case with the streamlit package."
  },
  {
    "objectID": "blogs/01-state-of-pyshiny.html#python-for-shiny-is-now-a-thing",
    "href": "blogs/01-state-of-pyshiny.html#python-for-shiny-is-now-a-thing",
    "title": "The State of Python Shiny",
    "section": "",
    "text": "DALL.E prompt: python logo golden Sparkling glittery bokeh bright.\n\n\nPython Shiny celebrates its first year anniversary on PyPi in July 2023. In case that needs further qualification, this is the Python-native implementation of the beloved RShiny dashboarding package that has been available in the R programming framework for over a decade now. Python Shiny has benefited from the experience that the developers at Posit have gained in building its more mature, R-flavoured sibling.\nPython Shiny has experienced what I would describe as an accelerated evolution and has the potential to become a front runner in production-grade applications. Mainly because it adheres to an evaluation strategy which is a bit ‘un-pythonic’. Shiny introduces dashboarding with lazy evaluation, meaning that elements of your code will only be evaluated when there is a need to do so. This is not always a great idea, particularly for standard scripting purposes. But it does make event-driven applications more efficient and scalable. More on Posit’s take on the USP of Python Shiny here.\nLet’s consider the following example in Python:\n\n# In Python\ntry:\n  def failing_print(param_a=\"Am I lazy?\", param_b=absolutely_not):\n    print(param_a)\nexcept NameError:\n  print(\"Python would not allow me to define this function.\")\n\nPython would not allow me to define this function.\n\n\nStrict (also known as eager) evaluation in Python has triggered an error. I’ve had to catch the error above with the try/except clause in order to render this website. Even though failing_print() never calls on param_b, Python checks that the default value of that parameter exists on definition of the function.\nLet us now compare the situation in R.\n\n# In R\npassing_print &lt;- function(param_a = \"Am I lazy?\", param_b = yes_you_are){\n  print(param_a)\n}\nprint(paste(\"Does `yes_you_are` exist? : \", exists(\"yes_you_are\")))\n\n[1] \"Does `yes_you_are` exist? :  FALSE\"\n\npassing_print()\n\n[1] \"Am I lazy?\"\n\n\nNo exception was raised although the value yes_you_are has not been defined.\nLazy evaluation in Python Shiny minimises the work the application needs to undertake. Values in the app backend will only be re-evaluated as they are needed and contrasts with more pythonic approaches to dashboarding, such as the case with the streamlit package."
  },
  {
    "objectID": "blogs/01-state-of-pyshiny.html#an-example-python-shiny-app.",
    "href": "blogs/01-state-of-pyshiny.html#an-example-python-shiny-app.",
    "title": "The State of Python Shiny",
    "section": "An Example Python Shiny App.",
    "text": "An Example Python Shiny App.\nThis application is written in Python and served with a free shinyapps.io account. I made the app to explore the quality of the spatial data available within OpenStreetMap (OSM) data for certain urban areas. OSM is an open-source, community-maintained source of transport network data. The files also contain other spatial features, such as land use polygons. The quality of this data varies by location, as you may find in the app.\nThe application is pretty straightforward. It uses pandas to read in some pre-prepared data tables from a data folder. These files were prepared with a python package called pyrosm. Pyrosm is a useful application for ingesting, assessing & visualising OSM data. Selecting a city from the dropdowns then clicking the “Go” button will read the correct table from this little database, then using matplotlib, it visualises the selected spatial features. The app also presents some simple summary statistics in tables at the bottom of the page.\nThe preparation of the data files is not strictly necessary. You can serve an app and have it make external connections to data services to ingest data. In this instance, I chose to pre-process the data as it helped to improve the performance of the app. Even so, selecting London or Leeds can result in some wait times, so please be aware of this. Options for improving the performance further could include parallel processing, though that is not implemented here and is beyond the scope of this little example.\nIf you’d like to take a look at the application code and have a GitHub account, you can access it here."
  },
  {
    "objectID": "blogs/01-state-of-pyshiny.html#so-what-does-a-shiny-project-look-like",
    "href": "blogs/01-state-of-pyshiny.html#so-what-does-a-shiny-project-look-like",
    "title": "The State of Python Shiny",
    "section": "So What Does a Shiny Project Look Like?",
    "text": "So What Does a Shiny Project Look Like?\n\nOrganising a Shiny Project.\nBelow is a directory tree diagram of the example Shiny application presented above. It shows the file structure in the various folders.\n.\n├── 01-update-db.py\n├── app.py\n├── config\n│   └── 01-update-db.toml\n├── data\n│   ├── leeds-landuse-2023-06-19.arrow\n│   ├── leeds-natural-2023-06-19.arrow\n│   ├── leeds-net-driving-2023-06-19.arrow\n│   ├── london-landuse-2023-06-19.arrow\n│   ├── london-natural-2023-06-19.arrow\n│   ├── london-net-driving-2023-06-19.arrow\n│   ├── marseille-landuse-2023-06-19.arrow\n│   ├── marseille-natural-2023-06-19.arrow\n│   ├── marseille-net-driving-2023-06-19.arrow\n│   ├── newport-landuse-2023-06-19.arrow\n│   ├── newport-natural-2023-06-19.arrow\n│   └── newport-net-driving-2023-06-19.arrow\n├── requirements.txt\n└── rsconnect-python\n    └── pyrosm-cities-app.json\nThe Shiny code that generates the application is in app.py. The data folder is where you put any data files you’d like to work with. Here I’m working with the .arrow format (More on that here). However, this folder could be used to store csvs, images, a GeoJSON database, whatever your application needs.\nThe file 01-update-db.py and the contents of the config folder are related. The python script is used to update the database in the data folder. This part of the workflow can be quite time consuming, particularly extracting the transport network from dense areas such as Greater London. In order to improve the performance of the app, I chose to pre-prepare this data and limit the number of areas to my specific use-case. A previous iteration of this dashboard simply sent requests using the python pyrosm package to get data for any valid area name. The TOML file in the config folder simply stores some parameters used to configure 01-update-db.py, such as named areas of interest, bounding box coordinates in order to crop the maps, that sort of thing. The requirements.txt file is super important. Not only will it help others to re-run your dashboard with the required package dependencies made explicit, this is also a file that hosting services such as shinyapps.io will use to recreate your development environment. More on that later.\n\n\n\n\n\n\nTip\n\n\n\nEnsuring that you have ‘pinned’ each package version in your requirements file will make publishing to a hosting service such as shinyapps.io a lot easier. It can be frustrating to push your locally working application up to a remote service, waiting for a potentially lengthy deployment routine to complete only to see that your app breaks as you’re working on a different version of numpy than the one available by default in the remote server.\n\n\nFinally, the rsconnect-python directory is not something that would appear in your Shiny project until you deploy it to a remote-hosting service like shinyapps.io. It contains some metadata about your application which the remote service will use to set up your app properly.\n\n\nA Closer Look at app.py.\nAs stated earlier, this is where your app code goes. But before diving straight into some code, let’s try to get a feel for how the reactivity works.\n\n\n\n\n\n\nflowchart TD\n  user[\"User\"]\n  subgraph Application\n  server[\"Server\n  - Executes python code.\n  - Processes data.\n  - Translates outputs into HTML.\n  \"]\n  ui[\"User Interface\n  - Displays input widgets.\n  - Displays rendered server outputs.\n  - Can be adjusted with HTML & CSS.\"]\n  db[(\"Data\")]\n  end\n  user --&gt;|Input Values| ui &lt;--&gt;|Dynamic Values| server\n  db --&gt;|Read| server\n\nlinkStyle default stroke-width:1px,color:blue;\n\n\n\n\n\n\n\n\n\nEach Shiny application comes in two distinct sections. The user interface is the frontend of the application, where a person engages with elements of a web page in order to control visuals, filter tables and so on. The web page translates Python code into HyperText Markup Language (HTML). HTML can be interpreted by a web browser in order to present the user with a structured web page. But in order to present an attractive web page, this HTML foundation needs to be styled. So Cascading Style Sheets (CSS) are used to add decoration to the foundation established by HTML.\n\n\nServer-Side Shenanigans.\nHaving an attractive web page that does not do something when you click around on it, enter your credit card details or click ‘decline’ on cookie consent notices would be rather pointless. This is where a web developer would use the language of the browser to work with data behind that pretty frontend. That language is JavaScript and it’s remarkably versatile and intimidating to many data-types.\nSo to make a functioning app, you need to learn 3 distinct languages. For this reason, packages such as Shiny, Dash and Streamlit have become very popular with analysts working within open-source programming frameworks such as Python and R. They allow the creation of applications while adhering to the syntax that an analyst would be more familiar with. Abstracting all that front-end know-how behind some very straightforward Python code allows the analyst to focus their efforts on the parts of their job that they excel in - gaining insight from data.\nThere would be little value in emulating the excellent Posit documentation on getting started with Python Shiny. To do that, I suggest using their docs. In combination with the examples provided, a comprehensive introduction to Shiny is readily available. In the next section, I will focus on introducing a minimal application and then expanding on some of the more subtle considerations in the reactive flow of the backend. I’ll choose to focus on things where I’ve needed to learn by trial and error, or by searching the Stack Overflow boards through some of the inevitable roadblocks a person encounters when programming dashboards. I’ll throw in some common ‘gotchas’ for good measure.\nThe fastest way to get up and running with Python Shiny would be to play with the interactive code snippets that Posit makes available on their overview documentation. They host live Shiny sessions with the interactive app and backend code side-by-side. This is a great place to try things out and feel free to copy code from the examples I provide here."
  },
  {
    "objectID": "blogs/01-state-of-pyshiny.html#python-shiny-ecosystem.",
    "href": "blogs/01-state-of-pyshiny.html#python-shiny-ecosystem.",
    "title": "The State of Python Shiny",
    "section": "Python Shiny Ecosystem.",
    "text": "Python Shiny Ecosystem.\nThe success of Python Shiny will be down to the adoption & support of the open-source community. This was easier to achieve 10 years ago in the R framework as Shiny had no real competitors at that time. This is not the case in the modern-day Python framework. Even with its particular USP in comparison to Streamlit & Dash, programmers and analysts are creatures of habit. Considering the new player on the block as an alternative to what you already know requires a considerable argument. There will always be overhead in developing familiarity with a new solution’s particular quirks. That said, some of the community-developed packages in the more mature RShiny make building apps at scale very manageable. This final section of this blog takes a look at what Python Shiny bolt-ons we can already play with and a more wistful look over the programming divide at RShiny, considering some of the amazing packages that make me hopeful for the potential future of Python Shiny.\n\nWhat We Already Have.\n\nCompatibility With Jupyter Widgets.\nJupyter Widgets is a well-known add-in for Jupyter Notebooks, allowing for interactive widgets to be displayed and used in Notebooks. This allows for some interesting mini-dashboard opportunities to be served directly within the notebook itself, alongside the code. Having this compatibility from the start is great. While it does introduce a little duplication (Python Shiny has many of its own equivalent ui input widgets), the particular look and range of these widgets will be appealing to many seasoned Python devs. Many of the Jupyter Widget aficionados would have previously developed the capability in specifying callback logic, to be able to capture the user’s interactions with elements of the widget. For example, select a category in a table and use that active selection to filter a chart, the sort of thing that tends to be more prevalent in Business Insight software. Achieving this sort of interaction in Shiny definitely falls under ‘advanced use’. In R, the package crosstalk has significantly lowered the bar in achieving this. Hopefully a dedicated Python Shiny module or package makes its way to PyPi soon.\n\n\nDeploying An App With rsconnect-python.\nThis package is a useful command line interface (CLI) tool that allows you to build application manifests from your Shiny project and deploy them to services such as shinyapps.io. Once you’re ready to share an app with the wider community, check out the docs.\n\n\nModular Dashboards (Shiny Modules).\nThis aspect of Shiny dashboarding is a bit meta. Essentially, when you (or your organisation) gets to a point that they have enough dashboards to be concerned with thinking about consistency and efficiency, take a read through the Python Shiny Modules docs. This is a way of reusing elements of your dashboard, much in the same way in which you reuse code by writing a function. Maybe your organisation has really committed to Shiny and has developed a beautifully-styled interface with house branding, or some interface component such as an Email field with some complex regular expression checking the validity of the Email address. There really is no need to copy-paste that code when you want to use it again. Package the element up as a Shiny Module and you can roll out this feature across multiple dashboards, secure in the knowledge that the quality of this module has already been agreed. Responsibility successfully deferred!\n\n\n\n\n\n\nCode reuse is the Holy Grail of Software Engineering — Douglas Crockford\n\n\n\nThe fact that Python Shiny devs have prioritised modular dashboarding so early is great. It illustrates a commitment to reproducibility that is rightly lauded within the open source community. Just to note that the ability to do this in RShiny took approximately 3 years. Python users are quite spoiled to be able to make use of this feature from the outset.\n\n\n\nWhat I Really Want.\nWhile I feel generally positive about the future of Python Shiny, I do look across the gulf that exists between the Python and R frameworks with hungry eyes. Particularly at the rich, community-driven plethora of RShiny add-ins that make Shiny the undeniable dashboarding front-runner in that framework. Here is my wish-list of Python Shiny helper tools, ranked order (opinionated warning).\n\n1. Reactlog.\n\n\n\nThere’s no 2 ways about it, debugging apps is a royal pain. One of my go to helpers in times of need is Posit’s own reactlog package. Reactlog is such an understated tool, yet it has been crucial to me in understanding what’s going on under the hood of my apps. So much so, that I now feel hopelessly exposed when writing apps in Python without it to lean on.\n\n\n\nThis package allows you to launch an app, showing you useful insights about the performance and reactive flow of your app. It’s an app inception. It can be used in a variety of modes, you can record yourself clicking away on your app and then pause the interface to inspect variable statuses. Reactlog visualises the dependency graph of variables in your server and really is an indispensable debugging tool.\n\n\n2. Shinytest.\nUnit tests for your RShiny app anyone? Authored by the ubiquitous Hadley Wickham, shinytest allows you to make assertions about the behaviour of your application. In combination with the ability to design Shiny modules, this is a key piece of the RShiny toolkit allowing for the design of stable, production-ready Shiny components.\n\n\n3. DT Data Tables.\nAnother one of Posit’s own packages, DT helps to create beautiful, customisable and interactive data tables for the presentation of tables. Not just for RShiny applications, DT is also widely used in Rmarkdown reports. A dedicated Python Shiny solution would be most welcome.\n\n\n4. Shinyhelper.\nThe shinyhelper package developed by Chris Mason-Thom is an excellent solution for those involved applications that need a little more guidance for the user.\n\n\n\nShinyhelper allows the developer to include markdown files with extensive instructions, images and gif animations of how to use your application. These documents will be hidden from the screen until the user chooses to click on the help icons that shinyhelper provides. These help icons can be easily styled and positioned around any element within your ui. Once clicked, they will launch modal windows with your beautifully formatted instructions, allowing a wealth of guidance to be included within your app without causing excessive clutter.\nTry clicking through the different helper symbols in the demo RShiny application below."
  },
  {
    "objectID": "blogs/01-state-of-pyshiny.html#in-review.",
    "href": "blogs/01-state-of-pyshiny.html#in-review.",
    "title": "The State of Python Shiny",
    "section": "In Review.",
    "text": "In Review.\nPython Shiny is a promising addition to the Python programming framework. It has a USP, and Posit have the know-how to apply the learning from their past development of RShiny to help ensure it has a bright future. It is also worth noting that Posit have recently put a lot of effort into developing Quarto, which you may think of as like Rmarkdown but with native Python, R and Julia support. The reason why this is important is that often, interactive widgets and solutions designed for use in notebooks or markdown documents take little refactoring to get them running in Shiny apps. Or conversely, there are ways to convert rmarkdown docs into Shiny or Shiny-like applications. Understanding the synergies between these tools and how they have influenced the evolution of RShiny may give some insight into how we may expect Python Shiny & Quarto to develop in the years ahead.\nThe Python community is well-served by dashboarding solutions which will affect its wider adoption. If you are newer to dashboarding, or if you are looking for a solution that perhaps scales better than some of the other options available in the Python framework, then consider Python Shiny. It’s already pretty great and will only get better as the community warms to this new tool in the Python toolkit."
  },
  {
    "objectID": "blogs/03-quarto-github-actions.html",
    "href": "blogs/03-quarto-github-actions.html",
    "title": "How to Automate Quarto Builds with GitHub Actions",
    "section": "",
    "text": "You’re set up with a GitHub account.\nYou’re able to run git commands [1] from a command line interface (CLI).\nYou’ve installed quarto. [2]\nYou’ve a preferred text editor installed, eg Visual Studio Code, Atom or similar.\n\nThis guide is based on the useful quarto continuous integration (CI) documentation [3] and the examples provided within the Quarto CI GitHub repository [4]."
  },
  {
    "objectID": "blogs/03-quarto-github-actions.html#assumptions",
    "href": "blogs/03-quarto-github-actions.html#assumptions",
    "title": "How to Automate Quarto Builds with GitHub Actions",
    "section": "",
    "text": "You’re set up with a GitHub account.\nYou’re able to run git commands [1] from a command line interface (CLI).\nYou’ve installed quarto. [2]\nYou’ve a preferred text editor installed, eg Visual Studio Code, Atom or similar.\n\nThis guide is based on the useful quarto continuous integration (CI) documentation [3] and the examples provided within the Quarto CI GitHub repository [4]."
  },
  {
    "objectID": "blogs/03-quarto-github-actions.html#ci-for-quarto",
    "href": "blogs/03-quarto-github-actions.html#ci-for-quarto",
    "title": "How to Automate Quarto Builds with GitHub Actions",
    "section": "CI for Quarto",
    "text": "CI for Quarto\nThe repository used for setting up this example is available on GitHub.\nThe renderred site should look like this on GitHub Pages.\n\nIn the GitHub User Interface\n\nCreate a repository.\nCopy the clone url.\n\n\n\nIn the CLI\n\ncd to wherever you would like to keep your local clone.\ngit clone &lt;INSERT REPO URL&gt;\ncd &lt;INSERT REPO PATH&gt;\ntouch .github/workflows/publish-quarto.yml\ntouch index.qmd\ntouch .nojekyll\ntouch _quarto.yml\n\n\n\nIn the text editor\n\nCopy and paste the below into .github/workflows/publish-quarto.yml (you may need to enable viewing hidden files on your system - command + shift + . On macOS):\n\nname: Render and Publish\non:\n  push:\n    branches:\n      - main  # changes pushed to this branch will trigger a build.\n\njobs:\n  build-deploy:\n      runs-on: ubuntu-latest\n      permissions:\n        contents: write\n      steps:\n        - name: Check out repository\n          uses: actions/checkout@v3\n          \n        - name: Set up Quarto\n          uses: quarto-dev/quarto-actions/setup@v2\n          with:\n            version: 1.3.340\n\n        - name: Publish to GitHub Pages (and render)\n          uses: quarto-dev/quarto-actions/publish@v2\n          with:\n            target: gh-pages # renderred html files will be pushed here\n          env:\n            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # this secret is always available for github actions\n\nCopy paste the below into the index.qmd, using your preferred text editor:\n\n---\ntitle: Hello Quarto CI\ndate: last-modified\nresources:\n  - .nojekyll\n---\nSetting up CI for quarto website build & publish.\n\nCopy paste the following into _quarto.yml:\n\nproject:\n  type: website\n  output-dir: docs\nexecute:\n  freeze: auto\nformat: html\n\n\nBack in the CLI\n\nAt the project root: quarto render. This will make a docs folder with your rendered website, a directory called index_files with more site dependencies and a .gitignore file. The only file needed to be committed is the .gitignore.\necho /docs/ &gt;&gt; .gitignore\necho /index_files/ &gt;&gt; .gitignore\ngit status should look like this:\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .github/workflows/publish-quarto.yml\n        .gitignore\n        _quarto.yml\n        index.qmd\n\ngit add .\ngit commit -m \"Configure quarto\"\ngit push\n\n\n\nBack to the Web Browser\n\nIf the push was successful, navigate to your repository\nClick on the drop down arrow next to main branch\nClick on ‘view all branches’\nClick the ‘new branch’ button\nCreate the branch gh-pages\nClick on ‘settings’ in the top ribbon of the repo site\nClick on ‘Pages’ in the menu to the left\nCheck that your GitHub Pages is setup is Build and deployment &gt; Source &gt; Deploy from a branch\nCheck that the Branch setup is gh-pages /root\nAfter the CI has finished building, you can click on the url that appears at the top of this page under “GitHub Pages” to check that the site has been deployed properly. Copy the url of your GitHub Pages site.\n\n\n\nHead Back to your Local Repo\n\nInsert your url into your _quarto.yml, like below:\n\nproject:\n  type: website\n  output-dir: docs\nexecute:\n  freeze: auto\nformat: html\nwebsite:\n  site-url: \"&lt;YOUR_URL_HERE&gt;\" # makes site links work on your remote site"
  },
  {
    "objectID": "blogs/03-quarto-github-actions.html#creating-a-workflow-build-status-badge",
    "href": "blogs/03-quarto-github-actions.html#creating-a-workflow-build-status-badge",
    "title": "How to Automate Quarto Builds with GitHub Actions",
    "section": "Creating a Workflow Build Status Badge",
    "text": "Creating a Workflow Build Status Badge\n\nUse the following format to create a workflow build status badge in your readme: https://github.com/OWNER/REPOSITORY/actions/workflows/WORKFLOW-FILE/badge.svg\nFor example: https://github.com/r-leyshon/quarto-ci-example/actions/workflows/publish-quarto.yml/badge.svg\n\nFinally, embed the url in the src of a markdown image, like: ![example workflow](https://github.com/r-leyshon/quarto-ci-example/actions/workflows/publish-quarto.yml/badge.svg)\n\n\n\n\nexample workflow\n\n\n\nfin!"
  },
  {
    "objectID": "blogs/05-signed-commits.html",
    "href": "blogs/05-signed-commits.html",
    "title": "Set Up Signed Commits on GitHub",
    "section": "",
    "text": "GitHub verification badge."
  },
  {
    "objectID": "blogs/05-signed-commits.html#acknowledgement",
    "href": "blogs/05-signed-commits.html#acknowledgement",
    "title": "Set Up Signed Commits on GitHub",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis article merely collates information from the following sources:\n\nAdding a GPG key to your GitHub account\nGenerating a new GPG key\nHow to understand the gpg failed to sign the data problem in git\n\nFor more information and troubleshooting, please visit these sources as they contain additional guidance which may be helpful for operating systems other than macos."
  },
  {
    "objectID": "blogs/05-signed-commits.html#the-scenario",
    "href": "blogs/05-signed-commits.html#the-scenario",
    "title": "Set Up Signed Commits on GitHub",
    "section": "The Scenario",
    "text": "The Scenario\nYou need to set up commit verification on your computer for the first time. Possibly you have changed computer and need to quickly set up once more. You are on macos  with access to the terminal."
  },
  {
    "objectID": "blogs/05-signed-commits.html#what-youll-need",
    "href": "blogs/05-signed-commits.html#what-youll-need",
    "title": "Set Up Signed Commits on GitHub",
    "section": "What you’ll need:",
    "text": "What you’ll need:\n\nmacos\nYour GitHub username\nThe Email address associated with your GitHub account\nAccess to the command line via Command Line Interface (CLI)\nA secure password wallet"
  },
  {
    "objectID": "blogs/05-signed-commits.html#instructions",
    "href": "blogs/05-signed-commits.html#instructions",
    "title": "Set Up Signed Commits on GitHub",
    "section": "Instructions",
    "text": "Instructions\n\nIn terminal, run:\n\n\n\nterminal\n\ngit config --global commit.gpgsign true\ngit config --global tag.gpgsign true\n\n\nVisit GPG suite and download the installer.\nFollow the installation steps and quit the screen that attempts to create a new key\nIn terminal, create a key with:\n\n\n\nterminal\n\ngpg --full-generate-key\n\n\nAt the prompt, accept the default values for key type, size and persistence\nEnsure you enter your real name, as it appears on GitHub, under your GitHub profile avatar. Use the primary Email associated with your GitHub account.\nEnter a passphrase, confirm it and store it in a secure password wallet. You will need it again in the final step of this process\nPrint out the long format of the key details with:\n\n\n\nterminal\n\ngpg --list-secret-keys --keyid-format=long\n\n\nCopy the long form of the key ID from the example output labelled as &lt;COPY THIS BIT ONLY&gt;, do not include the preceeding forward slash:\n\n$ gpg --list-secret-keys --keyid-format=long\n/Users/hubot/.gnupg/secring.gpg\n------------------------------------\nsec   XXXX/&lt;COPY THIS BIT ONLY&gt; 2023-10-23 \nuid                          your username\nssb   xxxXXXX/XXXXXXXXXXXXXXXX 2023-10-23\n\nAdjust this command with your copied key ID and run in terminal:\n\n\n\nterminal\n\ngit config --global user.signingkey &lt;INSERT YOUR KEY ID&gt;\n\n\nPaste your key ID into the command below and execute in terminal:\n\n\n\nterminal\n\ngpg --armor --export &lt;INSERT YOUR KEY ID&gt;\n\n\nCopy the output, including the -----BEGIN PGP PUBLIC KEY BLOCK----- and -----END PGP PUBLIC KEY BLOCK----- sections.\nGo to the GPG Keychain app, it should have detected the key in your clipboard and ask you to import the key to your keychain. Click OK\nOver to your web brower, go to GitHub  profile pic  settings  SSH and GPG keys\nAdd a new key to your account, give it an appropriate title and paste the key from your clipboard\nGitHub will ask you to authenticate in order to make this change\nNow ensure Git knows where to look for your GPG program:\n\n\n\nterminal\n\nwhere gpg\n\nCopy the path to the GPG program.\n\nUpdate the command below with the path in your clipboard:\n\n\n\nterminal\n\ngit config --global gpg.program \"&lt;INSERT/PATH/HERE&gt;\"\n\n\nCheck that your git config file looks as expected:\n\n\n\nterminal\n\ngit config --global --list \n\nExample output:\nuser.name=&lt;YOUR GITHUB USERNAME&gt;\nuser.email=&lt;YOUR PRIMARY GITHUB EMAIL&gt;\nuser.signingkey=&lt;YOUR GPG KEY ID&gt;\ncommit.gpgsign=true\ngpg.program=&lt;PATH TO YOUR GPG PROGRAM&gt;\ntag.gpgsign=true\n\n\nThe next time you need to commit, you will be asked to enter the passphrase you saved to your password wallet in order to add the key to your keychain"
  },
  {
    "objectID": "blogs/05-signed-commits.html#troubleshooting",
    "href": "blogs/05-signed-commits.html#troubleshooting",
    "title": "Set Up Signed Commits on GitHub",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\ngpg: signing failed: Inappropriate ioctl for device\n\n\n\n\n\nAdd the below to your initialisation file (eg ~/.zshrc or equivalent):\n\n\n~/.zshrc\n\nGPG_TTY=$(tty)\nexport GPG_TTY\n\nRestart your terminal. Try to commit once more. You’ll be asked for the GPG passphrase that you stored in your password wallet.\n\n\n\n\nfin!"
  },
  {
    "objectID": "blogs/07-schedule-deploy-shinyapps.html",
    "href": "blogs/07-schedule-deploy-shinyapps.html",
    "title": "Scheduled Deployment to Shinyapps.io",
    "section": "",
    "text": "Wikimedia commons: Creative Commons"
  },
  {
    "objectID": "blogs/07-schedule-deploy-shinyapps.html#introduction",
    "href": "blogs/07-schedule-deploy-shinyapps.html#introduction",
    "title": "Scheduled Deployment to Shinyapps.io",
    "section": "Introduction",
    "text": "Introduction\nThis guide walks the reader through automated application update and deployment, a process known as scheduled deployment. GitHub Actions will be used to update application data and publish to shinyapps.io [1] for cloud hosting. This guide intends to help the casual developer keep their application data up-to-date.\n\n\n\n\n\n\nA Note on the Tools\n\n\n\n\n\nThe tooling used in this article may not suit all requirements. GitHub Actions and Shinyapps.io all provide free tier services, allowing for equitable access. These services are ideal for prototyping but come with limitations that may render them unsuitable for your purposes. For more information on these services and their various paid plans, please consult the Shinyapps.io features [2] and the GitHub Actions documentation [3]. This is not a product endorsement. Note that other cloud compute services provide alternative solutions to those presented in this article.\n\n\n\n\nIntended Audience\nAn experienced python and git practitioner, able to create and manage a virtual environment but less familiar with the shinyapps.io service or GitHub Actions.\n\n\nThe Scenario\nYou are considering building a dynamic application, presenting the latest view of some data to your users. You would like to automate the data retrieval and application publication processes.\n\n\nWhat You’ll Need:\n\nA GitHub account\nA shinyapps.io account\nA permissive firewall\nPython package manager (eg pip)\nPython environment manager (eg venv, poetry etc)\nAccess to a command line interface (CLI) such as terminal / Bash.\nPython requirements:\n\n\n\nrequirements.txt\n\nshiny\nrsconnect-python"
  },
  {
    "objectID": "blogs/07-schedule-deploy-shinyapps.html#develop-the-application",
    "href": "blogs/07-schedule-deploy-shinyapps.html#develop-the-application",
    "title": "Scheduled Deployment to Shinyapps.io",
    "section": "Develop the Application",
    "text": "Develop the Application\nThe steps in this guide result in a minimal application that reports the time that it was deployed to shinyapps.io.\n\nConfigure the Development Environment\n\n\n\n\n\nflowchart LR\n    B(Job: Install dependencies)\n \n\n\n\n\n\n\n\nCreate a new repository or; if you would rather skip the application development; clone this repository and proceed to the deployment stage.\nClone the repository into your local development environment.\nCreate a clean virtual environment with python 3.11. At the time of writing, this is the most current version of python available to shinyapps.io servers.\nActivate the environment.\nInstall the python dependencies in requirements.txt.\n\n\n\nPrepare the Data\n\n\n\n\n\nflowchart LR\n    B(Job: Install dependencies)\n    B ==&gt; C(Job: Run save_time.py)\n    C --&gt; D[saved_time.txt]\n \n\n\n\n\n\n\nThis job prepares the database on which the application will depend. In this trivial example, we simply save the current time as a formatted string to a text file. This simple artifact will later be read into a shiny app to present as the time of deployment.\n\nCreate a python file called save_time.py.\nPaste the following content into save_time.py and hit save:\n\n\n\nsave_time.py\n\nfrom datetime import datetime\nnw = datetime.now()\nnw = datetime.strftime(nw, \"%Y-%m-%d %H:%M:%S\")\nwith open(\"saved_time.txt\", \"w\") as f:\n    f.write(nw)\n    f.close()\nprint(f\"Saved time is {nw}\")\n\n\nRun the script from the terminal. A saved_time.txt file should appear in the project root:\n\n\n\nCLI\n\npython3 save_time.py\n\n\n\nPresent the Data\n\n\n\n\n\nflowchart LR\n    B(Job: Install dependencies)\n    B ==&gt; C(Job: Run save_time.py)\n    C --&gt; D[saved_time.txt]\n\n    G{{app.py}}\n    D -.-&gt; G  \n\n\n\n\n\n\nAn application is needed to present the data to your user. In this example, the app will simply read the date string created in the previous step, format the date string in a sentence and present it within the user interface.\n\nCreate a python file called app.py.\nPaste the following into app.py:\n\n\n\nsave_time.py\n\nfrom shiny import App, render, ui\n# get the saved time\nwith open(\"saved_time.txt\", \"r\") as f:\n    nw = f.readlines()[0]\n    f.close()\n\napp_ui = ui.page_fluid(\n    ui.h2(\"Testing Deploy Schedule.\"),\n    ui.output_text(\"txt\"),\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def txt():\n        return f\"Deployed at {nw}.\"\n\napp = App(app_ui, server)\n\n\nTest the application locally by running it. This can be done from the CLI with the command:\n\n\n\nCLI\n\npython -m shiny run ./app.py\n\n\nIf the application successfully launches, you should see a localhost URL printed. Command and click on this or paste it into your browser to confirm that the application successfully launches. The app should present the sentence Deployed at &lt;SOME_DATETIME_INSERTED_HERE&gt;."
  },
  {
    "objectID": "blogs/07-schedule-deploy-shinyapps.html#deploy-the-application",
    "href": "blogs/07-schedule-deploy-shinyapps.html#deploy-the-application",
    "title": "Scheduled Deployment to Shinyapps.io",
    "section": "Deploy the Application",
    "text": "Deploy the Application\n\nLocal Deploy\n\n\n\n\n\nflowchart LR\n    B(Job: Install dependencies)\n    B ==&gt; C(Job: Run save_time.py)\n    C --&gt; D[saved_time.txt]\n    C ==&gt; E(Job: Configure rsconnect)\n    H([SHINYAPPS_USERNAME\\nSHINYAPPS_TOKEN\\nSHINYAPPS_SECRET]) -.-&gt; E\n    E ==&gt; F(Job: Deploy to rsconnect)\n    F ==&gt; G{{shinyapps.io: serve app.py}}\n    D -.-&gt; G  \n\n\n\n\n\n\n\nThe first stage of deployment should be to manually upload the application to your shinyapps.io account. It is required to do this manually in the first instance, as it will allow you to retrieve an app ID for the deployment. This will ensure that when you use GitHub Actions to do the same, complications overwriting the application data are avoided. For more information on deploying to shinyapps.io, consult the python shiny cloud hosting documentation [4], the shinyapps.io documentation [5] and the Rsconnect-python documentation [6].\n\nFollow the shinyapps.io deployment documentation [5] to retrieve your username, token and secret. Store these somewhere secure.\n\n\n\n\n\n\n\nCaution\n\n\n\nTake precautions not to accidentally commit these credentials to your repository. I recommend both gitignoring the file that you stored them and using detect-secrets [7] if you are familiar with pre-commit hooks.\n\n\n\nConfigure an rsconnect server with the following command in your CLI, replacing the account, token and secret with your credentials:\n\n\n\nCLI\n\nrsconnect add --account &lt;YOUR_USERNAME&gt; --name rsconnect-server --token &lt;YOUR_TOKEN&gt; --secret &lt;YOUR_SECRET&gt;\n\nNote that you can give the server any name you wish, here I have used the imaginative name rsconnect-server. You must refer to this server name in &lt;YOUR_SERVER_NAME&gt; in the next step. If successful, you should see output in the CLI like below:\nChecking shinyapps.io credential...              [OK]\nUpdated shinyapps.io credential \"&lt;YOUR_SERVER_NAME&gt;\".\n\nNow use the configured rsconnect server to deploy your local app to the cloud, run the below command in the CLI, inserting the name of the server you configured in the previous step and an appropriate application title. Avoid using spaces in the application title as I have found this causes issues when deploying with certain versions of rsconnect-python:\n\n\n\nCLI\n\nrsconnect deploy shiny ./ --name &lt;YOUR_SERVER_NAME&gt;  --title &lt;ENTER_AN_APP_TITLE&gt;\n\nA successful deployment will confirm in the CLI like below:\nApplication successfully deployed to &lt;YOUR_APP_URL&gt;\n        [OK]\nSaving deployed information...  [OK]\nVerifying deployed content...   [OK]\n\nThe previous step creates a metadata directory called rsconnect-python. Add this to your .gitignore file.\nThe hosted application should launch in your default browser on successful deployment. If not, then visit the URL printed in the terminal output and check everything is working as expected.\n\n\n\nRemote Deploy\n\n\n\n\n\nflowchart LR\n    A[update.yml] ==&gt; B(Job: Install dependencies)\n    B ==&gt; C(Job: Run save_time.py)\n    C --&gt; D[saved_time.txt]\n    C ==&gt; E(Job: Configure rsconnect)\n    H([SHINYAPPS_USERNAME\\nSHINYAPPS_TOKEN\\nSHINYAPPS_SECRET]) -.-&gt; E\n    E ==&gt; F(Job: Deploy to rsconnect)\n    K([APP_ID]) -.-&gt; F\n    F ==&gt; G{{shinyapps.io: serve app.py}}\n    D -.-&gt; G    \n\n\n\n\n\n\nNow that the app has been successfully deployed, the app ID can be retrieved and used in a GitHub Action workflow to schedule this deployment. This stage will involve configuring environment variables and secrets in our GitHub repository. The guidance for configuring this is correct at the time of writing but be advised that GitHub frequently update their user interface and CI/CD functionality.\n\nConfigure the Repository Environment\n\nRetrieve the application ID. You can find this either in your shinyapps.io dashboard or by copying the value from the json file saved in the rsconnect-python directory, following a successful local deployment.\nNavigate to the GitHub repository in your browser.\nAccess the environment variables menu under Settings  Secrets and variables  Actions  Variables.\nSave an environment variable called SHINYAPPS_USERNAME with your shinyapps.io username. Ensure that the values are correct and authenticate when prompted by GitHub to save the variable. 2 factor authentication may be required so have your phone to hand.\nNow click on the Secrets tab and repeat this process for the three required secrets:\n\nAPP_ID\nSHINYAPPS_TOKEN\nSHINYAPPS_SECRET\n\nThese values will be encrypted and masked if you try to print them in workflow logs. Consult the GitHub Actions Security Documentation [8] for more on features and best practice.\n\n\n\nSet Up the Workflow\nIn this section, a script is created that will execute the job of updating the app and deploying to shinyapps.io. It will grab the secrets and variables created in the previous stage.\n\nCreate the following directory to store the workflow script:\n\n\n\nCLI\n\nmkdir -p .github/workflows\n\n\nIn that folder, create a YAML file that will contain the necessary logic to update and deploy the app.\n\n\n\nCLI\n\ntouch .github/workflows/update.yml\n\n\nUpdate the YAML file with the content below, note that I have included notes in the file to help understand the purpose of each step, but feel free to remove them if preferred:\n\n\n\nupdate.yml\n\nname: Update Application\non:\n  schedule:\n    - cron: '0 0 * * 6' # at midnight every Saturday, see https://crontab.guru/\njobs:\n  build:\n    runs-on: ubuntu-latest # this os tends to be quick & flexible\n    steps:\n      - uses: actions/checkout@v3 # premade action that allows your action to access your code\n      - uses: actions/setup-python@v3 # premade action that will install & configure python\n        with:\n          python-version: 3.11 # this needs to be a version compatible with shinyapps.io servers\n      - name: Install dependencies \n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n      - name: Update saved time # create the little saved_time.txt artifact\n        run: |\n          python3 save_time.py\n      - name: Configure rsconnect # set up an rsconnect server. Relies on repo vars & secrets\n        run: |\n          rsconnect add --account ${{ vars.SHINYAPPS_USERNAME }} --name rsconnect-server --token ${{ secrets.SHINYAPPS_TOKEN }} --secret ${{ secrets.SHINYAPPS_SECRET }}\n      - name: Deploy to rsconnect # app-id parameter allows reliable overwriting of app content without creating duplicates.\n        run: |\n          rsconnect deploy shiny --app-id ${{ secrets.APP_ID }} ./ --name rsconnect-server  --title scheduled-deployment\n\n\nOnce everything has been done, add and commit all the changes and push them to the remote. The next time the cron job triggers, navigate to the repository in a browser and inspect the workflow logs under Actions  All workflows.\nIf the Action executed successfully, you will see the topmost build log will present a blue checkmark. Click on the link to see the details.\nClick on the build label to expose the steps in the workflow. By clicking on the steps the logs can be expanded. Check the time of deployment under the Update saved time step.\nClick on the Deploy to rsconnect step. The final line should state that the application was successfully deployed to a URL. Click on the link to launch the app and confirm that the time of deployment matches that reported in the Update saved time log."
  },
  {
    "objectID": "blogs/07-schedule-deploy-shinyapps.html#tips-troubleshooting",
    "href": "blogs/07-schedule-deploy-shinyapps.html#tips-troubleshooting",
    "title": "Scheduled Deployment to Shinyapps.io",
    "section": "Tips & Troubleshooting",
    "text": "Tips & Troubleshooting\n\nTo see a list of your configured rsconnect servers, use the rsconnect list command in the CLI.\nTo remove a server, use rsconnect remove --name &lt;SERVER_NAME&gt; in the CLI.\nIf you encounter an error when deploying your app that states “not found”, try deleting the folder called rsconnect-python if it exists and run the deploy command once more.\nOnce you have used GitHub Actions to schedule the deployment, you may notice that the time of execution does not precisely match the time specified in the workflow file. GitHub will initiate the job at the time specified, but will queue the job until a runner is available to execute it. At periods of heavy traffic, you may experience delays of half an hour or more."
  },
  {
    "objectID": "blogs/07-schedule-deploy-shinyapps.html#conclusion",
    "href": "blogs/07-schedule-deploy-shinyapps.html#conclusion",
    "title": "Scheduled Deployment to Shinyapps.io",
    "section": "Conclusion",
    "text": "Conclusion\nIn this guide, we have produced a python shiny application and used GitHub Actions to schedule its deployment to the shinyapps hosting service.\nThis workflow can be adapted to serve a real business need. For example, a similar workflow can be used to produce a basic ‘bounty board’ app to help colleagues in an organisation extract GitHub issues and PRs (Code available here).\nThe workflow can also be improved to take advantage of the new GitHub Actions caching feature [9]. This will make subsequent runs faster as dependencies and configuration can be stored between runs.\nGet some action!\n\nAction Man GIFfrom Action GIFs\n\n\n\nfin!"
  },
  {
    "objectID": "blogs/09-cycling-network-r5py.html#introduction",
    "href": "blogs/09-cycling-network-r5py.html#introduction",
    "title": "Bicycle Network Modelling with r5py",
    "section": "Introduction",
    "text": "Introduction\nr5py is a relatively new transport modelling package available on PyPI. It provides convenient wrappers to Conveyal’s r5 Java library, a performant routing engine originating from the ubiquitous Open Trip Planner (OTP). Whereas r5py may not be as feature-rich as OTP, its unique strength is in the production of origin:destination matrices at scale. This is important if the intention is to produce stable statistics based on routing algorithms, where the idiosyncrasies of local transport service availability means that departure times can have a significant impact upon overall journey duration.\nr5py achieves stable statistics by calculating travel times over multiple journeys within a time window, returning summaries such as the median travel time from point A to B.\n\n\n\n\n\n\nA Note on the Purpose\n\n\n\nThis tutorial aims to familiarise the reader with r5py and how it integrates with the python geospatial ecosystem of packages. This article is not to be used to attempt to infer service quality outcomes. Limitations of this analysis and suggested improvements will be discussed throughout.\n\n\n\nIntended Audience\nExperienced python practitioners with a robust working knowledge of the typical python GIS stack, eg geopandas, shapely and folium and coordinate reference systems (CRS). Familiarity with r5py is not assumed.\n\n\nOutcomes\n\nIngest London bike charging station locations.\nVisualise charging stations in an interactive hex map.\nGenerate a naive point plane of destinations.\nCheck that the point plane is large enough to accommodate station locations.\nAdjust point plane locations to ensure routability.\nCalculate origin:destination travel time matrix, by cycling modality and with a maximum journey time of 30 minutes.\nEngineer features to help analyse the cycling network accessibility.\nVisualise the cycling network coverage and the most remote points within that area.\n\n\n\nWhat You’ll Need:\n\nConda or miniconda\npip package manager\nAbility to install Java Development Kit\nAbility to request from Transport for London api\nTutorial compatible with macos. subprocess calls may require adaptation for other operating systems.\n\n\n\nrequirements.txt\n\ncontextily\ngeopandas\nhaversine\nfolium\nmapclassify\nmatplotlib\npydeck\npyproj\nr5py\nrequests\nscikit-learn\n\n\n\nConfiguring Java\nIt is required to configure a Java Virtual Machine for this tutorial. The transport routing depends on this. Please consult the r5py installation documentation [1] for guidance. In order to check that you have a compatible Java environment, run r5py.TransportNetwork(osm_pth) after exercise 1. For reference, I have used OpenJDK to manage my Java instance:\nopenjdk 11.0.19 2023-04-18 LTS\nOpenJDK Runtime Environment Corretto-11.0.19.7.1 (build 11.0.19+7-LTS)\nOpenJDK 64-Bit Server VM Corretto-11.0.19.7.1 (build 11.0.19+7-LTS, mixed mode)"
  },
  {
    "objectID": "blogs/09-cycling-network-r5py.html#london-cycle-station-service-coverage",
    "href": "blogs/09-cycling-network-r5py.html#london-cycle-station-service-coverage",
    "title": "Bicycle Network Modelling with r5py",
    "section": "London Cycle Station Service Coverage",
    "text": "London Cycle Station Service Coverage\nStart by loading the required packages.\n\nfrom datetime import datetime, timedelta\nimport os\nimport subprocess\nimport tempfile \nfrom typing import Union\n\nimport contextily as ctx\nimport folium\nimport geopandas as gpd\nfrom haversine import haversine, Unit\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pydeck as pdk\nimport pyproj\nimport r5py\nimport requests\nfrom sklearn import preprocessing\nfrom shapely.geometry import LineString, Point, Polygon\n\n\nStreet Network Data\nFirstly, we must acquire information about the transport network. There are a few sources of this, but we shall use the BBBikes website to ingest London-specific OpenStreetMap extracts. The required data should be in protocol buffer (.pbf) format.\n\nExercise 1\nFind the appropriate url that points to the london.osm.pbf file. Ingest the data and store at an appropriate location.\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\nEither using python requests or subprocess with the curl command, request the url of the pbf file and output the response to a data folder.\n\n\n\n\n\nSolution\n\n\nShow the code\n# As the osm files are large, I will use a tmp directory, though feel free to\n# store the data wherever you like.\ntmp = tempfile.TemporaryDirectory()\nosm_pth = os.path.join(tmp.name, \"london.osm.pbf\")\nsubprocess.run(\n    [\n        \"curl\",\n        \"https://download.bbbike.org/osm/bbbike/London/London.osm.pbf\",\n        \"-o\",\n        osm_pth,\n    ]\n)\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 45  103M   45 47.4M    0     0  43.0M      0  0:00:02  0:00:01  0:00:01 43.0M100  103M  100  103M    0     0  49.8M      0  0:00:02  0:00:02 --:--:-- 49.8M\n\n\nCompletedProcess(args=['curl', 'https://download.bbbike.org/osm/bbbike/London/London.osm.pbf', '-o', '/var/folders/qc/rrswmfbx1_v0sxklq1kr28jw0000gp/T/tmprnd4etkn/london.osm.pbf'], returncode=0)\n\n\n\n\n\nBike Charging Station Locations\n\nExercise 2\nTo get data about the bike charging stations in London, we will query Transport for London’s BikePoint API.\n\nExplore the site and find the correct endpoint to query.\nThe tutorial requires the following fields: station ID, the human-readable name, latitude and longitude for each available station.\nStore the data in a geopandas geodataframe with the appropriate coordinate reference system.\nInspect the head of the geodataframe.\n\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\n\nUsing the requests package, send a get request to the endpoint.\nStore the required fields in a list: “id”, “commonName”, “lat”, “lon”.\nCheck that the response returned HTTP status 200. If True, get the content in JSON format.\nCreate an empty list to store the station data.\nIterate through the content dictionaries. If a key is present within the required fields, store the key and value within a temporary dictionary.\nAppend the dictionary of required fields and their values to the list of stations.\nConvert the list of dictionaries to a pandas dataframe. Then convert this to a geopandas geodataframe, using the coordinate reference system “EPSG:4326”. As we have lat and lon in separate columns, use geopandas points_from_xy(), ensuring you pass values in the order of longitude, latitude.\nPrint out the head of the stations gdf.\n\n\n\n\n\n\nSolution\n\n\nShow the code\nENDPOINT = \"https://api.tfl.gov.uk/BikePoint/\"\nresp = requests.get(ENDPOINT)\nif resp.ok:\n    content = resp.json()\nelse:\n    raise requests.exceptions.HTTPError(\n        f\"{resp.status_code}: {resp.reason}\"\n    )\n\nneeded_keys = [\"id\", \"commonName\", \"lat\", \"lon\"]\nall_stations = list()\nfor i in content:\n    node_dict = dict()\n    for k, v in i.items():\n        if k in needed_keys:\n            node_dict[k] = v\n    all_stations.append(node_dict)\n\nstations = pd.DataFrame(all_stations)\nstation_gdf = gpd.GeoDataFrame(\n    stations,\n    geometry=gpd.points_from_xy(stations[\"lon\"], stations[\"lat\"]),\n    crs=4326,\n)\n\nstation_gdf.head()\n\n\n\n\n\n\n\n\n\n\nid\ncommonName\nlat\nlon\ngeometry\n\n\n\n\n0\nBikePoints_1\nRiver Street , Clerkenwell\n51.529163\n-0.109970\nPOINT (-0.10997 51.52916)\n\n\n1\nBikePoints_2\nPhillimore Gardens, Kensington\n51.499606\n-0.197574\nPOINT (-0.19757 51.49961)\n\n\n2\nBikePoints_3\nChristopher Street, Liverpool Street\n51.521283\n-0.084605\nPOINT (-0.08460 51.52128)\n\n\n3\nBikePoints_4\nSt. Chad's Street, King's Cross\n51.530059\n-0.120973\nPOINT (-0.12097 51.53006)\n\n\n4\nBikePoints_5\nSedding Street, Sloane Square\n51.493130\n-0.156876\nPOINT (-0.15688 51.49313)\n\n\n\n\n\n\n\n\nThat’s all the external data needed for this tutorial. Let’s now examine the station locations.\n\n\n\nStation Density\nAs the stations are densely located in and around central London, a standard matplotlib point map would suffer from overplotting. A better way is to present some aggregated value on a map, such as density.\n\nExercise 3\nPlot the density of cycle station locations on a map. The solution will use pydeck, but any visualisation library that can handle geospatial data would be fine.\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\n\nCreate a pydeck hexagon layer based on the station_gdf. The hexagon layer should be configured as below:\n\n\nextruded\nposition from lon and lat columns\nelevation scale is 100\nelevation range from 0 through 100\ncoverage is 1\nradius of hexagons is 250 metres\n\n\nCreate a pydeck view state object to control the initial position of the camera. Configure this as you see fit.\n(Optional) Add a custom tooltip, clarifying that the hexagon elevation is equal to the count of stations within that area. You may wish to consult the deck.gl documentation to help implement this.\nCreate a deck with the hexagon layer, custom view state, tooltip and a map style of your choosing.\n\n\n\n\n\n\nSolution\n\n\nShow the code\n# pydeck visuals - concentration of charging stations by r250m hex\nlayer = pdk.Layer(\n    \"HexagonLayer\",\n    station_gdf,\n    pickable=True,\n    extruded=True,\n    get_position=[\"lon\", \"lat\"],\n    auto_highlight=True,\n    elevation_scale=100,\n    elevation_range=[0, 100],\n    coverage=1,\n    radius=250,  # in metres, default is 1km\n    colorRange=[\n        [255, 255, 178, 130],\n        [254, 217, 118, 130],\n        [254, 178, 76, 130],\n        [253, 141, 60, 130],\n        [240, 59, 32, 130],\n        [189, 0, 38, 130],\n    ], # optionally added rgba values to allow transparency\n)\nview_state = pdk.ViewState(\n    # longitude=-0.140,# value for iframe\n    longitude=-0.070,# value for code chunk\n    latitude=51.535, \n    # zoom=10, # value for iframe\n    zoom=10.5, # value for code chunk\n    min_zoom=5,\n    max_zoom=15,\n    pitch=40.5,\n    bearing=-27.36,\n)\ntooltip = {\"html\": \"&lt;b&gt;n Stations:&lt;/b&gt; {elevationValue}\"}\nr = pdk.Deck(\n    layers=[layer],\n    initial_view_state=view_state,\n    tooltip=tooltip, # prettier than default tooltip\n    map_style=pdk.map_styles.LIGHT,\n)\nr\n\n\n\n        \n    \n\n\n\n\n\nGenerate Destinations\nWe can use the bike stations as journey origins. To compute travel times, we need to generate destination locations. This tutorial uses a simple approach to generating equally spaced points within a user-defined bounding box.\n\n\n\n\n\n\nLimitation\n\n\n\nGenerating a point plane is a fast way to get a travel time matrix. However, this is naive to locations that the riders would prefer to start or finish their journeys. A more robust approach would be to use locations of retail or residential features. The European Commission’s Global Human Settlement Layer [2] data would provide centroids to every populated grid cell down to a 10m2 resolution.\n\n\n\nExercise 4\nWrite a function called create_point_grid() that will take the following parameters:\n\nbbox_list expecting a bounding box list in [xmin, ymin, xmax, ymax] format with epsg:4326 longitude & latitude values.\nstepsize expecting a positive integer, specifying the spacing of the grid points in metres.\n\ncreate_point_grid() should return a geopandas geodataframe of equally spaced point locations - the point grid. The grid point locations should be in epsg:4326 projection. The geodataframe requires a geometry column and an id column equal to the index of the dataframe (required for r5py origin:destination matrix calculation).\nOnce you are happy with the function, use it to produce an example geodataframe and explore() a folium map of the point grid.\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\nNote that epsg:4326 is a geodetic projection, unsuitable for measuring distance between points in the point plane. Ensure that the crs is re-projected to an appropriate planar crs for distance calculation.\n\nStore the South-West and North-East coordinates as shapely.geometry.Point objects.\nUse pyproj.Transformer.from_crs() to create 2 transformers, one from geodetic to planar, and the other back from planar to geodetic.\nUse the transformers to convert the SW and NE points from geodetic to planar.\nUse nested loops to iterate over the area between the corner points. Store the point location as a shapely.geometry.Point object. Append the point to a list of grid points.\nIncrement the x and y values by the provided stepsize and continue to append points in this fashion until xmin has reached the xmax value and ymin has met the ymax value.\nEnsure the stored coordinates are converted back to epsg:4326.\nCreate a pandas dataframe with the geometry column using the appended point locations and an id column that uses the range() function to generate a unique integer value for each row.\nUse the defined function with a sample bounding box and explore the resulting geodataframe with an interactive folium map.\n\n\n\n\n\n\nSolution\n\n\nShow the code\ndef create_point_grid(bbox_list: list, stepsize: int) -&gt; gpd.GeoDataFrame:\n    \"\"\"Create a metric point plane for a given bounding box.\n\n    Return a geodataframe of evenly spaced points for a specified bounding box.\n    Distance between points is controlled by stepsize in metres.  As\n    an intermediate step requires transformation to epsg:27700, the calculation\n    of points is suitable for GB only.\n\n    Parameters\n    ----------\n    bbox_list : list\n        A list in xmin, ymin, xmax, ymax order. Expected to be in epsg:4326.\n        Use https://boundingbox.klokantech.com/ or similar to export a bbox.\n    stepsize : int\n        Spacing of grid points in metres. Must be larger than zero.\n\n    Returns\n    -------\n    gpd.GeoDataFrame\n        GeoDataFrame in epsg:4326 of the point locations.\n\n    Raises\n    ------\n    TypeError\n        bbox_list is not type list.\n        Coordinates in bbox_list are not type float.\n        step_size is not type int.\n    ValueError\n        bbox_list is not length 4.\n        xmin is greater than or equal to xmax.\n        ymin is greater than or equal to ymax.\n        step_size is not a positive integer.\n\n    \"\"\"\n    # defensive checks\n    if not isinstance(bbox_list, list):\n        raise TypeError(f\"bbox_list expects a list. Found {type(bbox_list)}\")\n    if not len(bbox_list) == 4:\n        raise ValueError(f\"bbox_list expects 4 values. Found {len(bbox_list)}\")\n    for coord in bbox_list:\n        if not isinstance(coord, float):\n            raise TypeError(\n                f\"Coords must be float. Found {coord}: {type(coord)}\"\n            )\n    # check points are ordered correctly\n    xmin, ymin, xmax, ymax = bbox_list\n    if xmin &gt;= xmax:\n        raise ValueError(\n            \"bbox_list value at pos 0 should be smaller than value at pos 2.\"\n        )\n    if ymin &gt;= ymax:\n        raise ValueError(\n            \"bbox_list value at pos 1 should be smaller than value at pos 3.\"\n        )\n    if not isinstance(stepsize, int):\n        raise TypeError(f\"stepsize expects int. Found {type(stepsize)}\")\n    if stepsize &lt;= 0:\n        raise ValueError(\"stepsize must be a positive integer.\")\n\n    # Set up crs transformers. Need a planar crs for work in metres - use BNG\n    planar_transformer = pyproj.Transformer.from_crs(4326, 27700)\n    geodetic_transformer = pyproj.Transformer.from_crs(27700, 4326)\n    # bbox corners\n    sw = Point((xmin, ymin))\n    ne = Point((xmax, ymax))\n    # Project corners to planar\n    planar_sw = planar_transformer.transform(sw.x, sw.y)\n    planar_ne = planar_transformer.transform(ne.x, ne.y)\n    # Iterate over metric plane\n    points = []\n    x = planar_sw[0]\n    while x &lt; planar_ne[0]:\n        y = planar_sw[1]\n        while y &lt; planar_ne[1]:\n            p = Point(geodetic_transformer.transform(x, y))\n            points.append(p)\n            y += stepsize\n        x += stepsize\n    df = pd.DataFrame({\"geometry\": points, \"id\": range(0, len(points))})\n    gdf = gpd.GeoDataFrame(df, crs=4326)\n    return gdf\n\ncreate_point_grid(\n  bbox_list=[-0.5917,51.2086,0.367,51.7575], stepsize=5000).explore(min_zoom=9)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\nStudy Area Size\nBefore we go ahead with the transport modelling, it is important to check that the point plane is large enough to contain a 30 minute journey from any of the stations. 30 minutes is the current charging interval for the pay as you ride options [3]. Note that making the point plane larger or the step size smaller means more travel times to calculate, so there is a compromise to be met. Using too small a grid will introduce edge effects [4] and therefore limit the validity of the study.\n\nExercise 5\nCheck that the point plane study area is large enough to accommodate an estimation of reachable area from the station locations.\n\n\nPart 1\n\nCreate a point_plane object with create_point_grid() that has a bounding box of your choosing. Use a stepsize of 1000 metres.\nGet the boundary polygon of this point plane. This is the study area.\nBuffer the bike station locations. This buffered area should represent the reachable area from within a 30 minute ride of any station and a presumed average urban cycle speed of 18 kilometres per hour (an opinionated speed based on a sample of one overweight, middle-aged man’s Strava data). Store the geometry as a Polygon object.\nCheck that the study area completely contains the buffered stations.\n\n\n\nPart 2\nCreate a diagnostic plot displaying the station locations, buffered station polygon and study area polygon together on one map.\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\nPart 1\nFor this exercise, you may find the Klokantech bounding box tool useful. Ensure that the output is formatted as csv.\n\nCreate a point plane of your choosing. Use the total_bounds attribute to extract values for minx, miny, maxx, maxy.\nUse shapely.geometry.Polygon to convert the 4 values above into a rectangular bounding box.\nRe-project the stations geodataframe to a planar crs. Buffer this geodataframe by an appropriate value in metres, convert back to epsg:4326 and store the unary union.\nCheck that the study area polygon completely contains the buffered station polygon by using an appropriately named GeoDataFrame method.\n\nPart 2\n\nPlot the stations gdf using marker values “o”, an alpha of 0.5, markersize of 10 and red colour. Store in a variable called ax.\nPlot the study area boundary polygon, ensuring the ax value from the previous step is used. Use an alpha of 0.2.\nAdd a plot of the buffered stations to the same axes, ensuring a colour “red” and an alpha of 0.2.\nUse contextily to add a basemap to the axes, selecting an appropriate crs value and tile provider.\nUse matplotlib.pyplot to show the plot.\n\n\n\n\n\n\nSolution\nPart 1\n\n\nShow the code\npoint_plane = create_point_grid(\n    [-0.4, 51.35, 0.15, 51.65], stepsize=1000\n)\n# Get a boundary polygon for the study area\nminx, miny, maxx, maxy = point_plane.total_bounds\nbbox_polygon = gpd.GeoSeries(\n    [Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)])]\n)\n# buffer the stations by presumed urban cycle speed of 18 kph\nstation_buffer = gpd.GeoSeries(\n    Polygon(\n      station_gdf.to_crs(27700).buffer(9000).to_crs(4326).geometry.unary_union\n      )\n)\ncond = bbox_polygon.contains(station_buffer)[0]\nprint(\n  f\"Point grid contains buffered stations? {cond}\")\n\n\nPoint grid contains buffered stations? True\n\n\nPart 2\n\n\nShow the code\nax = station_gdf.plot(marker=\"o\", color=\"red\", alpha=0.5, markersize=10)\n# plot the bounding box of the point plane boundary in blue\nbbox_polygon.plot(ax=ax, alpha=0.2, edgecolor=\"black\")\n# plot the buffered potential travel region around the stations\n# in light red\nstation_buffer.plot(ax=ax, alpha=0.2, color=\"red\")\nctx.add_basemap(\n    ax, crs=station_gdf.crs.to_string(), source=ctx.providers.CartoDB.Positron\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nWith a study area adequately encompassing the proposed reachable area, proceed to the transport routing.\n\n\n\nRoutability\nAs we have created a point plane naive to the transport network, it is important to check that the points are reachable by bike. Any point falling within an inaccessible portion of the transport network will report a null travel time. r5py comes with a utility function that helps to ‘snap’ unreachable locations to the nearest feature of the travel network.\n\nExercise 6\n\n\nPart 1\nUsing the r5py quickstart documentation, instantiate a transport_network object, passing osm_pth as its single argument.\n\n\nPart 2\n\nNow using the r5py advanced use section, create a new column in the point_plane gdf called snapped_geometry.\nUse the correct method of the transport_network to adjust the existing geometry column.\nUse a maximum search radius of 500 metres and specify the bicycle modality. Inspect the resulting geodataframe.\n\n\n\nPart 3\nNow that we have a point plane with a routable geography, we can check to ensure that the maximum search radius was observed.\n\nCalculate the haversine distance in metres between the original geometry and the snapped_geometry coordinates.\nAssign the result to point_plane[\"snap_dist_m\"].\nInspect the distribution of the snap_dist_m column. Confirm that there are no values above the maximum search radius value.\n\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\nPart 1\n\nr5py is imported as r5py. You can inspect the available classes by using the dir() function.\n\nPart 2\n\nUse dir() on the transport_network object instantiated in part 1 to find the appropriate method for snapping the coordinates to the nearest network feature.\nSpecify the radius argument with an appropriate value.\nFor the street_mode argument, select the appropriate r5py transport mode. To find this value, use dir(r5py.TransportMode).\n\nPart 3\n\nThe haversine function is imported. Unfortunately, it expects coordinates to be in a latitude, longitude order.\nApply a lambda function across every row in the point_plane GeoDataFrame. You will need to specify axis=1 to achieve this.\nWithin the lambda function, calculate the haversine distance in metres between the original coordinates and the snapped_geometry coordinates. Assign the output to the correctly named column.\nInspect the distribution of the haversine distances in whichever way you feel. Can you confirm that no coordinate was adjusted beyond the maximum search radius used when you instantiated the snapped_geometry column?\n\n\n\n\n\n\nSolution\n\n\nPart 1\n\n\nShow the code\ntransport_network = r5py.TransportNetwork(osm_pth)\n\n\nThis may seem like a small step, but if this passed, then you have correctly configured your Java Virtual Machine.\n\n\nPart 2\n\n\nShow the code\npoint_plane[\"snapped_geometry\"] = transport_network.snap_to_network(\n    point_plane[\"geometry\"],\n    radius=500,\n    street_mode=r5py.TransportMode.BICYCLE,\n)\npoint_plane.head()\n\n\n\n\n\n\n\n\n\n\ngeometry\nid\nsnapped_geometry\n\n\n\n\n0\nPOINT (-0.40000 51.35000)\n0\nPOINT (-0.40010 51.34960)\n\n\n1\nPOINT (-0.39463 51.34995)\n1\nPOINT (-0.39420 51.34960)\n\n\n2\nPOINT (-0.38926 51.34990)\n2\nPOINT (-0.38927 51.35027)\n\n\n3\nPOINT (-0.38390 51.34985)\n3\nPOINT (-0.38468 51.35024)\n\n\n4\nPOINT (-0.37853 51.34980)\n4\nPOINT (-0.37851 51.34984)\n\n\n\n\n\n\n\n\n\n\nPart 3\n\n\nShow the code\n# reverse the lonlat to latlon\npoint_plane[\"snap_dist_m\"] = point_plane.apply(\n    lambda row:haversine(\n        (row[\"geometry\"].y, row[\"geometry\"].x),\n        (row[\"snapped_geometry\"].y, row[\"snapped_geometry\"].x),\n        unit=Unit.METERS), axis=1)\n\npoint_plane[\"snap_dist_m\"].plot.hist(\n    bins=100,\n    title=\"Distribution of coordinate snap distance in point plane (m)\"\n    )\n\n\n\n\n\n\n\n\n\nHere we can confirm that the distribution of the haversine distances is strongly right-skewed. There appear to be no values greater than 450 metres.\n\npoint_plane[\"snap_dist_m\"].describe()\n\ncount    5871.000000\nmean       42.857620\nstd        56.671823\nmin         0.013986\n25%        11.194528\n50%        24.091469\n75%        48.486417\nmax       491.635701\nName: snap_dist_m, dtype: float64\n\n\nIn describing the column we can observe the actual maximum haversine distance of 492 metres. Note the differences in the mean and median are attributable to the strong skew in the distance distribution.\nThe spatial adjustment caused by the snapping can be visualised. This is useful for sanity-checking and edge-case investigation.\n\n\nExercise 7\n\n\nPart 1\n\nCreate a deep copy of the point_plane geodataframe and assign to a new object called largest_snaps.\nRetrieve the rows with the largest 100 snap_dist_m values. Visualising the points is expensive, so we will only display a subset.\nApply a lambda function over largest_snaps, creating a LineString between each geometry and snapped_geometry value. This will be used to draw a line between each point on a map.\nAssign the output to largest_snaps[\"lines\"].\nInspect the head of the resultant geodataframe.\n\n\n\nPart 2\n\nPlot largest_snaps on a folium map. Add a marker layer with red icons showing the locations of the original point plane geometry.\nAdd a second marker layer to the same folium map, adding the snapped geometry with a green marker.\nAdd the final layer to the map - the lines connecting the geometries to their snapped equivalents.\nShow the map.\n\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\nPart 1\n\nSort ascending the largest_snaps geodataframe by snap_dist_m. Take the top 100 records only.\nLineString takes 2 arguments, a start and end point coordinate.\nApply the LineString logic over each row in the geodataframe, ensuring the axis argument is set to 1.\nFor each row, pass the geometry column value to LineString as the first argument, and the snapped_geometry value as the second.\nAssign the returned value to an appropriately named column.\n\nPart 2\nThis exercise will exploit the kwargs available through the GeoDataFrame.explore() method. Alternatively, the map can be built from scratch using the folium library.\n\nCall explore on largest_snaps, specifying a “marker” marker_type. Use the marker_kwds parameter to pass a dictionary specifying which font-awesome icon to use for the geometry points. Assign the map to imap.\nSet the geometry to the snapped_geometry column and repeat the above process, this time specifying green markers for the points. Ensure the m parameter is set to imap in order to add this to the same map object.\nLastly, set the geometry to the lines column and explore, again setting m=imap to add the line geometries to the interactive map. Specify an appropriate zoom level.\nShow the map and explore the adjusted geometries.\n\n\n\n\n\n\nSolution\n\n\nPart 1\n\n\nShow the code\nlargest_snaps = point_plane.copy(deep=True)\n# retrieve the top 100 rows where coordinates adjusted by the greatest dist.\nlargest_snaps = largest_snaps.sort_values(\n    by=\"snap_dist_m\", ascending=False).head(100)\n# create the LineString geometry\nlargest_snaps[\"lines\"] = largest_snaps.apply(\n    lambda row: LineString([row[\"geometry\"], row[\"snapped_geometry\"]]),\n    axis=1\n)\nlargest_snaps.head()\n\n\n\n\n\n\n\n\n\n\ngeometry\nid\nsnapped_geometry\nsnap_dist_m\nlines\n\n\n\n\n825\nPOINT (-0.39423 51.39259)\n825\nPOINT (-0.39746 51.39652)\n491.635701\nLINESTRING (-0.39423 51.39259, -0.39746 51.39652)\n\n\n5388\nPOINT (-0.22671 51.62509)\n5388\nPOINT (-0.22553 51.62089)\n473.637347\nLINESTRING (-0.22671 51.62509, -0.22553 51.62089)\n\n\n5557\nPOINT (0.12522 51.62996)\n5557\nPOINT (0.11988 51.62753)\n457.484858\nLINESTRING (0.12522 51.62996, 0.11988 51.62753)\n\n\n5839\nPOINT (-0.01873 51.64566)\n5839\nPOINT (-0.02477 51.64732)\n455.327799\nLINESTRING (-0.01873 51.64566, -0.02477 51.64732)\n\n\n5632\nPOINT (-0.02407 51.63508)\n5632\nPOINT (-0.03049 51.63476)\n444.667055\nLINESTRING (-0.02407 51.63508, -0.03049 51.63476)\n\n\n\n\n\n\n\n\n\n\nPart 2\n\n\nShow the code\n# layer 1\nz_start = 9\nimap = largest_snaps.explore(\n    marker_type=\"marker\",\n    marker_kwds={\n        \"icon\": folium.map.Icon(color=\"red\", icon=\"ban\", prefix=\"fa\"),\n    },\n    map_kwds={\n        \"center\": {\"lat\": 51.550, \"lng\": -0.070}\n    },\n    zoom_start=z_start,\n)\n# layer 2\nimap = largest_snaps.set_geometry(\"snapped_geometry\").explore(\n    m=imap,\n    marker_type=\"marker\",\n    marker_kwds={\n        \"icon\": folium.map.Icon(color=\"green\", icon=\"bicycle\", prefix=\"fa\"),\n    }\n)\n# layer 3\nimap = largest_snaps.set_geometry(\"lines\").explore(\n    m=imap,\n    zoom_start=z_start\n)\nimap\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "blogs/09-cycling-network-r5py.html#travel-times",
    "href": "blogs/09-cycling-network-r5py.html#travel-times",
    "title": "Bicycle Network Modelling with r5py",
    "section": "Travel Times",
    "text": "Travel Times\nNow that we have everything we need to carry out the transport routing, there is one final adjustment to the point plane - ensure that the snapped geometries are set as the geometry column. The snapped distance and original geometry column can be removed.\n\npoint_plane.drop(columns=[\"geometry\", \"snap_dist_m\"], axis=1, inplace=True)\npoint_plane.rename(columns={\"snapped_geometry\": \"geometry\"}, inplace=True)\npoint_plane.set_geometry(\"geometry\", inplace=True)\npoint_plane.head()\n\n\n\n\n\n\n\n\n\nid\ngeometry\n\n\n\n\n0\n0\nPOINT (-0.40010 51.34960)\n\n\n1\n1\nPOINT (-0.39420 51.34960)\n\n\n2\n2\nPOINT (-0.38927 51.35027)\n\n\n3\n3\nPOINT (-0.38468 51.35024)\n\n\n4\n4\nPOINT (-0.37851 51.34984)\n\n\n\n\n\n\n\n\n\nCompute the Travel Time Matrix\nNow we use r5py to compute the travel times from the station locations to the adjusted point plane.\nSome notes on the parameters:\n\ntransport_modes takes a list of modes. For simplicity, only BICYCLE is used, though combining with WALK would allow routing through portions of the travel network tagged as pedestrian-only.\ndeparture_time takes a datetime that signifies the first trip start time and date. This should ideally match the date that you ingested the osm file, as the osm files will best represent the real-world state of the transport network.\ndeparture_time_window creates a time window to carry out routing operations. The first journey occurs at 8am, as specified by departure. Then r5py processes a journey at every minute until the departure_time_window is met. By default, r5py returns the median travel time across these journeys. This feature provides stable statistics, particularly when working with bus or train timetable data.\nThe points can be snapped during the computation job, by specifying snap_to_network=True. As we have already specified how to do this, we can set this to False.\nmax_time takes a timedelta object. Here we specify that the journeys should take no longer than 30 minutes, as this is the current hire charge period for the bikes.\nThe compute_travel_times method returns a pandas DataFrame of median travel times for every origin, destination combination.\n\nThis step can be expensive, depending on the size of your point plane. Don’t be alarmed by NaN in the travel_time column. These are points in the point plane that were not reachable from any bike station.\n\ndept_time = datetime.now().replace(hour=8, minute=0, second=0, microsecond=0)\n\ntravel_time_matrix = r5py.TravelTimeMatrixComputer(\n    transport_network,\n    origins=station_gdf,\n    destinations=point_plane,\n    transport_modes=[r5py.TransportMode.BICYCLE],\n    departure=dept_time,\n    departure_time_window=timedelta(minutes=10),\n    snap_to_network=False,\n    max_time=timedelta(minutes=30),\n).compute_travel_times()\n\ntravel_time_matrix.dropna().head()\n\n\n\n\n\n\n\n\n\nfrom_id\nto_id\ntravel_time\n\n\n\n\n2938\nBikePoints_1\n2938\n29.0\n\n\n2939\nBikePoints_1\n2939\n28.0\n\n\n3038\nBikePoints_1\n3038\n29.0\n\n\n3039\nBikePoints_1\n3039\n29.0\n\n\n3040\nBikePoints_1\n3040\n30.0\n\n\n\n\n\n\n\n\n\n\nFeature Engineering\nIn order to produce some informative visuals, the travel time matrix will need some summarisation and feature engineering.\n\nExercise 8\nProduce a table called med_tts with the following spec:\nid                       int64\ngeometry              geometry\nmedian_tt              float64\nn_stations_serving       int16\nn_stations_norm        float64\ninverted_med_tt        float64\nlisted_geom             object\n\n\nPart 1\n\nCalculate the median travel time for every point in the point plane across the stations, call the column median_tt.\nCalculate the number of bike stations that can reach each point in the point plane. Ensure that this column is formatted as “int16” and is called n_stations_serving.\nJoin the medians and number of stations to point_plane on to_id.\n\n\n\nPart 2\n\nUse a minmax scaler to scale the number of stations serving each point in the point plane. Assign this to med_tts[\"n_stations_norm\"].\nCalculate inverteded values for the median travel time. Use the same scaling as the previous step and assign to med_tts[\"inverted_med_tt\"].\nCreate a column with a list of the coordinate for each row, in [long, lat] order. This is the geometry format expected by pydeck. Assign this to med_tts[\"listed_geom\"].\n\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\nPart 1\n\nTo create the median travel times across bike stations, group by the to_id column and calculate the median.\nTo get the number of stations reaching each point, group by the to_id column and count the number of from_id values.\nJoin the above grouped dataframes to the point plane dataframe, using the to_id column as the search key. Ensure the columns are named as specified in the exercise.\nCast n_stations_serving to int16. First, ensure that NaNs get encoded as 0. To do this, create a boolean mask where that column is na and assign the column values that meet that boolean index to 0. Then casting to int will be possible.\n\nPart 2\n\nInstantiate a min_max_scaler using an appropriate class from sklearn.\nReshape the values of n_stations_serving into a 2D numpy array, containing enough rows for each value.\nUsing the min_max_scaler, transform the numpy array.\nFlatten the output of min_max_scaler and assign to a column called n_stations_norm.\nCreate a new column inverted_med_tt, which subtracts all median_tt values from the maximum value in that column.\nScale inverted_med_tt by repeating steps 2 through 4 for this column.\nUse a list comprehension to extract each of the geometry coordinate values as a long,lat list. Assign to listed_geom.\n\n\n\n\n\n\nSolution\n\n\nShow the code\ndef get_median_tts_for_all_stations(\n    tt: pd.DataFrame, pp: gpd.GeoDataFrame\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Join travel times point geometries & calculate medians.\n\n    Parameters\n    ----------\n    tt : pd.DataFrame\n        Matrix of travel times where `from_id` are stations and `to_id` are\n        points in the point plane.\n    pp : gpd.GeoDataFrame\n        Locations in the point plane.\n\n    Returns\n    -------\n    gpd.GeoDataFrame\n        Point plane locations with median travel times from stations and number\n        of stations serving each point.\n\n    \"\"\"\n    #----- Part 1 -----\n    # get median travel time from all stations:\n    med_tts = tt.groupby(\"to_id\")[\"travel_time\"].median()\n    # get number of stations serving each point in the grid\n    tt_dropna = tt.dropna()\n    n_stations = tt_dropna.groupby(\"to_id\")[\"from_id\"].count()\n    df = pp.join(med_tts).join(n_stations)\n    df = df.rename(\n        columns={\"travel_time\": \"median_tt\", \"from_id\": \"n_stations_serving\"}\n    )\n    # need integer for n_stations_serving but there are NaNs\n    bool_ind = df[\"n_stations_serving\"].isna()\n    df.loc[bool_ind, \"n_stations_serving\"] = 0.0\n    df[\"n_stations_serving\"] = df[\"n_stations_serving\"].astype(\"int16\")\n    #----- Part 2 -----\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x = df[\"n_stations_serving\"].values.reshape(-1, 1)\n    x_scaled = min_max_scaler.fit_transform(x)\n    df[\"n_stations_norm\"] = pd.Series(x_scaled.flatten())\n    max_tt = max(df[\"median_tt\"].dropna())\n    df[\"inverted_med_tt\"] = (max_tt - df[\"median_tt\"])\n    x = df[\"inverted_med_tt\"].values.reshape(-1, 1)\n    x_scaled = min_max_scaler.fit_transform(x)\n    df[\"inverted_med_tt\"] = pd.Series(x_scaled.flatten())\n    out_gdf = gpd.GeoDataFrame(df, crs=4326)\n    out_gdf[\"listed_geom\"] = [[c.x, c.y] for c in out_gdf[\"geometry\"]]\n\n    return out_gdf\n\n\nmed_tts = get_median_tts_for_all_stations(travel_time_matrix, point_plane)\nmed_tts.dropna().head()\n\n\n\n\n\n\n\n\n\n\nid\ngeometry\nmedian_tt\nn_stations_serving\nn_stations_norm\ninverted_med_tt\nlisted_geom\n\n\n\n\n1386\n1386\nPOINT (-0.14795 51.41766)\n30.0\n1\n0.002660\n0.117647\n[-0.1479481, 51.4176588]\n\n\n1388\n1388\nPOINT (-0.13660 51.41752)\n29.0\n1\n0.002660\n0.176471\n[-0.1366036, 51.4175184]\n\n\n1469\n1469\nPOINT (-0.25464 51.42343)\n31.0\n1\n0.002660\n0.058824\n[-0.2546385, 51.4234283]\n\n\n1470\n1470\nPOINT (-0.25322 51.42198)\n32.0\n2\n0.005319\n0.000000\n[-0.2532204, 51.4219807]\n\n\n1472\n1472\nPOINT (-0.23869 51.42585)\n27.5\n2\n0.005319\n0.264706\n[-0.2386902, 51.4258517]\n\n\n\n\n\n\n\n\nAs this final GeoDataFrame is all that is required for the remaining tutorial, go ahead and tidy up the environment.\n\n# Tidy up\ntmp.cleanup() # only needed if tmp directory used\ndel [station_gdf, travel_time_matrix, point_plane, transport_network, imap,\nlargest_snaps, z_start]\n\n\n\n\nVisualise Travel Time\n\nExercise 9\nUsing the pydeck scatterplot layer docs, write a function that will take the med_tts GeoDataFrame and plot an interactive map. Use the features available in med_tts to customise the point radius and colour. Use the function to render an interactive map.\n\n\nShow the code\ndef make_scatter_deck(\n    gdf: gpd.GeoDataFrame,\n    radius=\"(n_stations_norm * 20) + 20\",\n    start_lon: float=-0.005,\n    start_lat: float=51.518,\n    start_zoom: Union[int, float]=10,\n) -&gt; pdk.Deck:\n    \"\"\"Render a scatterPlotLayer of travel time & number of stations serving.\n\n    Intended for use with output of ./src/tt-london-bikes.py -&gt;\n    ./data/interim/median_travel_times.pkl.\n\n    Parameters\n    ----------\n    gdf : gpd.GeoDataFrame\n        Table of grid locations and travel time from station locations.\n    radius : str, optional\n        A valid pydeck get_radius expression, by default\n        \"(n_stations_norm * 25) + 20\"\n    start_lon: float, optional\n        The starting view longitude, by default -0.110.\n    start_lat: float, optional\n        The starting view latitude, by default 51.518.\n    start_zoom: Union[int, float], optional\n        The starting view zoom, by default 10.\n\n    Returns\n    -------\n    pdk.Deck\n        A rendered interactive map.\n\n    \"\"\"\n    # some basic defensive checks\n    if not isinstance(gdf, gpd.GeoDataFrame):\n        raise TypeError(f\"gdf expected gpd.GeoDataFrame, found {type(gdf)}\")\n    expected_cols = [\n        \"n_stations_serving\",\n        \"listed_geom\",\n        \"inverted_med_tt\",\n        \"median_tt\",\n    ]\n    coldiff = sorted(list(set(expected_cols).difference(gdf.columns)))\n    if coldiff:\n        raise AttributeError(f\"Required column names are absent: {coldiff}\")\n    gdf = gdf.rename(columns={\"n_stations_serving\": \"n_stats\"})\n    # create deck layers\n    layer = pdk.Layer(\n        \"ScatterplotLayer\",\n        gdf,\n        pickable=True,\n        opacity=0.2,\n        stroked=True,\n        filled=True,\n        radius_scale=6,\n        radius_min_pixels=1,\n        radius_max_pixels=100,\n        line_width_min_pixels=1,\n        get_position=\"listed_geom\",\n        get_radius=radius,\n        get_fill_color=\"[255,(median_tt * 255), (inverted_med_tt * 255)]\",\n        get_line_color=\"[255,(median_tt * 255), (inverted_med_tt * 255)]\",\n    )\n    # Set the viewport location\n    view_state = pdk.ViewState(\n        longitude=start_lon,\n        latitude=start_lat,\n        zoom=start_zoom,\n        min_zoom=5,\n        max_zoom=15,\n        pitch=40.5,\n        bearing=-27.36,\n    )\n    tooltip = {\n        \"text\": \"Stations serving: {n_stats}\\nMdn travel time: {median_tt}\"\n    }\n    # Render\n    r = pdk.Deck(\n        layers=[layer], initial_view_state=view_state, tooltip=tooltip\n    )\n    return r\n\n\nr = make_scatter_deck(med_tts)\nr\n\n\n\n        \n    \n\n\nTake some time to study the map. Pan in and out by scrolling. The angle of the map can be adjusted by shift + clicking and dragging the mouse left to right. Find the point with the greatest number of stations serving. Look for any curious patterns in the median travel time. Take a look at the Greenwich peninsula, where the River Thames appears to limit accessibility from the North.\n\n\nExercise 10\nLastly, we can identify those points within the thirty minute reachable area from the bike station network that are the most remote. Let’s visualise the 20 most remote locations. These would represent candidate locations for increasing the coverage of the bike network within the thirty minute reachable zone. Once more, please note that the point grid is naive to locations where people live or would like to travel to and from.\n\n\n\n\n\n\nClick to expand hint\n\n\n\n\n\n\nSort descending the med_tt dataframe on median_tt and n_stations_serving.\nTake the top 20 records.\nPass this dataframe to the function you wrote in exercise 9.\n\n\n\n\n\n\nShow the code\nn_isolated = (\n    med_tts.dropna()\n    .sort_values([\"median_tt\", \"n_stations_serving\"], ascending=[False, False])\n    .head(20)\n)\nr = make_scatter_deck(n_isolated, radius=50, start_zoom=9.5)\nr"
  },
  {
    "objectID": "blogs/09-cycling-network-r5py.html#tips",
    "href": "blogs/09-cycling-network-r5py.html#tips",
    "title": "Bicycle Network Modelling with r5py",
    "section": "Tips",
    "text": "Tips\n\nMost of the packages used in this tutorial expect the sequence of coordinates to be long, lat. If you select an alternative method for mapping visuals, remember to check that this is consistent. If you are getting empty interactive map visuals, pan out and check if an incorrect coordinate specification has resulted in your points being rendered off the East coast of Africa.\nThe exception to the above rule is haversine, which expects lat, long."
  },
  {
    "objectID": "blogs/09-cycling-network-r5py.html#conclusion",
    "href": "blogs/09-cycling-network-r5py.html#conclusion",
    "title": "Bicycle Network Modelling with r5py",
    "section": "Conclusion",
    "text": "Conclusion\nIf you made it this far, well done! This tutorial has got you up and running with the r5py package. You have used several libraries in the python GIS stack to execute an analysis of bike station locations in London:\n\nStation locations were ingested with TfL’s api.\nOpenStreetMap data was ingested from the BBBikes download service.\nA point plane of your size was generated.\nThe coverage of the point plane was inspected.\nThe point plane features were adjusted to the underlying transport network.\nMedian travel times from bike stations to the point plane were calculated.\nTravel times were summarised and features engineered for presentation in informative pydeck maps.\n\nNext steps for this work would be to explore how potential station locations could be informed by places where people would wish to commute, such as retail, businesses, tourist attractions and residential areas. OpenStreetMap files are a rich source of location data, which can be extracted with libraries such as pyosmium.\nService optimisation is an interesting area of operational research. This analysis does not consider local demand on the stations and their capacity to meet that demand. This sort of problem is known as the capacitated facility location problem and can provide operational efficiencies while improving service quality for the customer.\n\nfin!"
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html",
    "href": "blogs/11-fiddly-bits-of-pytest.html",
    "title": "Pytest Fixtures in Plain English",
    "section": "",
    "text": "Creative commons license by Ralph"
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html#introduction",
    "href": "blogs/11-fiddly-bits-of-pytest.html#introduction",
    "title": "Pytest Fixtures in Plain English",
    "section": "Introduction",
    "text": "Introduction\npytest is a testing package for the python framework. It is broadly used to quality assure code logic. This article discusses using test data as fixtures with pytest and is the first in a series of blogs called pytest in plain English, favouring accessible language and simple examples to explain the more intricate features of the pytest package.\nFor a wealth of documentation, guides and how-tos, please consult the pytest documentation.\n\n\n\n\n\n\nA Note on the Purpose (Click to expand)\n\n\n\n\n\nThis article intends to discuss clearly. It doesn’t aim to be clever or impressive. Its aim is to extend the audience’s understanding of the more intricate features of pytest by describing their utility with simple code examples.\n\n\n\n\nIntended Audience\nProgrammers with a working knowledge of python and some familiarity with pytest and packaging. The type of programmer who has wondered about how to optimise their test code.\n\n\nWhat You’ll Need:\n\nPreferred python environment manager (eg conda)\npip install pytest==8.1.1\nGit\nGitHub account\nCommand line access\n\n\n\nPreparation\nThis blog is accompanied by code in this repository. The main branch provides a template with the minimum structure and requirements expected to run a pytest suite. The repo branches contain the code used in the examples of the following sections.\nFeel free to fork or clone the repo and checkout to the example branches as needed.\nThe example code that accompanies this article is available in the fixtures branch of the example code repo."
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html#what-are-fixtures",
    "href": "blogs/11-fiddly-bits-of-pytest.html#what-are-fixtures",
    "title": "Pytest Fixtures in Plain English",
    "section": "What are fixtures?",
    "text": "What are fixtures?\nData. Well, data provided specifically for testing purposes. This is the essential definition for a fixture. One could argue the case that fixtures are more than this. Fixtures could be environment variables, class instances, connection to a server or whatever dependencies your code needs to run.\nI would agree that fixtures are not just data. But that all fixtures return data of some sort, regardless of the system under test."
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html#when-would-you-use-fixtures",
    "href": "blogs/11-fiddly-bits-of-pytest.html#when-would-you-use-fixtures",
    "title": "Pytest Fixtures in Plain English",
    "section": "When would you use fixtures?",
    "text": "When would you use fixtures?\nIt’s a bad idea to commit data to a git repository, right? Agreed. Though fixtures are rarely ‘real’ data. The data used for testing purposes should be minimal and are usually synthetic.\nMinimal fixtures conform to the schema of the actual data that the system requires. These fixtures will be as small as possible while capturing all known important cases. Keeping the data small maintains a performant test suite and avoids problems associated with large files and git version control.\nIf you have ever encountered a problem in a system that was caused by a problematic record in the data, the aspect of that record that broke your system should absolutely make it into the next version of your minimal test fixture. Writing a test that checks that the codebase can handle such problem records is known as ‘regression testing’ - safeguarding against old bugs resurfacing when code is refactored or new features are implemented. This scenario commonly occurs when a developer unwittingly violates Chesterton’s Principle.\n\n\nMany thanks to my colleague Mat for pointing me towards this useful analogy. A considerate developer would probably include a comment in their code about a specific problem that they’ve handled (like erecting a sign next to Chesterton’s fence). An experienced developer would do the same, and also write a regression test to ensure the problem doesn’t re-emerge in the future (monitoring the fence with CCTV…). Discovering these problem cases and employing defensive strategies avoids future pain for yourself and colleagues.\nAs you can imagine, covering all the important cases while keeping the fixture minimal is a compromise. At the outset of the work, it may not be obvious what problematic cases may arise. Packages such as hypothesis allow you to generate awkward cases. Non-utf-8 strings anyone? Hypothesis can generate these test cases for you, along with many more interesting edge-cases - ăѣ𝔠ծềſģȟᎥ𝒋ǩľḿꞑȯ𝘱𝑞𝗋𝘴ȶ𝞄𝜈ψ𝒙𝘆𝚣 (Non-utf8 strings often cause problems for web apps).\nNon-disclosive fixtures are those that do not expose personally identifiable or commercially-sensitive information. If you are working with this sort of data, it is necessary to produce toy test fixtures that mimic the schema of the real data. Names and addresses can be de-identified to random alphanumeric strings. Location data can be adjusted with noise. The use of dummy variables or categories can mitigate the risk of disclosure by differencing.\nBy adequately anonymising data and testing problem cases, the programmer exhibits upholds duties under the General Data Protection Regulation:\n\naccurately store, process, retain and erase personally-identifiable information.\n\nIn cases where the system integrates with data available in the public domain, it is may be permissible to include a small sample of the data as a test fixture. Ensure the license that the data is distributed under is compatible with your code’s license. If the license is compatible, I recommend including a reference to the fixture, its source and license within a LICENSE.note file. This practice is enforced by Comprehensive R Archive Network. You can read more about this in the R Packages documentation."
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html#scoping-fixtures",
    "href": "blogs/11-fiddly-bits-of-pytest.html#scoping-fixtures",
    "title": "Pytest Fixtures in Plain English",
    "section": "Scoping fixtures",
    "text": "Scoping fixtures\npytest fixtures have different scopes, meaning that they will be prepared differently dependent on the scope you specify. The available scopes are as follows:\n\n\n\nScope Name\nTeardown after each\n\n\n\n\nfunction\ntest function\n\n\nclass\ntest class\n\n\nmodule\ntest module\n\n\npackage\npackage under test\n\n\nsession\npytest session\n\n\n\nNote that the default scope for any fixtures that you define will be ‘function’. A function-scoped fixture will be set up for every test function that requires it. Once the function has executed, the fixture will then be torn down and all changes to this fixture will be lost. This default behaviour encourages isolation in your test suite. Meaning that the tests have no dependencies upon each other. The test functions could be run in any order without affecting the results of the test. Function-scoped fixtures are the shortest-lived fixtures. Moving down the table above, the persistence of the fixtures increases. Changes to a session-scoped fixture persist for the entire test execution duration, only being torn down once pytest has executed all tests.\n\nScoping for performance\n\n\nperformance vs isolation\n\nBy definition, a unit test is completely isolated, meaning that it will have no dependencies other than the code it needs to test. However, there may be a few cases where this would be less desirable. Slow test suites may introduce excessive friction to the software development process. Persistent fixtures can be used to improve the performance of a test suite.\nFor example, here we define some expensive class:\n\n\nexpensive.py\n\n\"\"\"A module containing an expensive class definition.\"\"\"\nimport time\nfrom typing import Union\n\n\nclass ExpensiveDoodah:\n    \"\"\"A toy class that represents some costly operation.\n\n    This class will sleep for the specified number of seconds on instantiation.\n\n    Parameters\n    ----------\n    sleep_time : Union[int, float]\n        Number of seconds to wait on init.\n\n    \"\"\"\n    def __init__(self, sleep_time: Union[int, float] = 2):\n        print(f\"Sleeping for {sleep_time} seconds\")\n        time.sleep(sleep_time)\n        return None\n\nThis class will be used to demonstrate the effect of scoping with some costly operation. This example could represent reading in a bulky xlsx file or querying a large database.\nTo serve ExpensiveDoodah to our tests, I will define a function-scoped fixture. To do this, we use a pytest fixture decorator to return the class instance with a specified sleep time of 2 seconds.\n\n\ntest_expensive.py\n\nimport pytest\n\nfrom example_pkg.expensive import ExpensiveDoodah\n\n\n@pytest.fixture(scope=\"function\")\ndef module_doodah():\n    \"\"\"Function-scoped ExpensiveDoodah.\"\"\"\n    return ExpensiveDoodah(2)\n\nNow to test ExpensiveDoodah we extend our test module to include a test class with 3 separate test functions. The assertions will all be the same for this simple example - that ExpensiveDoodah executes without raising any error conditions. Notice we must pass the name of the fixture in each test function’s signature.\n\n\ntest_expensive.py\n\n\"\"\"Tests for expensive.py using function-scoped fixture.\"\"\"\nfrom contextlib import nullcontext as does_not_raise\nimport pytest\n\nfrom example_pkg.expensive import ExpensiveDoodah\n\n\n@pytest.fixture(scope=\"function\")\ndef doodah_fixture():\n    \"\"\"Function-scoped ExpensiveDoodah.\"\"\"\n    return ExpensiveDoodah(2)\n\n\nclass TestA:\n    \"\"\"A test class.\"\"\"\n\n    def test_1(self, doodah_fixture):\n        \"\"\"Test 1.\"\"\"\n        with does_not_raise():\n            doodah_fixture\n\n    def test_2(self, doodah_fixture):\n        \"\"\"Test 2.\"\"\"\n        with does_not_raise():\n            doodah_fixture\n\n    def test_3(self, doodah_fixture):\n        \"\"\"Test 3.\"\"\"\n        with does_not_raise():\n            doodah_fixture\n\nThe result of running this test module can be seen below:\ncollected 3 items\n\n./tests/test_expensive_function_scoped.py ...    [100%]\n\n============================ 3 passed in 6.04s ================================\n\nNotice that the test module took just over 6 seconds to execute because the function-scoped fixture was set up once for each test function.\nIf instead we had defined doodah_fixture with a different scope, it would reduce the time for the test suite to complete by approximately two thirds. This is the sort of benefit that can be gained from considerate use of pytest fixtures.\n\n\ntest_expensive.py\n\n@pytest.fixture(scope=\"module\")\ndef doodah_fixture():\n    \"\"\"Module-scoped ExpensiveDoodah.\"\"\"\n    return ExpensiveDoodah(2)\n\ncollected 3 items\n\n./tests/test_expensive_function_scoped.py ...    [100%]\n\n============================ 3 passed in 2.02s ================================\n\nThe scoping feature of pytest fixtures can be used to optimise a test-suite and avoid lengthy delays while waiting for your test suites to execute. However, any changes to the fixture contents will persist until the fixture is next torn down. Keeping track of the states of differently-scoped fixtures in a complex test suite can be tricky and reduces segmentation overall. Bear this in mind when considering which scope best suits your needs.\n\n\nScope persistence\n\n\nfunction &lt; class &lt; module &lt; package &lt; session\n\nUsing scopes other than ‘function’ can be useful for end-to-end testing. Perhaps you have a complex analytical pipeline and need to check that the various components work well together, rather than in isolation as with a unit test. This sort of test can be extremely useful for developers in a rush. You can test that the so called ‘promise’ of the codebase is as expected, even though the implementation may change.\nThe analogy here would be that the success criteria of a SatNav is that it gets you to your desired destination whatever the suggested route you selected. Checking that you used the fastest or most fuel efficient option is probably a good idea. But if you don’t have time, you’ll just have to take the hit if you encounter a toll road. Though it’s still worth checking that the postcode you hastily input to the satnav is the correct one.\n\n\n Perhaps your success criteria is that you need to write a DataFrame to file. A great end-to-end test would check that the DataFrame produced has the expected number of rows, or even has rows! Of course it’s also a good idea to check the DataFrame conforms to the expected table schema, too: number of columns, names of columns, order and data types. This sort of check is often overlooked in favour of pressing on with development. If you’ve ever encountered a situation where you’ve updated a codebase and later realised you now have empty tables (I certainly have), this sort of test would be really handy, immediately alerting you to this fact and helping you efficiently locate the source of the bug.\n\nDefine Data\nIn this part, I will explore the scoping of fixtures with DataFrames. Again, I’ll use a toy example to demonstrate scope behaviour. Being a child of the ’90s (mostly), I’ll use a scenario from my childhood. Scooby Doo is still a thing, right?\nEnter: The Mystery Machine\n \nThe scenario: The passengers of the Mystery Machine van all have the munchies. They stop at a ‘drive thru’ to get some takeaway. We have a table with a record for each character. We have columns with data about the characters’ names, their favourite food, whether they have ‘the munchies’, and the contents of their stomach.\n\nimport pandas as pd\nmystery_machine = pd.DataFrame(\n        {\n            \"name\": [\"Daphne\", \"Fred\", \"Scooby Doo\", \"Shaggy\", \"Velma\"],\n            \"fave_food\": [\n                \"carrots\",\n                \"beans\",\n                \"scooby snacks\",\n                \"burgers\",\n                \"hot dogs\",\n            ],\n            \"has_munchies\": [True] * 5, # everyone's hungry\n            \"stomach_contents\": [\"empty\"] * 5, # all have empty stomachs\n        }\n    )\nmystery_machine\n\n\n\n\n\n\n\n\n\nname\nfave_food\nhas_munchies\nstomach_contents\n\n\n\n\n0\nDaphne\ncarrots\nTrue\nempty\n\n\n1\nFred\nbeans\nTrue\nempty\n\n\n2\nScooby Doo\nscooby snacks\nTrue\nempty\n\n\n3\nShaggy\nburgers\nTrue\nempty\n\n\n4\nVelma\nhot dogs\nTrue\nempty\n\n\n\n\n\n\n\n\nTo use this simple DataFrame as a fixture, I could go ahead and define it with @pytest.fixture() directly within a test file. But if I would like to share it across several test modules (as implemented later), then there are 2 options:\n\nWrite the DataFrame to disk as csv (or whatever format you prefer) and save in a ./tests/data/ folder. At the start of your test modules you can read the data from disk and use it for testing. In this approach you’ll likely define the data as a test fixture in each of the test modules that need to work with it.\nDefine the fixtures within a special python file called conftest.py, which must be located at the root of your project. This file is used to configure your tests. pytest will look in this file for any required fixture definitions when executing your test suite. If it finds a fixture with the same name as that required by a test, the fixture code may be run.\n\n\n\n\n\n\n\nCaution\n\n\n\nWait! Did you just say ‘may be run’?\n\n\nDepending on the scope of your fixture, pytest may not need to execute the code for each test. For example, let’s say we’re working with a session-scoped fixture. This type of fixture will persist for the duration of the entire test suite execution. Imagine test number 1 and 10 both require this test fixture. The fixture definition only gets executed the first time a test requires it. This test fixture will be set up as test 1 executes and will persist until tear down occurs at the end of the pytest session. Test 10 will therefore use the same instance of this fixture as test 1 used, meaning any changes to the fixture may be carried forward.\n\n\nDefine fixtures\nFor our example, we will create a conftest.py file and define some fixtures with differing scopes.\n\n\nconftest.py\n\n\"\"\"Demonstrate scoping fixtures.\"\"\"\nimport pandas as pd\nimport pytest\n\n\n@pytest.fixture(scope=\"session\")\ndef _mystery_machine():\n    \"\"\"Session-scoped fixture returning pandas DataFrame.\"\"\"\n    return pd.DataFrame(\n        {\n            \"name\": [\"Daphne\", \"Fred\", \"Scooby Doo\", \"Shaggy\", \"Velma\"],\n            \"fave_food\": [\n                \"carrots\",\n                \"beans\",\n                \"scooby snacks\",\n                \"burgers\",\n                \"hot dogs\",\n            ],\n            \"has_munchies\": [True] * 5,\n            \"stomach_contents\": [\"empty\"] * 5,\n        }\n    )\n\n\n@pytest.fixture(scope=\"session\")\ndef _mm_session_scoped(_mystery_machine):\n    \"\"\"Session-scoped fixture returning the _mystery_machine DataFrame.\"\"\"\n    return _mystery_machine.copy(deep=True)\n\n\n@pytest.fixture(scope=\"module\")\ndef _mm_module_scoped(_mystery_machine):\n    \"\"\"Module-scoped _mystery_machine DataFrame.\"\"\"\n    return _mystery_machine.copy(deep=True)\n\n\n@pytest.fixture(scope=\"class\")\ndef _mm_class_scoped(_mystery_machine):\n    \"\"\"Class-scoped _mystery_machine DataFrame.\"\"\"\n    return _mystery_machine.copy(deep=True)\n\n\n@pytest.fixture(scope=\"function\")\ndef _mm_function_scoped(_mystery_machine):\n    \"\"\"Function-scoped _mystery_machine DataFrame.\"\"\"\n    return _mystery_machine.copy(deep=True)\n\nFixtures can reference each other, if they’re scoped correctly. More on this in the next section. This is useful for my toy example as I intend the source functions to update the DataFrames directly, if I wasn’t careful about deep copying the fixtures, my functions would update the original _mystery_machine fixture’s table. Those changes would then be subsequently passed to the other fixtures, meaning I couldn’t clearly demonstrate how the different scopes persist.\n\n\nDefine the source functions\nNow, let’s create a function that will feed characters their favourite food if they have the munchies.\n\n\nfeed_characters.py\n\n\"\"\"Helping learners understand how to work with pytest fixtures.\"\"\"\nimport pandas as pd\n\n\ndef serve_food(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Serve characters their desired food.\n\n    Iterates over a df, feeding characters if they have 'the munchies' with\n    their fave_food. If the character is not Scooby Doo or Shaggy, then update\n    their has_munchies status to False. The input df is modified inplace.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        A DataFrame with the following columns: \"name\": str, \"fave_food\": str,\n        \"has_munchies\": bool, \"stomach_contents\": str.\n\n    Returns\n    -------\n    pd.DataFrame\n        Updated DataFrame with new statuses for stomach_contents and\n        has_munchies.\n\n    \"\"\"\n    for ind, row in df.iterrows():\n        if row[\"has_munchies\"]:\n            # if character is hungry then feed them\n            food = row[\"fave_food\"]\n            character = row[\"name\"]\n            print(f\"Feeding {food} to {character}.\")\n            df.loc[ind, [\"stomach_contents\"]] = food\n            if character not in [\"Scooby Doo\", \"Shaggy\"]:\n                # Scooby & Shaggy are always hungry\n                df.loc[ind, \"has_munchies\"] = False\n        else:\n            # if not hungry then do not adjust\n            pass\n    return df\n\nNote that it is commonplace to copy a pandas DataFrame so that any operations carried out by the function are confined to the function’s scope. To demonstrate changes to the fixtures I will instead choose to edit the DataFrame inplace.\n\n\nFixtures Within a Single Test Module\nNow to write some tests. To use the fixtures we defined earlier, we simply declare that a test function requires the fixture. pytest will notice this dependency on collection, check the fixture scope and execute the fixture code if appropriate. The following test test_scopes_before_action checks that the mystery_machine fixtures all have the expected has_munchies column values at the outset of the test module, i.e. everybody is hungry before our source function takes some action. This type of test doesn’t check behaviour of any source code and therefore would be unnecessary for quality assurance purposes. But I include it here to demonstrate the simple use of fixtures and prove to the reader the state of the DataFrame fixtures prior to any source code intervention.\n\n\n\n\n\n\nTesting pandas DataFrames\n\n\n\n\n\nYou may notice that the assert statements in the tests below requires pulling column values out and casting to lists. The pandas package has its own testing module that is super useful for testing all aspects of DataFrames. Check out the pandas testing documentation for more on how to write robust tests for pandas DataFrames and Series.\n\n\n\n\n\ntest_feed_characters.py\n\n\"\"\"Testing pandas operations with test fixtures.\"\"\"\nfrom example_pkg.feed_characters import serve_food\n\n\ndef test_scopes_before_action(\n    _mm_session_scoped,\n    _mm_module_scoped,\n    _mm_class_scoped,\n    _mm_function_scoped,\n):\n    \"\"\"Assert that all characters have the munchies at the outset.\"\"\"\n    assert list(_mm_session_scoped[\"has_munchies\"].values) == [True] * 5, (\n        \"The session-scoped DataFrame 'has_munchies' column was not as \",\n        \"expected before any action was taken.\",\n    )\n    assert list(_mm_module_scoped[\"has_munchies\"].values) == [True] * 5, (\n        \"The module-scoped DataFrame 'has_munchies' column was not as \",\n        \"expected before any action was taken.\",\n    )\n    assert list(_mm_class_scoped[\"has_munchies\"].values) == [True] * 5, (\n        \"The class-scoped DataFrame 'has_munchies' column was not as \",\n        \"expected before any action was taken.\",\n    )\n    assert list(_mm_function_scoped[\"has_munchies\"].values) == [True] * 5, (\n        \"The function-scoped DataFrame 'has_munchies' column was not as \",\n        \"expected before any action was taken.\",\n    )\n\nNow to test the serve_food() function operates as expected. We can define a test class that will house all tests for serve_food(). Within that class let’s define our first test that simply checks that the value of the has_munchies column has been updated as we would expect after using the serve_food() function.\n\n\ntest_feed_characters.py\n\nclass TestServeFood:\n    \"\"\"Tests that serve_food() updates the 'has_munchies' column.\"\"\"\n\n    def test_serve_food_updates_df(\n        self,\n        _mm_session_scoped,\n        _mm_module_scoped,\n        _mm_class_scoped,\n        _mm_function_scoped,\n    ):\n        \"\"\"Test serve_food updates the has_munchies columns as expected.\n\n        This function will update each fixture in the same way, providing each\n        character with their favourite_food and updating the contents of their\n        stomach. The column we will assert against will be has_munchies, which\n        should be updated to False after feeding in all cases except for Scooby\n        Doo and Shaggy, who always have the munchies.\n        \"\"\"\n        # first lets check that the session-scoped dataframe gets updates\n        assert list(serve_food(_mm_session_scoped)[\"has_munchies\"].values) == [\n            False,\n            False,\n            True,\n            True,\n            False,\n        ], (\n            \"The `serve_food()` has not updated the session-scoped df\",\n            \" 'has_munchies' column as expected.\",\n        )\n        # next check the same for the module-scoped fixture\n        assert list(serve_food(_mm_module_scoped)[\"has_munchies\"].values) == [\n            False,\n            False,\n            True,\n            True,\n            False,\n        ], (\n            \"The `serve_food()` has not updated the module-scoped df\",\n            \" 'has_munchies' column as expected.\",\n        )\n        # Next check class-scoped fixture updates\n        assert list(serve_food(_mm_class_scoped)[\"has_munchies\"].values) == [\n            False,\n            False,\n            True,\n            True,\n            False,\n        ], (\n            \"The `serve_food()` has not updated the class-scoped df\",\n            \" 'has_munchies' column as expected.\",\n        )\n        # Finally check the function-scoped df does the same...\n        assert list(\n            serve_food(_mm_function_scoped)[\"has_munchies\"].values\n        ) == [\n            False,\n            False,\n            True,\n            True,\n            False,\n        ], (\n            \"The `serve_food()` has not updated the function-scoped df\",\n            \" 'has_munchies' column as expected.\",\n        )\n\nNotice that the test makes exactly the same assertion for every differently scoped fixture? In every instance, we have fed the characters in the mystery machine DataFrame and therefore everyone’s has_munchies status (apart from Scooby Doo and Shaggy’s) gets updated to False.\n\n\n\n\n\n\nParametrized Tests\n\n\n\n\n\nWriting the test out this way makes things explicit and easy to follow. However, you could make this test smaller by using a neat feature of the pytest package called parametrized tests. This is basically like applying conditions to your tests in a for loop. Perhaps you have a bunch of conditions to check, multiple DataFrames or whatever. These can be programmatically served with parametrized tests. While outside of the scope of this article, I intend to write a blog on this in the future.\n\n\n\nNext, we can add to the test class, including a new test that checks the state of the fixtures. At this point, we will start to see some differences due to scoping. The new test_expected_states_within_same_class() will assert that the changes to the fixtures brought about in the previous test test_serve_food_updates_df() will persist, except for the the case of _mm_function_scoped which will go through teardown at the end of every test function.\n\n\ntest_feed_characters.py\n\nclass TestServeFood:\n    \"\"\"Tests that serve_food() updates the 'has_munchies' column.\"\"\"\n    # ... (test_serve_food_updates_df)\n\n    def test_expected_states_within_same_class(\n        self,\n        _mm_session_scoped,\n        _mm_module_scoped,\n        _mm_class_scoped,\n        _mm_function_scoped,\n    ):\n        \"\"\"Test to ensure fixture states are as expected.\"\"\"\n        # Firstly, session-scoped changes should persist, only Scooby Doo &\n        # Shaggy should still have the munchies...\n        assert list(_mm_session_scoped[\"has_munchies\"].values) == [\n            False,\n            False,\n            True,\n            True,\n            False,\n        ], (\n            \"The changes to the session-scoped df 'has_munchies' column have\",\n            \" not persisted as expected.\",\n        )\n        # Secondly, module-scoped changes should persist, as was the case for\n        # the session-scope test above\n        assert list(_mm_module_scoped[\"has_munchies\"].values) == [\n            False,\n            False,\n            True,\n            True,\n            False,\n        ], (\n            \"The changes to the module-scoped df 'has_munchies' column have\",\n            \" not persisted as expected.\",\n        )\n        # Next, class-scoped changes should persist just the same\n        assert list(_mm_class_scoped[\"has_munchies\"].values) == [\n            False,\n            False,\n            True,\n            True,\n            False,\n        ], (\n            \"The changes to the class-scoped df 'has_munchies' column have\",\n            \" not persisted as expected.\",\n        )\n        # Finally, demonstrate that function-scoped fixture starts from scratch\n        # Therefore all characters should have the munchies all over again.\n        assert (\n            list(_mm_function_scoped[\"has_munchies\"].values) == [True] * 5\n        ), (\n            \"The function_scoped df 'has_munchies' column is not as expected.\",\n        )\n\nIn the above test, we assert that the function-scoped fixture values have the original fixture’s values. The function-scoped fixture goes through set-up again as test_expected_states_within_same_class is executed, ensuring a ‘fresh’, unchanged version of the fixture DataFrame is provided.\nWithin the same test module, we can add some other test class and make assertions about the fixtures. This new test will check whether the stomach_contents column of the module and class-scoped fixtures have been updated. Recall that the characters start out with \"empty\" stomach contents.\n\n\ntest_feed_characters.py\n\n\n# ... (TestServeFood)\n    # (test_serve_food_updates_df)\n    # (test_expected_states_within_same_class) ...\n\nclass TestSomeOtherTestClass:\n    \"\"\"Demonstrate persistence of changes to class-scoped fixture.\"\"\"\n\n    def test_whether_changes_to_stomach_contents_persist(\n        self, _mm_class_scoped, _mm_module_scoped\n    ):\n        \"\"\"Check the stomach_contents column.\"\"\"\n        assert list(_mm_module_scoped[\"stomach_contents\"].values) == [\n            \"carrots\",\n            \"beans\",\n            \"scooby snacks\",\n            \"burgers\",\n            \"hot dogs\",\n        ], \"Changes to module-scoped fixture have not propagated as expected.\"\n        assert (\n            list(_mm_class_scoped[\"stomach_contents\"].values) == [\"empty\"] * 5\n        ), \"Values in class-scoped fixture are not as expected\"\n\nIn this example, it is demonstrated that changes to the class-scoped fixture have been discarded. As test_whether_changes_to_stomach_contents_persist() exists within a new class called TestSomeOtherTestClass, the code for _mm_class_scoped has been executed again, providing the original DataFrame values.\n\nBalancing Isolation & Persistence\n\nWhile the persistence of fixtures may be useful for end to end tests, this approach reduces isolation in the test suite. Be aware that this may introduce a bit of friction to your pytest development process. For example, it can be commonplace to develop a new test and to check that it passes by invoking pytest with the keyword -k flag to run that single test (or subset of tests) only. This approach is useful if you have a costly test suite and you just want to examine changes in a single unit.\nAt the current state of the test module, executing the entire test module by running pytest ./tests/test_feed_characters.py will pass. However, running pytest -k \"TestSomeOtherTestClass\" will fail. This is because the assertions in TestSomeOtherTestClass rely on code being executed within the preceding test class. Tests in TestSomeOtherTestClass rely on changes elsewhere in your test suite and by definition are no longer unit tests. For those developers who work with pytest-randomly to help sniff out poorly-isolated tests, this approach could cause a bit of a headache.\nA good compromise would be to ensure that the use of fixture scopes other than function are isolated and clearly documented within a test suite. Thoughtful grouping of integration tests within test modules or classes can limit grief for collaborating developers. Even better would be to mark tests according to their scoped dependencies. This approach allows tests to be grouped and executed separately, though the implementation of this is beyond the scope of this article.\n\n\n\nFixtures Across Multiple Test Modules\nFinally in this section, we will explore fixture behaviour across more than one test module. Below I define a new source module with a function used to update the mystery_machine DataFrame. This function will update the fave_food column for a character if it has already eaten. This is meant to represent a character’s preference for a dessert following a main course. Once more, this function will not deep copy the input DataFrame but will allow inplace adjustment.\n\n\n\nupdate_food.py\n\n\"\"\"Helping learners understand how to work with pytest fixtures.\"\"\"\nimport pandas as pd\n\n\ndef fancy_dessert(\n    df: pd.DataFrame,\n    fave_desserts: dict = {\n        \"Daphne\": \"brownie\",\n        \"Fred\": \"ice cream\",\n        \"Scooby Doo\": \"apple crumble\",\n        \"Shaggy\": \"pudding\",\n        \"Velma\": \"banana bread\",\n    },\n) -&gt; pd.DataFrame:\n    \"\"\"Update a characters favourite_food to a dessert if they have eaten.\n\n    Iterates over a df, updating the fave_food value for a character if the\n    stomach_contents are not 'empty'.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        A dataframe with the following columns: \"name\": str, \"fave_food\": str,\n        \"has_munchies\": bool, \"stomach_contents\": str.\n    fave_desserts : dict, optional\n        A mapping of \"name\" to a replacement favourite_food, by default\n        { \"Daphne\": \"brownie\", \"Fred\": \"ice cream\",\n        \"Scooby Doo\": \"apple crumble\", \"Shaggy\": \"pudding\",\n        \"Velma\": \"banana bread\", }\n\n    Returns\n    -------\n    pd.DataFrame\n        Dataframe with updated fave_food values.\n\n    \"\"\"\n    for ind, row in df.iterrows():\n        if row[\"stomach_contents\"] != \"empty\":\n            # character has eaten, now they should prefer a dessert\n            character = row[\"name\"]\n            dessert = fave_desserts[character]\n            print(f\"{character} now wants {dessert}.\")\n            df.loc[ind, \"fave_food\"] = dessert\n        else:\n            # if not eaten, do not adjust\n            pass\n    return df\n\nNote that the condition required for fancy_dessert() to take action is that the contents of the character’s stomach_contents should be not equal to “empty”. Now to test this new src module, we create a new test module. We will run assertions of the fave_food columns against the differently-scoped fixtures.\n\n\ntest_update_food.py\n\n\"\"\"Testing pandas operations with test fixtures.\"\"\"\nfrom example_pkg.update_food import fancy_dessert\n\n\nclass TestFancyDessert:\n    \"\"\"Tests for fancy_dessert().\"\"\"\n\n    def test_fancy_dessert_updates_fixtures_as_expected(\n        self,\n        _mm_session_scoped,\n        _mm_module_scoped,\n        _mm_class_scoped,\n        _mm_function_scoped,\n    ):\n        \"\"\"Test fancy_dessert() changes favourite_food values to dessert.\n\n        These assertions depend on the current state of the scoped fixtures. If\n        changes performed in\n        test_feed_characters::TestServeFood::test_serve_food_updates_df()\n        persist, then characters will not have empty stomach_contents,\n        resulting in a switch of their favourite_food to dessert.\n        \"\"\"\n        # first, check update_food() with the session-scoped fixture.\n        assert list(fancy_dessert(_mm_session_scoped)[\"fave_food\"].values) == [\n            \"brownie\",\n            \"ice cream\",\n            \"apple crumble\",\n            \"pudding\",\n            \"banana bread\",\n        ], (\n            \"The changes to the session-scoped df 'stomach_contents' column\",\n            \" have not persisted as expected.\",\n        )\n        # next, check update_food() with the module-scoped fixture.\n        assert list(fancy_dessert(_mm_module_scoped)[\"fave_food\"].values) == [\n            \"carrots\",\n            \"beans\",\n            \"scooby snacks\",\n            \"burgers\",\n            \"hot dogs\",\n        ], (\n            \"The module-scoped df 'stomach_contents' column was not as\",\n            \" expected\",\n        )\n        # now, check update_food() with the class-scoped fixture. Note that we\n        # are now making assertions about changes from a different class.\n        assert list(fancy_dessert(_mm_class_scoped)[\"fave_food\"].values) == [\n            \"carrots\",\n            \"beans\",\n            \"scooby snacks\",\n            \"burgers\",\n            \"hot dogs\",\n        ], (\n            \"The class-scoped df 'stomach_contents' column was not as\",\n            \" expected\",\n        )\n        # Finally, check update_food() with the function-scoped fixture. As\n        # in TestServeFood::test_expected_states_within_same_class(), the\n        # function-scoped fixture starts from scratch.\n        assert list(\n            fancy_dessert(_mm_function_scoped)[\"fave_food\"].values\n        ) == [\"carrots\", \"beans\", \"scooby snacks\", \"burgers\", \"hot dogs\"], (\n            \"The function-scoped df 'stomach_contents' column was not as\",\n            \" expected\",\n        )\n\nNote that the only fixture expected to have been adjusted by update_food() is _mm_session_scoped. When running the pytest command, changes from executing the first test module test_feed_characters.py propagate for this fixture only. All other fixture scopes used will go through teardown and then setup once more on execution of the second test module.\nThis arrangement is highly dependent on the order of which the test modules are collected. pytest collects tests in alphabetical ordering by default, and as such test_update_food.py can be expected to be executed after test_feed_characters.py. This test module is highly dependent upon the order of the pytest execution. This makes the tests less portable and means that running the test module with pytest tests/test_update_food.py in isolation would fail. I would once more suggest using pytest marks to group these types of tests and execute them separately to the rest of the test suite."
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html#scopemismatch-error",
    "href": "blogs/11-fiddly-bits-of-pytest.html#scopemismatch-error",
    "title": "Pytest Fixtures in Plain English",
    "section": "ScopeMismatch Error",
    "text": "ScopeMismatch Error\nWhen working with pytest fixtures, occasionally you will encounter a ScopeMismatch exception. This may happen when attempting to use certain pytest plug-ins or perhaps if trying to use temporary directory fixtures like tmp_path with fixtures that are scoped differently to function-scope. Occasionally, you may encounter this exception when attempting to reference your own fixture in other fixtures, as was done with the mystery_machine fixture above.\nThe reason for ScopeMismatch is straightforward. Fixture scopes have a hierarchy, based on their persistence:\n\nfunction &lt; class &lt; module &lt; package &lt; session\n\nFixtures with a greater scope in the hierarchy are not permitted to reference those lower in the hierarchy. The way I remember this rule is that:\n\nFixtures must only reference equal or greater scopes.\n\nIt is unclear why this rule has been implemented other than to reduce complexity (which is reason enough in my book). There was talk about implementing scope=\"any\" some time ago, but it looks like this idea was abandoned. To reproduce the error:\n\n\ntest_bad_scoping.py\n\n\"\"\"Demomstrate ScopeMismatch error.\"\"\"\n\nimport pytest\n\n@pytest.fixture(scope=\"function\")\ndef _fix_a():\n    return 1\n\n@pytest.fixture(scope=\"class\")\ndef _fix_b(_fix_a):\n    return _fix_a + _fix_a\n\n\ndef test__fix_b_return_val(_fix_b):\n    assert _fix_b == 2\n\nExecuting this test module results in:\n================================= ERRORS ======================================\n________________ ERROR at setup of test__fix_b_return_val _____________________\nScopeMismatch: You tried to access the function scoped fixture _fix_a with a\nclass scoped request object, involved factories:\ntests/test_bad_scoping.py:9:  def _fix_b(_fix_a)\ntests/test_bad_scoping.py:5:  def _fix_a()\n========================== short test summary info ============================\nERROR tests/test_bad_scoping.py::test__fix_b_return_val - Failed:\nScopeMismatch: You tried to access the function scoped fixture _fix_a with a\nclass scoped request object, involved factories:\n=========================== 1 error in 0.01s ==================================\nThis error can be avoided by adjusting the fixture scopes to adhere to the hierarchy rule, so updating _fix_a to use a class scope or greater would result in a passing test."
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html#summary",
    "href": "blogs/11-fiddly-bits-of-pytest.html#summary",
    "title": "Pytest Fixtures in Plain English",
    "section": "Summary",
    "text": "Summary\nHopefully by now you feel comfortable in when and how to use fixtures for pytest. We’ve covered quite a bit, including:\n\nWhat fixtures are\nUse-cases\nWhere to store them\nHow to reference them\nHow to scope them\nHow changes to fixtures persist or not\nHandling scope errors\n\nIf you spot an error with this article, or have suggested improvement then feel free to raise an issue on GitHub.\nHappy testing!"
  },
  {
    "objectID": "blogs/11-fiddly-bits-of-pytest.html#acknowledgements",
    "href": "blogs/11-fiddly-bits-of-pytest.html#acknowledgements",
    "title": "Pytest Fixtures in Plain English",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nTo past and present colleagues who have helped to discuss pros and cons, establishing practice and firming-up some opinions. Particularly:\n\nClara\nDan C\nDan S\nEdward\nEthan\nHenry\nIan\nIva\nJay\nMark\nMartin R\nMartin W\nMat\nSergio\n\n\nfin!"
  },
  {
    "objectID": "blogs/13-pytest-parametrize.html",
    "href": "blogs/13-pytest-parametrize.html",
    "title": "Parametrized Tests With Pytest in Plain English",
    "section": "",
    "text": "Complex sushi conveyor belt with a futuristic theme in a pastel palette."
  },
  {
    "objectID": "blogs/13-pytest-parametrize.html#introduction",
    "href": "blogs/13-pytest-parametrize.html#introduction",
    "title": "Parametrized Tests With Pytest in Plain English",
    "section": "Introduction",
    "text": "Introduction\npytest is a testing package for the python framework. It is broadly used to quality assure code logic. This article discusses what parametrized tests mean and how to implement them with pytest. This blog is the third in a series of blogs called pytest in plain English, favouring accessible language and simple examples to explain the more intricate features of the pytest package.\nFor a wealth of documentation, guides and how-tos, please consult the pytest documentation.\n\n\n\n\n\n\nA Note on the Purpose (Click to expand)\n\n\n\n\n\nThis article intends to discuss clearly. It doesn’t aim to be clever or impressive. Its aim is to extend understanding without overwhelming the reader.\n\n\n\n\nIntended Audience\nProgrammers with a working knowledge of python and some familiarity with pytest and packaging. The type of programmer who has wondered about how to follow best practice in testing python code.\n\n\nWhat You’ll Need:\n\nPreferred python environment manager (eg conda)\npip install pytest==8.1.1\nGit\nGitHub account\nCommand line access\n\n\n\nPreparation\nThis blog is accompanied by code in this repository. The main branch provides a template with the minimum structure and requirements expected to run a pytest suite. The repo branches contain the code used in the examples of the following sections.\nFeel free to fork or clone the repo and checkout to the example branches as needed.\nThe example code that accompanies this article is available in the parametrize branch of the repo."
  },
  {
    "objectID": "blogs/13-pytest-parametrize.html#overview",
    "href": "blogs/13-pytest-parametrize.html#overview",
    "title": "Parametrized Tests With Pytest in Plain English",
    "section": "Overview",
    "text": "Overview\n\nWhat Are Parametrized Tests?\nParametrized tests are simply tests that are applied recursively to multiple input values. For example, rather than testing a function on one input value, a list of different values could be passed as a parametrized fixture.\nA standard approach to testing could look like Figure 1 below, where separate tests are defined for the different values we need to check. This would likely result in a fair amount of repeated boilerplate code.\n\n\n\nFigure 1: Testing multiple values without parametrization\n\n\nInstead, we can reduce the number of tests down to 1 and pass a list of tuples to the test instead. Each tuple should contain a parameter value and the expected result, as illustrated in Figure 2.\n\n\n\nFigure 2: Parametrized testing of multiple values\n\n\nSo let’s imagine we have a simple function called double(), the setup for the parametrized list is illustrated in Figure 3.\n\n\n\nFigure 3: Exemplified paramatrization for test_double()\n\n\n\n\nWhy use Parametrization?\nThis approach allows us to thoroughly check the behaviour of our functions against multiple values, ensuring that edge-cases are safely treated or exceptions are raised as expected.\nIn this way, we serve multiple parameters and expected outcomes to a single test, reducing boilerplate code. Parametrization is not a silver bullet, and we still need to define all of our parameters and results in a parametrized fixture. This approach is not quite as flexible as the property-based testing achievable with a package such as hypothesis. However, the learning curve for hypothesis is a bit greater and may be disproportionate to the job at hand.\nFor the reasons outlined above, there are likely many competent python developers that never use parametrized fixtures. But parametrization does allow us to avoid implementing tests with a for loop or vectorized approaches to the same outcomes. When coupled with programmatic approaches to generating our input parameters, many lines of code can be saved. And things get even more interesting when we pass multiple parametrized fixtures to our tests, which I’ll come to in a bit. For these reasons, I believe that awareness of parametrization should be promoted among python developers as a useful solution in the software development toolkit."
  },
  {
    "objectID": "blogs/13-pytest-parametrize.html#implementing-parametrization",
    "href": "blogs/13-pytest-parametrize.html#implementing-parametrization",
    "title": "Parametrized Tests With Pytest in Plain English",
    "section": "Implementing Parametrization",
    "text": "Implementing Parametrization\nIn this section, we will compare some very simple examples of tests with and without parametrization. Feel free to clone the repository and check out to the example code branch to run the examples.\n\nDefine the Source Code\nHere we define a very basic function that checks whether an integer is prime. If a prime is encountered, then True is returned. If not, then False. The value 1 gets its own treatment (return False). Lastly, we include some basic defensive checks, we return a TypeError if anything other than integer is passed to the function and a ValueError if the integer is less than or equal to 0.\n\ndef is_num_prime(pos_int: int) -&gt; bool:\n    \"\"\"Check if a positive integer is a prime number.\n\n    Parameters\n    ----------\n    pos_int : int\n        A positive integer.\n\n    Returns\n    -------\n    bool\n        True if the number is a prime number.\n\n    Raises\n    ------\n    TypeError\n        Value passed to `pos_int` is not an integer.\n    ValueError\n        Value passed to `pos_int` is less than or equal to 0.\n    \"\"\"\n    if not isinstance(pos_int, int):\n        raise TypeError(\"`pos_int` must be a positive integer.\")\n    if pos_int &lt;= 0:\n        raise ValueError(\"`pos_int` must be a positive integer.\")\n    elif pos_int == 1:\n        return False\n    else:\n        for i in range(2, (pos_int // 2) + 1):\n            # If divisible by any number 2&lt;&gt;(n/2)+1, it is not prime\n            if (pos_int % i) == 0:\n                return False\n        else:\n            return True\n\nRunning this function with a range of values demonstrates its behaviour.\n\nfor i in range(1, 11):\n  print(f\"{i}: {is_num_prime(i)}\")\n\n1: False\n2: True\n3: True\n4: False\n5: True\n6: False\n7: True\n8: False\n9: False\n10: False\n\n\n\n\nLet’s Get Testing\nLet’s begin with the defensive tests. Let’s say I need to check that the function can be relied upon to raise on a number of conditions. The typical approach may be to test the raise conditions within a dedicated test function.\n\n\"\"\"Tests for primes module.\"\"\"\nimport pytest\n\nfrom example_pkg.primes import is_num_prime\n\n\ndef test_is_num_primes_exceptions_manually():\n    \"\"\"Testing the function's defensive checks.\n\n    Here we have to repeat a fair bit of pytest boilerplate.\n    \"\"\"\n    with pytest.raises(TypeError, match=\"must be a positive integer.\"):\n        is_num_prime(1.0)\n    with pytest.raises(ValueError, match=\"must be a positive integer.\"):\n        is_num_prime(-1)\n\nWithin this function, I can run multiple assertions against several hard-coded inputs. I’m only checking against a couple of values here but production-ready code may test against many more cases. To do that, I’d need to have a lot of repeated pytest.raises statements. Perhaps more importantly, watch what happens when I run the test.\n% pytest -k \"test_is_num_primes_exceptions_manually\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 56 items / 55 deselected / 1 selected                                \n\ntests/test_primes.py .                                                   [100%]\n\n======================= 1 passed, 55 deselected in 0.01s ======================\n\nNotice that both assertions will either pass or fail together as one test. This could potentially make it more challenging to troubleshoot a failing pipeline. It could be better to have separate test functions for each value, but that seems like an awful lot of work…\n\n\n…Enter Parametrize\nNow to start using parametrize, we need to use the @pytest.mark.parametrize decorator, which takes 2 arguments, a string and an iterable.\n\n@pytest.mark.parametrize(\n    \"some_values, exception_types\", [(1.0, TypeError), (-1, ValueError)]\n    )\n\nThe string should contain comma separated values for the names that you would like to refer to when iterating through the iterable. They can be any placeholder you would wish to use in your test. These names will map to the index of elements in the iterable.\nSo when I use the fixture with a test, I will expect to inject the following values:\niteration 1… “some_values” = 1.0, “exception_types” = TypeError\niteration 2… “some_values” = -1, “exception_types” = ValueError\nLet’s go ahead and use this parametrized fixture with a test.\n\n@pytest.mark.parametrize(\n    \"some_values, exception_types\", [(1.0, TypeError), (-1, ValueError)]\n    )\ndef test_is_num_primes_exceptions_parametrized(some_values, exception_types):\n    \"\"\"The same defensive checks but this time with parametrized input.\n\n    Less lines in the test but if we increase the number of cases, we need to\n    add more lines to the parametrized fixture instead.\n    \"\"\"\n    with pytest.raises(exception_types, match=\"must be a positive integer.\"):\n        is_num_prime(some_values)\n\nThe outcome for running this test is shown below.\n% pytest -k \"test_is_num_primes_exceptions_parametrized\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 56 items / 54 deselected / 2 selected                                \n\ntests/test_primes.py ..                                                  [100%]\n\n======================= 2 passed, 54 deselected in 0.01s ======================\n\nIt’s a subtle difference, but notice that we now get 2 passing tests rather than 1? We can make this more explicit by passing the -v flag (for verbose) when we invoke pytest.\n% pytest -k \"test_is_num_primes_exceptions_parametrized\" -v \n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 \ncachedir: .pytest_cache\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 56 items / 54 deselected / 2 selected                                \n\ntest_is_num_primes_exceptions_parametrized[1.0-TypeError] PASSED         [ 50%]\ntest_is_num_primes_exceptions_parametrized[-1-ValueError] PASSED         [100%]\n\n======================= 2 passed, 54 deselected in 0.01s ======================\n\nIn this way, we get a helpful printout of the test and parameter combination being executed. This can be very helpful in identifying problem cases.\n\n\nYet More Cases\nNext up, we may wish to check return values for our function with several more cases. To keep things simple, let’s write a test that checks the return values for a range of numbers between 1 and 5.\n\ndef test_is_num_primes_manually():\n    \"\"\"Test several positive integers return expected boolean.\n\n    This is quite a few lines of code. Note that this runs as a single test.\n    \"\"\"\n    assert is_num_prime(1) == False\n    assert is_num_prime(2) == True\n    assert is_num_prime(3) == True\n    assert is_num_prime(4) == False\n    assert is_num_prime(5) == True\n\nOne way that this can be serialised is by using a list of parameters and expected results.\n\ndef test_is_num_primes_with_list():\n    \"\"\"Test the same values using lists.\n\n    Less lines but is run as a single test.\n    \"\"\"\n    answers = [is_num_prime(i) for i in range(1, 6)]\n    assert answers == [False, True, True, False, True]\n\nThis is certainly neater than the previous example. Although both implementations will evaluate as a single test, so a failing instance will not be explicitly indicated in the pytest report.\n% pytest -k \"test_is_num_primes_with_list\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 56 items / 55 deselected / 1 selected                               \n\ntests/test_primes.py .                                                   [100%]\n\n======================= 1 passed, 55 deselected in 0.01s ======================\nTo parametrize the equivalent test, we can take the below approach.\n\n@pytest.mark.parametrize(\n    \"some_integers, answers\",\n    [(1, False), (2, True), (3, True), (4, False), (5, True)]\n    )\ndef test_is_num_primes_parametrized(some_integers, answers):\n    \"\"\"The same tests but this time with parametrized input.\n\n    Fewer lines and 5 separate tests are run by pytest.\n    \"\"\"\n    assert is_num_prime(some_integers) == answers\n\nThis is slightly more lines than test_is_num_primes_with_list but has the advantage of being run as separate tests:\n% pytest -k \"test_is_num_primes_parametrized\" -v\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0\ncachedir: .pytest_cache\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 56 items / 51 deselected / 5 selected                               \n\ntests/test_primes.py::test_is_num_primes_parametrized[1-False] PASSED    [ 20%]\ntests/test_primes.py::test_is_num_primes_parametrized[2-True] PASSED     [ 40%]\ntests/test_primes.py::test_is_num_primes_parametrized[3-True] PASSED     [ 60%]\ntests/test_primes.py::test_is_num_primes_parametrized[4-False] PASSED    [ 80%]\ntests/test_primes.py::test_is_num_primes_parametrized[5-True] PASSED     [100%]\n\n======================= 5 passed, 51 deselected in 0.01s ======================\n\nWhere this approach really comes into its own is when the number of cases you need to test increases, you can explore ways of generating cases rather than hard-coding the values, as in the previous examples.\nIn the example below, we can use the range() function to generate the integers we need to test, and then zipping these cases to their expected return values.\n\n# if my list of cases is growing, I can employ other tactics...\nin_ = range(1, 21)\nout = [\n    False, True, True, False, True, False, True, False, False, False,\n    True, False, True, False, False, False, True, False, True, False,\n    ]\n\n\n@pytest.mark.parametrize(\"some_integers, some_answers\", zip(in_, out))\ndef test_is_num_primes_with_zipped_lists(some_integers, some_answers):\n    \"\"\"The same tests but this time with zipped inputs.\"\"\"\n    assert is_num_prime(some_integers) == some_answers\n\nRunning this test yields the following result:\n\n% pytest -k \"test_is_num_primes_with_zipped_lists\" -v \n============================= test session starts =============================\nplatform darwin -- Python 3.11.6, pytest-7.4.3, pluggy-1.3.0\ncachedir: .pytest_cache\nconfigfile: pyproject.toml\ntestpaths: ./tests\nplugins: anyio-4.0.0\ncollected 56 items / 36 deselected / 20 selected\n\n/test_primes.py::test_is_num_primes_with_zipped_lists[1-False] PASSED  [  5%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[2-True] PASSED   [ 10%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[3-True] PASSED   [ 15%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[4-False] PASSED  [ 20%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[5-True] PASSED   [ 25%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[6-False] PASSED  [ 30%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[7-True] PASSED   [ 35%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[8-False] PASSED  [ 40%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[9-False] PASSED  [ 45%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[10-False] PASSED [ 50%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[11-True] PASSED  [ 55%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[12-False] PASSED [ 60%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[13-True] PASSED  [ 65%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[14-False] PASSED [ 70%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[15-False] PASSED [ 75%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[16-False] PASSED [ 80%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[17-True] PASSED  [ 85%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[18-False] PASSED [ 90%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[19-True] PASSED  [ 95%]\n/test_primes.py::test_is_num_primes_with_zipped_lists[20-False] PASSED [100%]\n\n====================== 20 passed, 36 deselected in 0.02s ======================"
  },
  {
    "objectID": "blogs/13-pytest-parametrize.html#stacked-parametrization",
    "href": "blogs/13-pytest-parametrize.html#stacked-parametrization",
    "title": "Parametrized Tests With Pytest in Plain English",
    "section": "Stacked Parametrization",
    "text": "Stacked Parametrization\n\nParametrize gets really interesting when you have a situation where you need to test combinations of input parameters against expected outputs. In this scenario, stacked parametrization allows you to set up all combinations with very little fuss.\nFor this section, I will define a new function built on top of our is_num_prime() function. This function will take 2 positive integers and add them together, but only if both of the input integers are prime. Otherwise, we’ll simply return the input numbers. To keep things simple, we’ll always return a tuple in all cases.\n\ndef sum_if_prime(pos_int1: int, pos_int2: int) -&gt; tuple:\n    \"\"\"Sum 2 integers only if they are prime numbers.\n\n    Parameters\n    ----------\n    pos_int1 : int\n        A positive integer.\n    pos_int2 : int\n        A positive integer.\n\n    Returns\n    -------\n    tuple\n        Tuple of one integer if both inputs are prime numbers, else returns a\n        tuple of the inputs.\n    \"\"\"\n    if is_num_prime(pos_int1) and is_num_prime(pos_int2):\n        return (pos_int1 + pos_int2,)\n    else:\n        return (pos_int1, pos_int2)\n\nThen using this function with a range of numbers:\n\nfor i in range(1, 6):\n    print(f\"{i} and {i} result: {sum_if_prime(i, i)}\")\n\n1 and 1 result: (1, 1)\n2 and 2 result: (4,)\n3 and 3 result: (6,)\n4 and 4 result: (4, 4)\n5 and 5 result: (10,)\n\n\nTesting combinations of input parameters for this function will quickly become burdensome:\n\nfrom example_pkg.primes import sum_if_prime\n\n\ndef test_sum_if_prime_with_manual_combinations():\n    \"\"\"Manually check several cases.\"\"\"\n    assert sum_if_prime(1, 1) == (1, 1)\n    assert sum_if_prime(1, 2) == (1, 2)\n    assert sum_if_prime(1, 3) == (1, 3)\n    assert sum_if_prime(1, 4) == (1, 4)\n    assert sum_if_prime(1, 5) == (1, 5)\n    assert sum_if_prime(2, 1) == (2, 1)\n    assert sum_if_prime(2, 2) == (4,) # the first case where both are primes\n    assert sum_if_prime(2, 3) == (5,) \n    assert sum_if_prime(2, 4) == (2, 4)\n    assert sum_if_prime(2, 5) == (7,)\n    # ...\n\n% pytest -k \"test_sum_if_prime_with_manual_combinations\"\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 56 items / 55 deselected / 1 selected\n\ntests/test_primes.py .                                                   [100%]\n\n====================== 1 passed, 55 deselected in 0.01s =======================\n\n\nSingle Assertions\nBecause we take more than one input parameter, we can use stacked parametrization to easily inject all combinations of parameters to a test. Simply put, this means that we pass more than one parametrized fixture to the same test. Behind the scenes, pytest prepares all parameter combinations to inject into our test.\nThis allows us to very easily pass all parameter combinations to a single assertion statement, as in the diagram below.\n\n\n\nStacked parametrization against a single assertion\n\n\nTo use stacked parametrization against our sum_if_prime() function, we can use 2 separate iterables:\n\n@pytest.mark.parametrize(\"first_ints\", range(1,6))\n@pytest.mark.parametrize(\"second_ints\", range(1,6))\ndef test_sum_if_prime_stacked_parametrized_inputs(\n    first_ints, second_ints, expected_answers):\n    \"\"\"Using stacked parameters to set up combinations of all cases.\"\"\"\n    assert isinstance(sum_if_prime(first_ints, second_ints), tuple)\n\n\n% pytest -k \"test_sum_if_prime_stacked_parametrized_inputs\" -v\n============================= test session starts =============================\nplatform darwin -- Python 3.11.6, pytest-7.4.3, pluggy-1.3.0 \ncachedir: .pytest_cache\nconfigfile: pyproject.toml\ntestpaths: ./tests\nplugins: anyio-4.0.0\ncollected 56 items / 31 deselected / 25 selected\n\ntest_sum_if_prime_stacked_parametrized_inputs[1-1] PASSED                [  4%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-2] PASSED                [  8%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-3] PASSED                [ 12%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-4] PASSED                [ 16%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-5] PASSED                [ 20%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-1] PASSED                [ 24%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-2] PASSED                [ 28%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-3] PASSED                [ 32%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-4] PASSED                [ 36%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-5] PASSED                [ 40%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-1] PASSED                [ 44%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-2] PASSED                [ 48%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-3] PASSED                [ 52%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-4] PASSED                [ 56%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-5] PASSED                [ 60%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-1] PASSED                [ 64%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-2] PASSED                [ 68%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-3] PASSED                [ 72%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-4] PASSED                [ 76%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-5] PASSED                [ 80%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-1] PASSED                [ 84%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-2] PASSED                [ 88%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-3] PASSED                [ 92%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-4] PASSED                [ 96%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-5] PASSED                [100%]\n\n====================== 25 passed, 31 deselected in 0.01s ======================\n\n\n The above test; which is 6 lines long; executed 25 tests. This is clearly a very beneficial feature of pytest. However, the eagle-eyed among you may have spotted a problem - this is only going to work if the expected answer is always the same. The test we defined is only checking that a tuple is returned in all cases. How can we ensure that we serve the expected answers to the test too? This is where things get a little fiddly.\n\n\nMultiple Assertions\nTo test our function against combinations of parameters with different expected answers, we must employ a dictionary mapping of the parameter combinations as keys and the expected assertions as values.\n\n\n\nUsing a dictionary to map multiple assertions against stacked parametrized fixtures\n\n\nTo do this, we need to define a new fixture, which will return the required dictionary mapping of parameters to expected values.\n\n# Using stacked parametrization, we can avoid manually typing the cases out,\n# though we do still need to define a dictionary of the expected answers...\n@pytest.fixture\ndef expected_answers() -&gt; dict:\n    \"\"\"A dictionary of expected answers for all combinations of 1 through 5.\n\n    First key corresponds to `pos_int1` and second key is `pos_int2`.\n\n    Returns\n    -------\n    dict\n        Dictionary of cases and their expected tuples.\n    \"\"\"\n    expected= {\n        1: {1: (1,1), 2: (1,2), 3: (1,3), 4: (1,4), 5: (1,5),},\n        2: {1: (2,1), 2: (4,), 3: (5,), 4: (2,4), 5: (7,),},\n        3: {1: (3,1), 2: (5,), 3: (6,), 4: (3,4), 5: (8,),},\n        4: {1: (4,1), 2: (4,2), 3: (4,3), 4: (4,4), 5: (4,5),},\n        5: {1: (5,1), 2: (7,), 3: (8,), 4: (5,4), 5: (10,),},\n    }\n    return expected\n\nPassing our expected_answers fixture to our test will allow us to match all parameter combinations to their expected answer. Let’s update test_sum_if_prime_stacked_parametrized_inputs to use the parameter values to access the expected assertion value from the dictionary.\n\n@pytest.mark.parametrize(\"first_ints\", range(1,6))\n@pytest.mark.parametrize(\"second_ints\", range(1,6))\ndef test_sum_if_prime_stacked_parametrized_inputs(\n    first_ints, second_ints, expected_answers):\n    \"\"\"Using stacked parameters to set up combinations of all cases.\"\"\"\n    assert isinstance(sum_if_prime(first_ints, second_ints), tuple)\n    answer = sum_if_prime(first_ints, second_ints)\n    # using the parametrized values, pull out their keys from the\n    # expected_answers dictionary\n    assert answer == expected_answers[first_ints][second_ints]\n\nFinally, running this test produces the below pytest report.\n\n% pytest -k \"test_sum_if_prime_stacked_parametrized_inputs\" -v\n============================= test session starts =============================\nplatform darwin -- Python 3.12.3, pytest-8.1.1, pluggy-1.5.0 \ncachedir: .pytest_cache\nconfigfile: pyproject.toml\ntestpaths: ./tests\ncollected 56 items / 31 deselected / 25 selected\n\ntest_sum_if_prime_stacked_parametrized_inputs[1-1] PASSED                [  4%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-2] PASSED                [  8%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-3] PASSED                [ 12%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-4] PASSED                [ 16%]\ntest_sum_if_prime_stacked_parametrized_inputs[1-5] PASSED                [ 20%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-1] PASSED                [ 24%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-2] PASSED                [ 28%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-3] PASSED                [ 32%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-4] PASSED                [ 36%]\ntest_sum_if_prime_stacked_parametrized_inputs[2-5] PASSED                [ 40%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-1] PASSED                [ 44%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-2] PASSED                [ 48%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-3] PASSED                [ 52%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-4] PASSED                [ 56%]\ntest_sum_if_prime_stacked_parametrized_inputs[3-5] PASSED                [ 60%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-1] PASSED                [ 64%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-2] PASSED                [ 68%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-3] PASSED                [ 72%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-4] PASSED                [ 76%]\ntest_sum_if_prime_stacked_parametrized_inputs[4-5] PASSED                [ 80%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-1] PASSED                [ 84%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-2] PASSED                [ 88%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-3] PASSED                [ 92%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-4] PASSED                [ 96%]\ntest_sum_if_prime_stacked_parametrized_inputs[5-5] PASSED                [100%]\n\n====================== 25 passed, 31 deselected in 0.01s ======================"
  },
  {
    "objectID": "blogs/13-pytest-parametrize.html#summary",
    "href": "blogs/13-pytest-parametrize.html#summary",
    "title": "Parametrized Tests With Pytest in Plain English",
    "section": "Summary",
    "text": "Summary\nThere you have it - how to use basic and stacked parametrization in your tests. We have:\n\nused parametrize to inject multiple parameter values to a single test.\nused stacked parametrize to test combinations of parameters against a single assertion.\nused a nested dictionary fixture to map stacked parametrize input combinations to different expected assertion values.\n\nIf you spot an error with this article, or have a suggested improvement then feel free to raise an issue on GitHub.\nHappy testing!"
  },
  {
    "objectID": "blogs/13-pytest-parametrize.html#acknowledgements",
    "href": "blogs/13-pytest-parametrize.html#acknowledgements",
    "title": "Parametrized Tests With Pytest in Plain English",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nTo past and present colleagues who have helped to discuss pros and cons, establishing practice and firming-up some opinions. Particularly:\n\nCharlie\nEthan\nHenry\nSergio\n\nThe diagrams used in this article were produced with the excellent Excalidraw, with thanks to Mat for the recommendation.\n\nfin!"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Byte-Wise Musings",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGitHub Actions Security\n\n\n4 min\n\n\n\nHow-to\n\n\nGitHub\n\n\nGitHub Actions\n\n\nCI:CD\n\n\nSecurity\n\n\n\nCan you actually rely on that pre-built Action you rely on?\n\n\n\nRich Leyshon\n\n\nJun 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParametrized Tests With Pytest in Plain English\n\n\n23 min\n\n\n\nExplanation\n\n\npytest\n\n\nUnit tests\n\n\nparametrize\n\n\npytest-in-plain-english\n\n\n\nPlain English Discussion of Pytest Parametrize\n\n\n\nRich Leyshon\n\n\nJun 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytest With tmp_path in Plain English\n\n\n16 min\n\n\n\nExplanation\n\n\npytest\n\n\nUnit tests\n\n\ntmp_path\n\n\ntmp_path_factory\n\n\nfixtures\n\n\npytest-in-plain-english\n\n\n\nPlain English Discussion of Pytest Temporary Fixtures.\n\n\n\nRich Leyshon\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytest Fixtures in Plain English\n\n\n38 min\n\n\n\nExplanation\n\n\npytest\n\n\nUnit tests\n\n\nfixtures\n\n\npytest-in-plain-english\n\n\n\nPlain English Discussion of Pytest Fixtures.\n\n\n\nRich Leyshon\n\n\nApr 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Pydeck to Play Nicely with GeoPandas.\n\n\n5 min\n\n\n\nHow To\n\n\nGeospatial\n\n\npydeck\n\n\ngeopandas\n\n\n\nBuilding Pydeck Maps from GeoPandas GeoDataFrames.\n\n\n\nRich Leyshon\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBicycle Network Modelling with r5py\n\n\n32 min\n\n\n\nTutorial\n\n\nTransport Modelling\n\n\nREST API\n\n\nWeb data\n\n\nGeospatial\n\n\nr5py\n\n\npydeck\n\n\n\nAnalysing service coverage in London’s Santander Bike network.\n\n\n\nRich Leyshon\n\n\nFeb 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConda Environments for Quarto Documents\n\n\n5 min\n\n\n\nHow-to\n\n\nQuarto\n\n\nConda Environments\n\n\nConda\n\n\n\nHow to specify a specific Conda environment when rendering quarto documents.\n\n\n\nRich Leyshon\n\n\nJan 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScheduled Deployment to Shinyapps.io\n\n\n11 min\n\n\n\nHow-to\n\n\nPython Shiny\n\n\nCI:CD\n\n\nGitHub\n\n\n\nHow to ingest data using Python requests & ArcGIS REST API.\n\n\n\nRich Leyshon\n\n\nDec 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Data from ONS Open Geography Portal\n\n\n12 min\n\n\n\nTutorial\n\n\nONS Open Geography Portal\n\n\nREST API\n\n\nWeb data\n\n\n\nIngesting data using Python requests & ArcGIS REST API.\n\n\n\nRich Leyshon\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet Up Signed Commits on GitHub\n\n\n3 min\n\n\n\nHow-to\n\n\nGitHub\n\n\nAuthentication\n\n\nVerification\n\n\n\nA quick guide to setting up commit verification using a GPG key.\n\n\n\nRich Leyshon\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing Loot Gains in Starfield: A Data-Driven Approach\n\n\n5 min\n\n\n\nExplanation\n\n\nVideo games\n\n\nData analysis\n\n\n\nDiscover the hidden riches of Bethesda’s Starfield with this data-driven guide. Learn what to strategically hoard in order to maximize your loot gains.\n\n\n\nRich Leyshon\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Automate Quarto Builds with GitHub Actions\n\n\n4 min\n\n\n\nHow-to\n\n\nCI/CD\n\n\nFront End Dev\n\n\n\nSetting up a website with Quarto? Want to automate the website build and publication with GitHub Actions? Could you use a quick guide? I’ve got your back.\n\n\n\nRich Leyshon\n\n\nSep 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe State of Python Shiny\n\n\n15 min\n\n\n\nExplanation\n\n\nPython Shiny\n\n\n\nAn overview of the progress of Python Shiny, and where it could possibly go.\n\n\n\nRich Leyshon\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Build a Basic Python Shiny App\n\n\n21 min\n\n\n\nTutorial\n\n\nPython Shiny\n\n\n\nAn interactive tutorial allowing newcomers to Shiny to build a basic application.\n\n\n\nRich Leyshon\n\n\nJul 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "book-reviews/02-algorithms-to-live-by.html",
    "href": "book-reviews/02-algorithms-to-live-by.html",
    "title": "Algorithms to Live By",
    "section": "",
    "text": "Image credit: https://www.rawpixel.com/"
  },
  {
    "objectID": "book-reviews/02-algorithms-to-live-by.html#introduction",
    "href": "book-reviews/02-algorithms-to-live-by.html#introduction",
    "title": "Algorithms to Live By",
    "section": "Introduction",
    "text": "Introduction\nAs of today’s date, Algorithms to Live By (AtLB) scores an average of 4.13 / 5.00 across over 2.5k ratings on goodreads.\nAtLB made the MiT Technology Review Best Books of 2016 and Forbes’ Must Read Brain Books of the same year."
  },
  {
    "objectID": "book-reviews/02-algorithms-to-live-by.html#summary",
    "href": "book-reviews/02-algorithms-to-live-by.html#summary",
    "title": "Algorithms to Live By",
    "section": "Summary",
    "text": "Summary\nAtLB is a captivating read for those unfamiliar with the intricacies of computer science and how it manifests in our daily lives. Whether it’s deciding on a new restaurant to try, efficiently finding your next home, or optimizing your sock drawer, this book unveils how the brilliance of computer science has already addressed these everyday dilemmas. The authors guide us through the unexpected implications that computer science offers, presented in a manner that makes it easily accessible to a wider audience under the umbrella of popular science. While they skillfully refrain from downplaying its impact, reading this book will undoubtedly reshape your perception of sorting and problem-solving\n\nOn the Authors\nTom Griffiths is a celebrated Princeton professor in computational cognition and director of the University’s Computational Cognitive Science Laboratory [1]. The recipient of numerous awards and a leader in applied artificial intelligence research, Griffiths has many influential papers across a diverse set of problem spaces: Causal inference, natural language processing and pedagogy.\nBrian Christian has written several award winning works of nonfiction, notably “The Alignment Problem” which examines machine learning ethics, for which he received numerous awards. A visiting scholar at the University of California, Berkeley; Christian’s expertise ranges from programming to poetry, earning his laureate at San Francisco Public Library in 2016."
  },
  {
    "objectID": "book-reviews/02-algorithms-to-live-by.html#an-overview-of-the-book",
    "href": "book-reviews/02-algorithms-to-live-by.html#an-overview-of-the-book",
    "title": "Algorithms to Live By",
    "section": "An Overview of the Book",
    "text": "An Overview of the Book\nEach of the book’s eleven chapters delves into specific problem domains, unraveling the algorithms crafted to tackle them. From seemingly ‘simple’ tasks like optimized sorting of library books to the profound insights into behavioural economics explored in the game theory chapter, the book covers a spectrum of complexity in the problems addressed. The authors skillfully illustrate these concepts, infusing the text with a wealth of relatable examples, making it approachable even for those not well-versed in numerical intricacies.\nIn the sections below, I will explore some of the more engaging aspects of the book’s content.\n\nA Gentle Introduction to Operational Research\nThe authors skillfully address prevalent misconceptions about both computer science and human behavior in a manner that resonates with their audience. They challenge the notion that humans are too fuzzy-minded and fallible to benefit from optimization, emphasizing that many real-world challenges can indeed be optimised, albeit within defined parameters. Although certain problems may prove intractable due to an overwhelming number of potential solutions, the authors adeptly navigate this complexity by outlining how problems can be defined at their outset.\nThe introductory chapter poignantly captures these concepts through a relatable scenario — the search for the perfect New York apartment. By briefly exploring the optimal stopping problem in this context—whether to make an offer for an apartment or risk losing out on potentially the best one on the market, the authors provide readers with a glimpse into the fascinating direction of the book.\n\n\n\n\n\n\nNote\n\n\n\n“…spend 37% of your apartment hunt (eleven days, if you’ve given yourself a month for the search) noncommittally exploring options. Leave the checkbook at home; you’re just calibrating. But after that point, be prepared to immediately commit - deposit and all - to the very first place that beats whatever you’ve already seen.” [2, p. 2]\n\n\nAt that point, the authors hit the reader with a little taste of the magic in this book - that this problem is in many ways solved. That this same type of solution can be applied to a range of real world problems from parking your car to choosing a long-term partner, the concept that there exists a proof that at 37% of your search time you should immediately flip to committing has an almost Douglas Adams sort of quality to it.\n\n\n\n\n\n\nNote\n\n\n\n42: the “Answer to the Ultimate Question of Life, the Universe, and Everything” [3]\n\n\n\n\nOptimal Stopping\nChapter 1 of the book dedicates more to explaining the debated origins of what has become known as ‘The Secretary Problem’ - a common description of the Catch-22 described in the introduction. In the context of hiring an employee for example, hiring at random regardless of the candidate’s performance from a pool of 100 candidates would result in success for the employer on average 1% of the time. Success implies selecting the best applicant available in the pool.\nBy following the optimal stopping strategy, the average chances of hiring the best candidate within the pool increases to 37%. Whether or not you feel that is a good outcome is sort of subjective, but the authors discuss that this outcome remains stable as you scale up the applicant pool while random recruitment does not. For instance, interviewing a million candidates while adhering to optimal stopping could result in successfully recruiting the best candidate 37% of the time. Conversely, random recruitment under the same conditions would yield success at a rate of merely 0.0001%.\nThe example of scaling up this problem prompted me to consider some potential flaws in human behaviour, that would render any recruitment campaign unfair - optimised or otherwise. Standardising the recruitment process is known to be extremely challenging. Treating the problem like trial design, controlling for factors quickly results in confounding variables. If choosing to minimise variation in the appraisal process, you may consider retaining the same panel for each interview. Ensuring that the panel’s attention and mood are not affected by a demanding interview schedule becomes the next problem. Attempting to control for that results in extending the schedule period, possibly from days to months.\n\n\n\n\n\n\nNote\n\n\n\n““…no free lunch” (NFL) theorems… for any algorithm, any elevated performance over one class of problems is offset by performance over another class” [4]\n\n\nThe secretary problem is based on the presumption that an interviewer can only make relative comparisons or rankings of candidates, lacking absolute criteria or standardized aptitude tests to measure them against. Such an approach to interviewing, devoid of an objective threshold, would rightly be criticized for its susceptibility to human bias.\nAdditionally, the secretary problem assumes that an offer of employment would always lead to the candidate accepting, while passing up on a candidate would result in losing out on the possibility of recruiting that candidate forever. Few recruitment campaigns could operate under such conditions, however these assumptions could hold more relevance to scenarios such as selecting a parking space or selecting a long-term partner. Possibly. Although humans are complex organisms, the motivations driving that behaviour may lead to stable, predictable outcomes when considering a larger sample size.\nIn my assessment, the authors provided an intriguing treatment of optimal stopping solutions, prompting contemplation on their relevance to everyday scenarios and broader implications. However, I remain cautious about generalising this approach to most real-world scenarios, given the potential violations of the problem’s underlying assumptions.\n\n\nSorting\nAt the outset of the book, I realised that I had relied upon sorting algorithms in my professional and everyday life, but was a bit oblivious to their implementation. It is a commonplace need when exploring data to be able to sort columns of values in order to spot trends, identify outliers or problem values within a table. The authors demonstrate that sorting has become so deeply integrated into our lives that we may not even consciously acknowledge its presence or its role in enhancing our efficiency. Some of the more tangible sorting requirements explored are returning library books, the results of your Google search query and managing your Email inbox. Being able to sort based on any ordinal aspect of our data proves pivotal, allowing us to sequence our results and efficiently extract meaning.\n\n\n\n\n\n\nNote\n\n\n\n‘“What’s the best way to sort a million thirty-two-bit integers?” Without missing a beat, Obama cracked a wry smile and replied, “I think the Bubble Sort would be the wrong way to go.”’ [2, p. 65]\n\n\nThis chapter explores different sort algorithm’s efficacy. Shunning bubble-sort, dancing through insertion-sort, illustrating bucket-sort with actual buckets, talking through merge-sort complete with its own flow chart - the diversity on offer here is unexpected.\nConsider the method you choose for sorting your sock drawer. It may seem trivial, but it could have more fundamental implications than you might think. Did you ever imagine that you might be wasting precious moments of your life employing an inefficient insertion sort method? Perhaps the prospect of sock sorting inefficiencies isn’t the most pressing concern, but it does underscore a fascinating point — sorting algorithms, even in the realm of socks, can have a surprising impact on our daily routines. After all, can anything get more fundamental than starting with your socks?\nPerhaps the most meaningful takeaway for me in this chapter, is the concept that relaxing the rules of the sort can be acceptable in certain circumstances, and leads to pretty big performance gains. In all cases, performing a sort is useless without a search. Deciding on the accuracy (and time) of the sort should first consider the needs of the subsequent search.\n\n\n\n\n\n\nNote\n\n\n\n“Sorting something that you will never search is a complete waste; searching something you never sorted is merely inefficient” [2, p. 72]\n\n\nConsider a scenario where you have a vast dataset of customer transactions. If the objective is to quickly identify recent transactions within the last week, a more relaxed sorting approach might be perfectly adequate, prioritizing speed over precise chronological order. This relaxation of the sort rules can significantly improve the search efficiency and overall system performance. Understanding when and how to optimize the sort based on the requirements of the subsequent search is a crucial skill in efficiently handling data and making informed decisions.\nRelaxing the constraints on the accuracy of the sort and pre-sorting popular search items is revealed to be the approach adopted by many of the big search engines. With big data at their disposal, Google can ensure popular search terms have cached sorted results to be served immediately, ensuring a smooth experience for their users. The amount of resource and infrastructure in caching high traffic sorts across different territories must be mind-boggling.\nThis glimpse into the inner workings of search engine optimization not only makes the abstract concepts of sorting tangible but also instills a sense of admiration for the innovative minds behind these technological advancements. Paired with the forthcoming chapter on caching, the insights gained here are remarkably applicable to our everyday lives, shedding light on the strategies that power our online searches and experiences.\n\n\nCaching\nThis chapter is my highlight of the book. Beginning with a tangible concept of treating your wardrobe as a physical cache for your clothes, this chapter goes on to explore the evolution of memory management. Beginning with the fundamental trade off between capacity and performance, the authors discuss the invention of the memory hierarchy. This concept that forms the basis of modern memory management introduces a hierarchical structure, with a small, fast (high performance) amount of memory used to cache high frequency data. This memory cache would be coupled with larger, less expensive forms of slower-access memory.\n\n\n\n\n\n\nNote\n\n\n\n“The key, of course, would be managing that small, fast, precious memory so it had what you were looking for as often as possible.” [2, p. 87]\n\n\nClever optimisation of a more and more elaborate cache system is cited as the principle solution to the disparity in the exponential improvement of processing power relative to memory performance over the previous half a century. Again, the authors with their ability to ground the computer science in tangible analogy. Imagine a factory able to double its productivity year on year only to find that its output is soon limited by its supply chain - it simply cannot ingest the raw material at a rate to satisfy its output potential. Overcoming the problem of memory performance, referred to as the ‘memory wall’ has been crucial in securing performant applications and devices.\nManaging the state of the cache is treated with some detail. Competing algorithms for cache eviction - rules used to decide which memory to free up and when - are compared against the success criteria of avoiding what is known as a cache miss. A cache miss is a state where the data that you are looking for cannot be found within the cache and therefore must be retrieved from the slower memory store. Achieving this state appears to be on a par with divining the future, as who knows what the user will ask for next. Of the competing cache eviction algorithms considered are:\n\nRandom Eviction: Overwriting old data from the cache at random. Apparently not as poor a choice as it sounds.\nFirst in, First Out (FIFO): Overwriting the data that first entered the cache.\nLeast Recently Used (LRU): Overwriting the data that has remained untouched for the longest duration.\n\nAmong these, the ‘Least Recently Used’ algorithm emerges as the optimal choice, consistently outperforming the others in preventing cache misses. The authors go on to illuminate the far-reaching implications of this optimization, spanning from the physical construction of processors to the logistics of Amazon’s warehouses.\n\n\n\n\n\n\nNote\n\n\n\n“Computers’ default file-browsing interface makes you click through folders in alphabetical order - but the power of LRU suggest that you should override this, and display your files by”Last Opened” rather than “Name”. What you’re looking for will almost always be at or near the top.” [2, p. 98]"
  },
  {
    "objectID": "book-reviews/02-algorithms-to-live-by.html#analysis-and-evaluation",
    "href": "book-reviews/02-algorithms-to-live-by.html#analysis-and-evaluation",
    "title": "Algorithms to Live By",
    "section": "Analysis and Evaluation",
    "text": "Analysis and Evaluation\nAtLB proves to be an engaging read, making the intricate world of computer science accessible even to those without an undergrad in the field. The book adeptly navigates through five decades of computational thought and its implementation, ensuring the content never overwhelms the casual reader. One of its strengths lies in emphasizing tangible analogies and illustrating practical implications for everyday life, prompting thought and curiosity.\nThere are a number of instances where the problem domains have been framed in a way that has not aged so well. By today’s standard, The Secretary Problem could do with an overhaul. The authors acknowledge that the framing of this problem is a product of its time and that it could use an overhaul. But an aspect of AtLB is to provide the historic context and evolution of an idea, where it is available. In doing so, the authors reveal the stories of the people behind the big ideas, aspects of their life, work and the problems they were wrangling.\nFor IT professionals, while AtLB may not break new ground, it excels in illustrating the impact of optimization in a way that surpasses many formal textbooks. The book effectively builds awareness around the subject and its role in advancing society, providing a valuable framework for professionals in the field."
  },
  {
    "objectID": "book-reviews/02-algorithms-to-live-by.html#comparisons",
    "href": "book-reviews/02-algorithms-to-live-by.html#comparisons",
    "title": "Algorithms to Live By",
    "section": "Comparisons",
    "text": "Comparisons\nThis book offers a treatment of understanding the nuts and bolts and how society has implemented and optimised in response to the knowledge. This book can therefore be compared to a broad selection of popular science. Some similar books are:\n\nNaked Statistics by Charles Wheelan [5] explores how statistics can be misused to obfuscate key findings or serve a political agenda. It serves as a toolkit for understanding algorithms and being aware of their potential misapplications, providing readers with the ability to discern and critically analyse statistical information.\nHello World by Hannah Fry [6] delves into the implementation of algorithms in complex domains like Crime, Justice, and Healthcare. The book grapples with intricate ethical challenges, approaching the subject with warranted skepticism and scrutinising the far-reaching implications of algorithmic applications in various societal domains.\n\nIn contrast, AtLB offers a distinctly optimistic treatment, aiming to educate and instill an appreciation for the scale and implication of optimized solutions. It provides readers with an understanding of how computational thought has shaped our world and the positive impacts it can have in our rapidly evolving society."
  },
  {
    "objectID": "book-reviews/02-algorithms-to-live-by.html#recommendation",
    "href": "book-reviews/02-algorithms-to-live-by.html#recommendation",
    "title": "Algorithms to Live By",
    "section": "Recommendation",
    "text": "Recommendation\nI completed my reading of ‘AtLB’ in January of 2023, and to pen down this review, I revisited the book, speed-reading through noteworthy sections to reacquaint myself with its messages. At times I began deep reading the material, drawn in by its captivating narrative and insightful content. The book is a treasure trove of anecdotes and key summaries.\nIn my estimation, this is too good to miss for anyone with an interest in logic, analysis or is generally curious about exactly what computer science has achieved for us so far."
  },
  {
    "objectID": "book-reviews/04-dark-data.html",
    "href": "book-reviews/04-dark-data.html",
    "title": "Dark Data",
    "section": "",
    "text": "Image credit: https://api.ndla.no/"
  },
  {
    "objectID": "book-reviews/04-dark-data.html#book-summary",
    "href": "book-reviews/04-dark-data.html#book-summary",
    "title": "Dark Data",
    "section": "Book Summary",
    "text": "Book Summary\n…"
  },
  {
    "objectID": "book-reviews/04-dark-data.html#on-the-author",
    "href": "book-reviews/04-dark-data.html#on-the-author",
    "title": "Dark Data",
    "section": "On the Author",
    "text": "On the Author\n…"
  },
  {
    "objectID": "book-reviews/04-dark-data.html#takeaways",
    "href": "book-reviews/04-dark-data.html#takeaways",
    "title": "Dark Data",
    "section": "3 Takeaways",
    "text": "3 Takeaways\n…"
  },
  {
    "objectID": "book-reviews/04-dark-data.html#in-summary",
    "href": "book-reviews/04-dark-data.html#in-summary",
    "title": "Dark Data",
    "section": "In Summary",
    "text": "In Summary\n…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to The Data Savvy Corner!",
    "section": "",
    "text": "A site to store blogs about programming concepts, software development and data science. Generally things that future me will be thankful that I’ve recorded these things somewhere."
  },
  {
    "objectID": "index.html#intro",
    "href": "index.html#intro",
    "title": "Welcome to The Data Savvy Corner!",
    "section": "",
    "text": "A site to store blogs about programming concepts, software development and data science. Generally things that future me will be thankful that I’ve recorded these things somewhere."
  },
  {
    "objectID": "index.html#site-overview",
    "href": "index.html#site-overview",
    "title": "Welcome to The Data Savvy Corner!",
    "section": "Site Overview",
    "text": "Site Overview\n\nByte-Wise Musings : Technical blogs, Explanations, tutorials, how-to guides. All articles are categorised according to the excellent diataxis framework.\nProductivity Pulse : Music reviews. I listen to strictly non-verbal music when programming. This was a habit that I developed while working within a busy office environment. Instrumental music of several genres has helped me efficiently establish a state of mental flow required for deep concentration. I’ll include reviews of some of my favourite albums here, categorised by genre.\nBookmarked Insights : Book reviews. Much of my casual reading is non-fiction tangential to programming. I’ll review material here that I have found to be supportive of my work in various aspects, categorised by genre."
  },
  {
    "objectID": "index.html#recent-articles",
    "href": "index.html#recent-articles",
    "title": "Welcome to The Data Savvy Corner!",
    "section": "Recent Articles",
    "text": "Recent Articles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDark Data\n\n\nWhy what you don’t know matters\n\n\n\nNon-fiction\n\n\nData\n\n\nScience\n\n\nMathematics\n\n\nStatistics\n\n\n\nReflecting on Neil Postman’s criticism of modern culture.\n\n\n\n\n\nJul 1, 2024\n\n\nRich Leyshon\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub Actions Security\n\n\n\n\n\n\nHow-to\n\n\nGitHub\n\n\nGitHub Actions\n\n\nCI:CD\n\n\nSecurity\n\n\n\nCan you actually rely on that pre-built Action you rely on?\n\n\n\n\n\nJun 26, 2024\n\n\nRich Leyshon\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nParametrized Tests With Pytest in Plain English\n\n\n\n\n\n\nExplanation\n\n\npytest\n\n\nUnit tests\n\n\nparametrize\n\n\npytest-in-plain-english\n\n\n\nPlain English Discussion of Pytest Parametrize\n\n\n\n\n\nJun 7, 2024\n\n\nRich Leyshon\n\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhen Worlds Collide\n\n\n\n\n\n\nInstrumental\n\n\nSynthwave\n\n\nElectronic\n\n\nElectro\n\n\n\nThomas Barrandon’s understated scifi masterpiece\n\n\n\n\n\nMay 19, 2024\n\n\nRich Leyshon\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPytest With tmp_path in Plain English\n\n\n\n\n\n\nExplanation\n\n\npytest\n\n\nUnit tests\n\n\ntmp_path\n\n\ntmp_path_factory\n\n\nfixtures\n\n\npytest-in-plain-english\n\n\n\nPlain English Discussion of Pytest Temporary Fixtures.\n\n\n\n\n\nApr 25, 2024\n\n\nRich Leyshon\n\n\n16 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome to The Data Savvy Corner!",
    "section": "About Me",
    "text": "About Me\n\nI am a Senior Data Scientist at the Office for National Statistics Data Science Campus. I enjoy programming & automation and an advocate for the standards outlined in Quality Assurance of Code for Analysis & Research. For more detail regarding my professional experience, please follow this link to my resumé. And to collaborate with me, here’s my GitHub profile."
  },
  {
    "objectID": "music-reviews/01-ghostrunner-ost.html",
    "href": "music-reviews/01-ghostrunner-ost.html",
    "title": "Ghostrunner",
    "section": "",
    "text": "I’m thrilled to kick off the music review section of my blog with an engaging work by one of my favourite synth artists.\n\n\n\n\n\n\nNote\n\n\n\nThis is a record with high turnover & great energy - one to put on loop when you need to pull out all the stops on a thorny problem.\n\n\nGhostrunner was one of the acclaimed independent platformer games of 2020. It’s aged particularly well on the latest generation of consoles, thanks in no small part to its fantastic soundtrack. The developers made a safe bet in selecting Daniel Deluxe to compose the nihilistic, cyberpunk soundtrack. Deluxe has been producing knockout darksynth since 2014 and has been a regular feature of my working music since I started programming.\nDeluxe’s 2014 effort by the name of Darkness was one of the first synthwave songs I had come across and partly the reason why I have gone so deep with this genre. It’s a fantastic piece of 80s romantic nostalgia with evocative speech samples taken from the 1985 fantasy movie Legend, starring Tim Curry as the eponymous Darkness - a character that haunted the nightmares of many ’80s children. Much of Deluxe’s catalogue is worthy of your time and Ghostrunner OST is the crowning achievement. As his fifth and latest (at time of writing) studio album, Ghostrunner draws and builds upon the stylistic themes explored in previous albums. Though in this effort, a shorter average track length has kept things laser-focussed. Ghostrunner is a great introduction to synthwave of the catalogue of Daniel Deluxe as a whole, but luckily for the newcomer - there’s plenty of depth to mine from this musical vein."
  },
  {
    "objectID": "music-reviews/01-ghostrunner-ost.html#introduction",
    "href": "music-reviews/01-ghostrunner-ost.html#introduction",
    "title": "Ghostrunner",
    "section": "",
    "text": "I’m thrilled to kick off the music review section of my blog with an engaging work by one of my favourite synth artists.\n\n\n\n\n\n\nNote\n\n\n\nThis is a record with high turnover & great energy - one to put on loop when you need to pull out all the stops on a thorny problem.\n\n\nGhostrunner was one of the acclaimed independent platformer games of 2020. It’s aged particularly well on the latest generation of consoles, thanks in no small part to its fantastic soundtrack. The developers made a safe bet in selecting Daniel Deluxe to compose the nihilistic, cyberpunk soundtrack. Deluxe has been producing knockout darksynth since 2014 and has been a regular feature of my working music since I started programming.\nDeluxe’s 2014 effort by the name of Darkness was one of the first synthwave songs I had come across and partly the reason why I have gone so deep with this genre. It’s a fantastic piece of 80s romantic nostalgia with evocative speech samples taken from the 1985 fantasy movie Legend, starring Tim Curry as the eponymous Darkness - a character that haunted the nightmares of many ’80s children. Much of Deluxe’s catalogue is worthy of your time and Ghostrunner OST is the crowning achievement. As his fifth and latest (at time of writing) studio album, Ghostrunner draws and builds upon the stylistic themes explored in previous albums. Though in this effort, a shorter average track length has kept things laser-focussed. Ghostrunner is a great introduction to synthwave of the catalogue of Daniel Deluxe as a whole, but luckily for the newcomer - there’s plenty of depth to mine from this musical vein."
  },
  {
    "objectID": "music-reviews/01-ghostrunner-ost.html#music-for-cyborg-ninjas",
    "href": "music-reviews/01-ghostrunner-ost.html#music-for-cyborg-ninjas",
    "title": "Ghostrunner",
    "section": "Music for Cyborg Ninjas",
    "text": "Music for Cyborg Ninjas\nIt’s worth noting that this music isn’t for everyone. But I’ve never favoured that sort of music myself and wouldn’t find the time to review it. This music was written to energise players ready to engage in parkour-slash-em-up fun, without distracting from the brief interludes of narrative. As with all my favoured music for work, you won’t find any ballads or warbling vocals here, so if you want something to hum along to, you’d best find another blog!\nBut if you’re down for this, I’d recommend clicking play on the spotify player button and enjoying the tunes while you read.\n\n\n Let’s take a look at the audio feature analysis for this album. For a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature.\n\n\n\nGhostrunner OST is clearly an instrumental album. This plot shows that the tracks all score lowly on speechiness and highly on instrumentalness, with tight distributions. The album generally displays high measures for energy and danceability, though with broader distributions than that displayed by speechiness & instrumentalness. The majority of the album is clear, steady and fast rhythm. Interestingly, the acousticness feature presents a very broad range of distribution in the confidence that tracks do not have electrical amplification. This really does not seem correct to me, as the album is unapologetically synth all the way. This is potentially affected by production effects favouring reverb and echo, but that is surmising on my part. Lastly, I’ll turn to valence, which is a measure of positivity within the music. This feature is right-skewed. A higher number of lower valence values makes sense, as much of the album carries a sinister, Orwellian, digital-dystopian tone - generally interpretable as negative I would agree. The presence of higher values in this feature suggests that a fewer number of the tracks may be characterised with positive emotion."
  },
  {
    "objectID": "music-reviews/01-ghostrunner-ost.html#the-tracks",
    "href": "music-reviews/01-ghostrunner-ost.html#the-tracks",
    "title": "Ghostrunner",
    "section": "The Tracks",
    "text": "The Tracks\nBelow are the mean audio feature values for each track. The last row (green) presents a mean summary of the album.\n\n\n\n\n\n\ntrack_name\ndanceability\nenergy\nspeechiness\nacousticness\ninstrumentalness\nvalence\n\n\n\n\nAir\n0.5730\n0.7320\n0.02960\n0.2660000\n0.8930\n0.05400\n\n\nInfiltrator\n0.6070\n0.5600\n0.04340\n0.0239000\n0.8120\n0.03720\n\n\nBlood and Steel\n0.6960\n0.3260\n0.05130\n0.0061700\n0.8210\n0.03760\n\n\nDharma\n0.6160\n0.8290\n0.03830\n0.1510000\n0.8860\n0.17100\n\n\nCapture\n0.7070\n0.6310\n0.04420\n0.0439000\n0.8350\n0.40100\n\n\nForget the Past\n0.4490\n0.5400\n0.03330\n0.2940000\n0.9050\n0.03320\n\n\nAccess Denied\n0.5680\n0.5320\n0.03660\n0.4890000\n0.8820\n0.04460\n\n\nLet Them Know\n0.8380\n0.6600\n0.05060\n0.7990000\n0.7790\n0.74100\n\n\nSector\n0.5450\n0.8440\n0.03040\n0.0035500\n0.7040\n0.03770\n\n\nFactory\n0.4630\n0.9280\n0.03680\n0.3610000\n0.8990\n0.29100\n\n\nStriker\n0.5340\n0.8180\n0.03440\n0.0102000\n0.8380\n0.22600\n\n\nCelerity\n0.6430\n0.8840\n0.03360\n0.0109000\n0.8540\n0.03650\n\n\nThe Orb\n0.5120\n0.9470\n0.05290\n0.3180000\n0.8410\n0.74600\n\n\nTruth to Power\n0.6530\n0.8970\n0.05590\n0.3430000\n0.8700\n0.61300\n\n\nBlaster\n0.5720\n0.8750\n0.03870\n0.2190000\n0.8380\n0.09020\n\n\nRazor\n0.3250\n0.9190\n0.05040\n0.0002510\n0.6570\n0.05070\n\n\nVendetta\n0.6400\n0.8900\n0.05720\n0.0502000\n0.8240\n0.14500\n\n\nAir (Cybervoid)\n0.5180\n0.7210\n0.02730\n0.3240000\n0.9510\n0.16400\n\n\nSolitude\n0.4830\n0.9510\n0.04010\n0.0090000\n0.8210\n0.39000\n\n\nSundown\n0.2860\n0.3880\n0.06380\n0.2700000\n0.6560\n0.06490\n\n\nAlbum Mean\n0.5614\n0.7436\n0.04244\n0.1996036\n0.8283\n0.21873\n\n\n\n\n\n\n\n\nAir Beautiful synth bleep arrangement overlayed with menacing robot groans. A mashy bass beat rounds out this atmospheric ode to a mysterious, futuristic cityscape.\nInfiltrator Bassy experiments with tempo against a stripped-back rhythm section. Reminds me of scrap brain zone on sonic the hedgehog. Intermittent alert signals and robotic glitches. Glimpses of some wonderful serene motif before being thrust back into the main formation. At the time of writing, this track is currently the most popular on this album.\nBlood and steel This track is very recognizable. Inspires an unsettling feeling of Deja vú within the listener. Conjuring images of a futuristic crime scene investigation. A dreamy intro before a strong drum & bass injection. The track culminates in a growing phaser storm that smothers the rest of the track before it cuts at the point of its culmination. A highlight of the album.\nDharma Anyone of a certain age will likely remember ABC’s gripping sci fi thriller Lost, of the early noughties. Dharma was the name of the malevolent. corporation behind much of the protagonists woes. This track strikes a fittingly menacing tone. A dripping, ticking lick loops over a bossy bass hook. Lazer phasers growing in tone & pitch throughout for good measure.\nCapture A zoned out, decidedly creepy intro with an android snigger looped over bell tolls, metallic pipes clanging and a menagerie of industrial noise. A slower beat chugging away only to take frequent breaks to peak at you from the shadows. Sinister but with style.\nForget the past Begins on a melancholy reflection that seems to consider the song’s title some advice to self. Continues the chilling set by the previous track with an inhumane battery of bass noise, giving way to serene, elongated spacetrips of harmonic synth.\nAccess denied A collective of error signals experienced through a thick gelatinous membrane. This song feels like being rudely awoken from the matrix. An electric wire to the brain in the high tempo intro of a squelchy phaser bleep on loop. Drum & cymbal snares introduce some peril to the experience. Another favourite of mine.\nLet them know High pitch melodic trills over a stripped back drum beat with a growing, bubbling, frothy arsenal of phaser barrages. The trills give way to a strong bass line, dipping in and out of the foreground. Halfway in, a Wipeout-style high speed fade is accompanied by checkpoint ticks that sound a bit like hitting metal on metal. This track has a great time trial feel. The layers wipe together in a busy, well-structured crescendo. This is the highlight of the album for me.\nSector Starting on a mysterious, hyper-speed contrail of sound. With some productionised robot intonation rapping over the crunchy bass drum. Joined later by a Japanese-inspired treble section for the interested Japanophile. Reminiscent of the excellent Triads by Code Elektro (review sure to come at some point).\nFactory A clever mix of major and minor android aria signals. Locking in some bassy robo groans with some mysterious echo-flared laser trills. The track descends into a mission impossible repeato while adding some beautiful high frequency synth melody.\nStriker Enters with an industrial explosion experienced from a vast distance, rippling out into a wide pool of synth integration tests, like circuit fingers working through a mossy undertow. High frequency synth chimes with a glitched notation slips away to a seedy, menacing robotic bridge. The pretty synth returns to work over the menacing robo. A close second favourite.\nCelerity - Celerity is not a word I have encountered before, but according to dictionary.com it means swift movement. An interesting choice of title for this track as I interpret the rhythm to be more steady. Starting on a Prodigy-like industrial D&B replete with an accompanying fairground loop. A glitched-up affair with a jarring, chugging interference playing with your expectations throughout. This moves on to some graceful tech wipes for a moment before the interference dominates once more. Ending on a commandeering double-time march to up the ante, this track conjures a long-distance robotic pursuit.\nThe orb At this point in the album, we encounter a run of shorter, more intense songs. The orb has a brilliant, shimmering, robot talk phasing in and out of consciousness. Moving on to experiment with beat and tone patterns, swaying in and out of the fore as the beat establishes command. Short, but oh, so sweet.\nTruth to power A chunky drum section overlayed by a whispered android complaint growing to take full form, before relegated to radio playback. The android voice jumps in and out of focus while comets wipe by and leaves a trail of icy debris zooming past the listener. Likely to be the sort of music that Chat-GPT generates when frustrated by the lack of imagination presented by its users.\nBlaster Watch out for this one - there’s something larger rising out of the depths, waving away in front of your ears. Once surfaced, it reveals itself to be largely drum-engined automation with a bassy tempest of brass billowing away under the surface, like an automaton that’s managed to preserve the very last of the human tuba instrumentalists, lifespan extended with cybernetic implants.\nRazor Accusatory synth calls out against a halloween-esque synth sustain. Emerging out onto an ’80s horror set before adopting an attention-demanding rhythm. High-end synth parrots away while some os-level tech blurb ramps up to the fore.\nVendetta A reliable, robotic trill dances between pitches, laying off for some harmonious electro organ to swing in and out overhead. Machine gun volleys shower their weight to the beat.\nAir (Cybervoid) A welcome reprise to the first track of the album revisits a suspicious first day in a cybernetic future. A subtle difference to the original but more immediate. Blazing and warm bass tones intervene the laser hook and encompasses around the track as it progresses.\nSolitude A dreamy space-age synthetic Bach rises from the murky depths to sit in a shimmering orchestral pool, composing his techno-symphonies. In stark contrast, squishy, rubbery deep bass android vocals sound out over the alien morass.\nSundown A gothic android monk prayer cedes to laser piano overlays. Cutting out to flatlined electro strings & the futuristic remnants of whatever birds evolved into once AI conquered the Earth. A contemplative end to a high-energy album. Likely to reflect a sombre tone to the end of the video game, though I cannot confirm this as I was pretty rubbish when I tried it."
  },
  {
    "objectID": "music-reviews/01-ghostrunner-ost.html#highs-and-lows",
    "href": "music-reviews/01-ghostrunner-ost.html#highs-and-lows",
    "title": "Ghostrunner",
    "section": "Highs and Lows",
    "text": "Highs and Lows\nNo real lows to talk of in this album, but the three standout tracks that I recommend are:\n\nLet them know\nStriker\nBlood and steel"
  },
  {
    "objectID": "music-reviews/01-ghostrunner-ost.html#overall",
    "href": "music-reviews/01-ghostrunner-ost.html#overall",
    "title": "Ghostrunner",
    "section": "Overall",
    "text": "Overall\nA smashing piece of dark synthpop that delivers focussed energy while conserving your concentration. The theme is sinister at points but never loses its potency for concentrainment. A good choice for smashing out some shallow work or a briefing note in a short space of time. Perhaps not as inspiring or uplifting as some of my other instrumental ambient choices for creative energy. But this album establishes a steady cadence and will keep your foot tapping while your hands do the typing."
  },
  {
    "objectID": "music-reviews/03-dk-county-piano.html",
    "href": "music-reviews/03-dk-county-piano.html",
    "title": "Donkey Kong Country Relaxing Piano (Instrumental)",
    "section": "",
    "text": "Donkey Kong Country was a seminal moment for the SNES. I recall when a childhood friend of mine had received it as an Easter present. The graphics were mind blowing, the 2 player couch co-op highly addictive, and the audio certainly made an impression that lasted longer than I had realised. At the time I recall I hadn’t made the transition to the 16-bit generation, and in comparison to my decidedly lacklustre Master System, this looked like the future of video gaming.\nThirty years later, I asked Alexa to play some soothing dog music for the dachshunds while I did the school run - like most dachshunds, they’re the anxious type. As I returned to start work, my nostalgia radar was piqued as a beautiful reimagining of Aquatic Ambience sweetly serenaded the pups. That was my serendipitous introduction to this little melancholy gem.\n\n\n\nFor a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\nAs the album is so short, we see some very tight distributions here and no surprises this time, apart from a middling danceability (ballet perhaps?). All tracks are very high in instrumentalness and acousticness. The album is very low in speechiness as you’d expect. Energy is low throughout. Even where the source material is decidedly spritely, it is played here in a soft and ponderous tempo. A slightly broader distribution in valence and danceability can be observed.\nPut this album on when you need to pick through failing code. It will encourage alpha brain waves and keep you from switching context to something less problematic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature."
  },
  {
    "objectID": "music-reviews/03-dk-county-piano.html#swinging-to-serenity",
    "href": "music-reviews/03-dk-county-piano.html#swinging-to-serenity",
    "title": "Donkey Kong Country Relaxing Piano (Instrumental)",
    "section": "",
    "text": "Donkey Kong Country was a seminal moment for the SNES. I recall when a childhood friend of mine had received it as an Easter present. The graphics were mind blowing, the 2 player couch co-op highly addictive, and the audio certainly made an impression that lasted longer than I had realised. At the time I recall I hadn’t made the transition to the 16-bit generation, and in comparison to my decidedly lacklustre Master System, this looked like the future of video gaming.\nThirty years later, I asked Alexa to play some soothing dog music for the dachshunds while I did the school run - like most dachshunds, they’re the anxious type. As I returned to start work, my nostalgia radar was piqued as a beautiful reimagining of Aquatic Ambience sweetly serenaded the pups. That was my serendipitous introduction to this little melancholy gem.\n\n\n\nFor a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\nAs the album is so short, we see some very tight distributions here and no surprises this time, apart from a middling danceability (ballet perhaps?). All tracks are very high in instrumentalness and acousticness. The album is very low in speechiness as you’d expect. Energy is low throughout. Even where the source material is decidedly spritely, it is played here in a soft and ponderous tempo. A slightly broader distribution in valence and danceability can be observed.\nPut this album on when you need to pick through failing code. It will encourage alpha brain waves and keep you from switching context to something less problematic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature."
  },
  {
    "objectID": "music-reviews/03-dk-county-piano.html#the-tracks",
    "href": "music-reviews/03-dk-county-piano.html#the-tracks",
    "title": "Donkey Kong Country Relaxing Piano (Instrumental)",
    "section": "The Tracks",
    "text": "The Tracks\nBelow are the mean audio feature values for each track. The last row (green) presents a mean summary of the album.\n\n\n\n\n\n\ntrack_name\ndanceability\nenergy\nspeechiness\nacousticness\ninstrumentalness\nvalence\n\n\n\n\nIsland Swing - Instrumental\n0.3940\n0.02040\n0.05620\n0.9900\n0.9450\n0.2720\n\n\nForest Frenzy - Instrumental\n0.3770\n0.01800\n0.05760\n0.9950\n0.9540\n0.2070\n\n\nAquatic Ambiance - Instrumental\n0.3890\n0.03830\n0.06950\n0.9870\n0.9530\n0.1030\n\n\nSimian Segue - Instrumental\n0.3210\n0.04400\n0.05730\n0.9900\n0.9420\n0.2570\n\n\nGang-Plank Galleon - Instrumental\n0.4400\n0.01690\n0.07810\n0.9950\n0.9600\n0.2500\n\n\nAlbum Mean\n0.3842\n0.02752\n0.06374\n0.9914\n0.9508\n0.2178\n\n\n\n\n\n\n\n\nIn the table we get to see the cause of the broader distribution in valence and danceability. Island Swing, Simian Segue and Gang-Plank Galleon drag the mean valence of the album up, while in terms of danceability, Gang-Plank-Galleon is a relative toe tapper.\nIsland Swing\nThe album starts with a gorgeous, plaintiff melody, turning to the distinctive saloon-inspired hook that clearly identifies the original game. This track pauses for new material at points so much that I felt it must be a medley. However on reprising the original audio, it does pay a faithful tribute to the original. It finishes as sweetly as it started and sets the scene well for what’s to be expected.\nForest Frenzy\nA deeply ponderous cadence interspersed by moments of treble clarity played so softly. The artist really has tempered the frenzy in this forest very well. What remains is a lullaby.\nAquatic Ambience\nThis track absolutely steals the show. The original was genius, but this is such an exquisite rendition. The movements are played very freely, softening the tighter tempo of the original. This is the track not to miss.\nSimian Segue\nThis is the track from the level select map. The valence rises a fair bit here, which is very understandable considering this must be the most recognisable hook from the game. It’s still a bit playful, even when played so gently. Some powerful major chord shifts making this a decidedly cheery affair.\n\nGang-Plank Galleon\nI have no recollection of this track. It must be owing to the fact that this was the music for the final boss of the game. I was clearly not good enough to make it this far as a kid. The song begins as its title would suggest with a decidedly swashbuckling theme. This soon recedes, handing the limelight to a beautiful melancholy solo that sounds like a distinct song. Once more, reprising the original, it’s clear that this is a pretty faithful reprise of the soundtrack that came with the game."
  },
  {
    "objectID": "music-reviews/03-dk-county-piano.html#overall",
    "href": "music-reviews/03-dk-county-piano.html#overall",
    "title": "Donkey Kong Country Relaxing Piano (Instrumental)",
    "section": "Overall",
    "text": "Overall\nWhether or not 1990’s Nintendo is your thing, what a beautiful effort from the artist. I’d recommend listening to their other albums in a similar vein. But this one is my favourite, though I am likely biased due to some formative connection to the source material. But whether serenity is your goal, or maybe you just need to get a baby off to sleep, this little album is the right tool for the job.\nFor more music to encourage your work efforts, check out Productivity Pulse."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html",
    "href": "music-reviews/05-when-worlds-collide.html",
    "title": "When Worlds Collide",
    "section": "",
    "text": "The EP offers a mesmerising journey through electronic soundscapes, evoking a sense of otherworldly exploration. Barrandon masterfully combines retro synthwave elements with modern cinematic vibes.\n\n\n\nFor a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\nOverall, a laid-back and peaceful affair, with distribution in the lower valence and speechiness values. Acousticness and instrumentalness are mostly confined to lower values, with the distinct exception of the final track, “A New Start”. A broad middle-range distribution in energy and danceability is common across all tracks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#a-cosmic-convergence",
    "href": "music-reviews/05-when-worlds-collide.html#a-cosmic-convergence",
    "title": "When Worlds Collide",
    "section": "",
    "text": "The EP offers a mesmerising journey through electronic soundscapes, evoking a sense of otherworldly exploration. Barrandon masterfully combines retro synthwave elements with modern cinematic vibes.\n\n\n\nFor a formal description of the audio feature categories presented here, please consult the Spotify Developer API Documentation.\nOverall, a laid-back and peaceful affair, with distribution in the lower valence and speechiness values. Acousticness and instrumentalness are mostly confined to lower values, with the distinct exception of the final track, “A New Start”. A broad middle-range distribution in energy and danceability is common across all tracks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Ridge Plot (Click to expand)\n\n\n\n\n\nThese ridge plots show a histogram of the mean audio feature values across each track in the album. Look across the horizontal axis at where the peaks for each feature occur. For example, if the album contains lots of tracks with busy vocals, the speechiness feature will show a peak to the right of the horizontal axis. Conversely, an instrumental album would have a speechiness peak to the left of the horizontal axis but you would also expect it to have a peak to the right of the axis in the instrumentalness feature."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#the-tracks",
    "href": "music-reviews/05-when-worlds-collide.html#the-tracks",
    "title": "When Worlds Collide",
    "section": "The Tracks",
    "text": "The Tracks\nBelow are the mean audio feature values for each track. The last row (green) presents a mean summary of the album.\n\n\n\n\n\n\ntrack_name\ndanceability\nenergy\nspeechiness\nacousticness\ninstrumentalness\nvalence\n\n\n\n\nExtinction\n0.511\n0.3610\n0.03490\n0.199000\n0.9360\n0.0741\n\n\nDoomsday Clock\n0.477\n0.5160\n0.02780\n0.001440\n0.3090\n0.1080\n\n\nPale Blue Dot\n0.172\n0.3050\n0.03360\n0.020400\n0.8900\n0.0347\n\n\nFragment\n0.559\n0.5500\n0.04330\n0.069800\n0.1870\n0.0734\n\n\nNew Start\n0.236\n0.0355\n0.03110\n0.836000\n0.5810\n0.0388\n\n\nAlbum Mean\n0.391\n0.3535\n0.03414\n0.225328\n0.5806\n0.0658\n\n\n\n\n\n\n\n\n…"
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#extinction",
    "href": "music-reviews/05-when-worlds-collide.html#extinction",
    "title": "When Worlds Collide",
    "section": "Extinction",
    "text": "Extinction\nThe opening track sets the tone with its haunting melodies and pulsating beats. It’s reminiscent of the Stranger Things theme, yet maintains its unique identity, aiming for a more subtle tension to emerge."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#doomsday-clock",
    "href": "music-reviews/05-when-worlds-collide.html#doomsday-clock",
    "title": "When Worlds Collide",
    "section": "Doomsday Clock",
    "text": "Doomsday Clock\nFrom the outset a percussive metronome gradually builds tension. A brooding composition, the synths create an eerie atmosphere, leaving you on the edge of anticipation."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#pale-blue-dot",
    "href": "music-reviews/05-when-worlds-collide.html#pale-blue-dot",
    "title": "When Worlds Collide",
    "section": "Pale Blue Dot",
    "text": "Pale Blue Dot\n\n\n\nLook again at that dot. That’s here. That’s home. That’s us. (Carl Sagan)\n\n\nA beautiful piece that balances melancholy and hope. The delicate piano interludes blend seamlessly with the electronic textures."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#fragment",
    "href": "music-reviews/05-when-worlds-collide.html#fragment",
    "title": "When Worlds Collide",
    "section": "Fragment",
    "text": "Fragment\nThe standout track, this shorter number feels like a glitch in the matrix. Its beats and fragmented sounds add an intriguing layer."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#new-start",
    "href": "music-reviews/05-when-worlds-collide.html#new-start",
    "title": "When Worlds Collide",
    "section": "New Start",
    "text": "New Start\nShort but impactful, this track offers a sense of resolution. The uplifting melodies and arpeggios leave you with a feeling of renewal. Peaceful piano is used to great effect, a distinct step change to the synth of the rest of the EP."
  },
  {
    "objectID": "music-reviews/05-when-worlds-collide.html#overall",
    "href": "music-reviews/05-when-worlds-collide.html#overall",
    "title": "When Worlds Collide",
    "section": "Overall",
    "text": "Overall\nWhen Worlds Collide is a journey that transcends time and space. Each track contributes to the EP’s cohesive narrative, making it a must-listen for fans of synthwave and ambient music.\nFor more music to encourage your work efforts, check out Productivity Pulse."
  }
]