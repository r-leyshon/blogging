---
title: "The Fiddly Bits of Pytest"
author: "Rich Leyshon"
date: "March 24 2024"
description: "Plain English discussion of the more intrictate aspects of testing with pytest."
categories:
    - Explanation
    - pytest
    - Unit tests
image: "https://images.pixexid.com/sculptural-simplicity-monochrome-background-highlighting-the-beauty-of-minimali-jmhkipzb.jpeg?h=699&amp;q=70"
image-alt: "Sculptural simplicity, monochrome background highlighting the beauty of minimalist sculptures by [Ralph](https://pixexid.com/profile/cjxrsxsl7000008s6h21jecoe)"
toc: true
---

<figure class=center>
  <img class="shaded_box" width=400px src="https://images.pixexid.com/sculptural-simplicity-monochrome-background-highlighting-the-beauty-of-minimali-jmhkipzb.jpeg"></img>
  <figcaption style="text-align:center;">Creative commons license by [Ralph](https://pixexid.com/profile/cjxrsxsl7000008s6h21jecoe)</figcaption>
</figure>

## Introduction

`pytest` is a testing package for the python framework. It is broadly used to
quality assure code logic. This article discusses some of the more intricate
features of `pytest`, favouring accessible language and simple examples to
describe the scenarios.


- <a href="https://docs.pytest.org/en/8.0.x/" target="_blank">Pytest documentation</a>

:::{.callout collapse="true"}

### A Note on the Purpose (Click to expand)

This article intends to discuss clearly. It doesn't aim to be clever or
impressive. Its aim is to extend the audience's understanding of the more
intricate features of `pytest` by describing their utility with simple code
examples.  

:::

### Intended Audience

Programmers with a working knowledge of python and some familiarity with
`pytest` and packaging. The type of programmer who has wondered about how to
mock an api call or how to optimise their test code.

### What You'll Need:

- [ ] Preferred python environment manager (eg `conda`)
- [ ] `pip install pytest==8.1.1`
- [ ] Git
- [ ] GitHub account
- [ ] Command line access

### Preparation

This blog is accompanied by code in
[this repository](https://github.com/r-leyshon/pytest-fiddly-examples). The
main branch provides a template with the minimum structure and requirements
expected to run a `pytest` suite. The repo branches contain the code used in
the examples of the following sections.

Feel free to fork or clone the repo and checkout to the example branches as
needed.

## Fixtures

The example code that accompanies this section is available in the fixtures
branch of the 
[example code repo](https://github.com/r-leyshon/pytest-fiddly-examples/tree/fixtures).

### What are fixtures?

Data. Well, data provided specifically for testing purposes. This is the
essential definition for a fixture. One could argue the case that fixtures are
more than this. Fixtures could be environment variables, class instances,
connection to a server or whatever dependencies your code needs to run.

I would agree that fixtures are not just data. But that all fixtures return
data of some sort, regardless of the system under test.

### When would you use fixtures?

It's a bad idea to commit data to a git version-controlled repository, right?
Agreed. Though fixtures are rarely 'real' data. The data used for testing
purposes should be minimal and are usually synthetic. 

**Minimal fixtures** conform to the schema of the actual data under test.
These fixtures will be as small as possible while capturing all known important
cases. Keeping the data small maintains a performant test suite and avoids
problems associated with large files and git version control.

If you have ever encountered a problem in a system that was caused by a
problematic record in the data, the aspect of that record that broke your
system should absolutely make it into the next version of your minimal test
fixture. Writing a test that checks that the codebase can handle such problem
records is known as 'regression testing' - safeguarding against old bugs
resurfacing when code is refactored or new features are implemented. This
scenario commonly occurs when a developer unwittingly violates Chesterton's
Principle.


<iframe src="https://www.youtube.com/embed/qPGbl2gxGqI" class="shaded-box" title="Chesterton's Fence by Sprouts" style="display: block; width: 600px; height: 338px" frameborder="0">
</iframe>

Many thanks to my colleague Mat for pointing me towards this useful analogy. A
considerate developer would probably include a comment in their code about a
specific problem that they've handled (like erecting a sign next to
Chesterton's fence). An experienced developer would do the same, and also write
a regression test to ensure the problem doesn't re-emerge in the future
(monitoring the fence with CCTV...). Discovering these problem cases and
employing defensive strategies avoids future pain for yourself and colleagues.

As you can imagine, covering all the important cases and keeping the fixture
minimal are a compromise. At the outset of the work, it may not be obvious what
problematic cases may arise. Packages such as
[`hypothesis`](https://hypothesis.readthedocs.io/en/latest/) allow you to
generate awkward cases. Non-utf-8 strings anyone? Hypothesis can generate
these test cases for you - *ăѣ𝔠ծềſģȟᎥ𝒋ǩľḿꞑȯ𝘱𝑞𝗋𝘴ȶ𝞄𝜈ψ𝒙𝘆𝚣*.

**Non-disclosive fixtures** are those that do not expose personally
identifiable or commercially-sensitive information. If you are working with
this sort of data, it is necessary to produce toy test fixtures that mimic the
schema of the real data. Names and addresses can be de-identified to random
alphanumeric strings. Location data can be adjusted with noise. The use of
dummy variables or categories can mitigate the risk of disclosure by
differencing.

By adequately anonymising data and testing problem cases, the programmer
exhibit their duty to upholding organisational duty to the General Data
Protection Regulation, specifically to the duties to accurately store, process,
retain and erase personally-identifiable information.

In cases where system integrates with data available in the public domain, it
is may be permissable to include a small sample of the data as a test fixture.
Ensure the license that the data is distributed under is compatible with your
code's license. If the license is compatible, I recommend including a reference
to the fixture, its source and license within a LICENSE.note file. This
practice is enforced by Comprehensive R Archive Network. You can read more about
this in the
[R Packages documentation](https://r-pkgs.org/license.html#sec-how-to-include).

### Scoping Fixtures

Pytest fixtures have different scopes, meaning that they will be prepared
differently dependent on the scope you specify. The avalable scopes are as
follows. 

| Scope Name  | Teardown after each |
| ----------- | ------------------- |
| function    | test function       |
| class       | test class          |
| module      | test module         |
| session     | pytest session      |

Note that the default scope for any fixtures that you define will be
'function'. A function-scoped fixture will be set up for every test function
that requires it. Once the function has executed, the fixture will then be
torn down and all changes to this fixture will be lost. This default behaviour
encourages isolation in your test suite. Meaning that the tests have no
dependencies upon each other. The test functions could be run in any order
without affecting the results of the test. Function-scoped fixtures are the
shortest-lived fixtures. Moving down the table, the persistence of the fixtures
increases, so that changes to a session-scoped fixture persist for the entire
test execution direction, only being torn down once pytest has executed all
tests.

#### Scoping for Performance
***

> performance vs isolation

By definition, a unit test is completely isolated. However, there may be a few
cases where this would be less desirable. Slow test suites may introduce
excessive friction to the software development process. Persistent fixtures can
be used to improve the performance of a test suite. 

For example, here we define some expensive class:

```{.python filename=expensive.py}
"""A module containing an expensive class definition."""
import time
from typing import Union


class ExpensiveDoodah:
    """A toy class that represents some costly operation.

    This class will sleep for the specified number of seconds on instantiation.

    Parameters
    ----------
    sleep_time : Union[int, float]
        Number of seconds to wait on init.

    """
    def __init__(self, sleep_time: Union[int, float] = 2):
        print(f"Sleeping for {sleep_time} seconds")
        time.sleep(sleep_time)
        return None

```

This class will be used to demonstrate the effect of scoping with some costly
operation. This example could represent reading in a bulky xlsx file, for
instance.

To test `ExpensiveDoodah`, I will define a function-scoped fixture. To do this,
we use pytest's fixture decorator to return the class instance with a specified
sleep time of 2 seconds.

```{.python filename=test_expensive.py}
import pytest

from example_pkg.expensive import ExpensiveDoodah


@pytest.fixture(scope="function")
def module_doodah():
    """Function-scoped ExpensiveDoodah."""
    return ExpensiveDoodah(2)

```
Now we extend our test module to include a test class with 3 separate test
functions, all making assertions about the `ExpensiveDoodah`. The assertions
will all be the same for this simple example, that `ExpensiveDoodah` executes
without raising any error conditions. Notice we must pass the name of the
fixture in each test function's signature.

```{.python filename=test_expensive.py}
"""Tests for expensive.py using function-scoped fixture."""
from contextlib import nullcontext as does_not_raise
import pytest

from example_pkg.expensive import ExpensiveDoodah


@pytest.fixture(scope="function")
def doodah_fixture():
    """Function-scoped ExpensiveDoodah."""
    return ExpensiveDoodah(2)


class TestA:
    """A test class."""

    def test_1(self, doodah_fixture):
        """Test 1."""
        with does_not_raise():
            doodah_fixture

    def test_2(self, doodah_fixture):
        """Test 2."""
        with does_not_raise():
            doodah_fixture

    def test_3(self, doodah_fixture):
        """Test 3."""
        with does_not_raise():
            doodah_fixture

```

The result of running this test module can be seen below:

```
collected 3 items

./tests/test_expensive_function_scoped.py ...    [100%]

============================ 3 passed in 6.04s ================================

```

Notice that the test module took just over 6 seconds to execute because the
function-scoped fixture was set up once for each test function.

If instead we had defined `doodah_fixture` with a different scope, it would
reduce the time for the test suite to complete.

```{.python filename=test_expensive.py}
@pytest.fixture(scope="module")
def doodah_fixture():
    """Module-scoped ExpensiveDoodah."""
    return ExpensiveDoodah(2)

```

```
collected 3 items

./tests/test_expensive_function_scoped.py ...    [100%]

============================ 3 passed in 2.02s ================================

```

The scoping feature of pytest fixtures can be used to optimise a test-suite and
avoid lengthy delays while waiting for your test suites to execute. However,
any changes to the fixture contents will persist until the fixture is next torn
down. Keeping track of the states of differently-scoped fixtures in a complex
test suite can be tricky and reduces segmentation overall. Bear this in mind
when considering which scope best suits your needs.

#### Scope Persistence
***

> function < class < module < session

Using scopes other than 'function' can be useful for end-to-end testing.
Perhaps you have a complex analytical pipeline and need to check that the
various components work well **together**, rather than in isolation as with a
unit test. This sort of test can be extremely useful for developers in a rush.
You can test that the so called 'promise' of the codebase is as expected, even
though the implementation may change.

The analogy here would be that the success criteria of a SatNav is that it gets
you to your desired destination whatever the suggested route you selected.
Checking that you used the fastest or most fuel efficient option is probably a
good idea. But if you don't have time, you'll just have to take the hit if you
encounter a toll road. It's still worth checking that the postcode you hastily
input to the satnav is the correct one.

<iframe src="https://www.google.com/maps/embed?pb=!1m26!1m12!1m3!1d19867.3453412218!2d-0.12357612910538017!3d51.50554381133014!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!4m11!3e0!4m3!3m2!1d51.5020874!2d-0.0776174!4m5!1s0x48760520cd5b5eb5%3A0xa26abf514d902a7!2sBuckingham%20Palace%2C%20London!3m2!1d51.501363999999995!2d-0.14189!5e0!3m2!1sen!2suk!4v1711782045831!5m2!1sen!2suk" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade" class="shaded-box" title="Google Maps displaying multiple routes to Buckingham Palace. (c) Google." style="display: block; width: 600px; height: 338px">
</iframe>

Perhaps your success criteria is that you need to write a dataframe to file. 
A great end-to-end test would check that the dataframe produced has the
expected number of rows, or even has rows! Of course it's also a good idea to
check the dataframe conforms to the expected table schema, too: number of
columns, names of columns, order and data types. This sort of check is often
overlooked in favour of pressing on with development. If you've ever
encountered a situation where you've updated a codebase and later realised you
now have empty tables (I certainly have), this sort of test would be really
handy, immediately alerting you to this fact and helping you efficiently locate
the source of the bug.

In this part, I will explore the scoping of fixtures with dataframes. Again,
I'll use a toy example to demonstrate scope behaviour. Being a child of the
'90s (mostly), I'll use a scenario from my childhood. Scooby Doo is still a
thing, right?

##### **Enter: The Mystery Machine**

<img src=/./www/11-fiddly-bits-of-pytest/mm.jpg alt="Scooby Doo & the gang in the Mystery Machine" class=center>
</img>

The scenario: The passengers of the Mystery Machine van all have the munchies.
They stop at a 'drive thru' to get some takeaway. We have a table with a record
for each character. We have data about the characters' names, their favourite
food, whether they have 'the munchies', and the contents of their stomach.

```{python}
import pandas as pd
mystery_machine = pd.DataFrame(
        {
            "name": ["Daphne", "Fred", "Scooby Doo", "Shaggy", "Velma"],
            "fave_food": [
                "carrots",
                "beans",
                "scooby snacks",
                "burgers",
                "hot dogs",
            ],
            "has_munchies": [True] * 5, # everyone's hungry
            "stomach_contents": ["empty"] * 5, # all have empty stomachs
        }
    )
mystery_machine
```

To use this simple dataframe as a fixture, I could go ahead and define it with
`@pytest.fixture()` directly within a test file. But if I would like to share
it across several test modules, then there are 2 options:

1. Write the dataframe to disk as csv (or whatever format you prefer) and save
in the `./tests/data/` folder. At the start of your test modules you can read
the data from disk and use it for testing. In this approach you'll likely
define the data as a test fixture in each of the test modules that need to work
with it.
2. Define the fixtures within a special python file called `conftest.py`, which
must be located at the root of your project. This file is used to configure
your tests. `pytest` will look in this file for any required fixture
definitions when executing your test suite. If it finds a fixture with the same
name as that required by a test, the fixture code may be run. 

:::{.callout-caution}
Wait! Did you just say '**may** be run'?
:::

Depending on the scope of your fixture, `pytest` may not need to execute the
code for each test. If you've defined a session-scoped fixture, it's going to
persist for the duration of the entire test suite execution. Imagine test
number 1 and 10 both require the same test fixture. The fixture definition only
gets executed the first time a test requires it.

##### **Define Fixtures**

For our example, we will create a `conftest.py` file and define some fixtures
with differing scopes.

```{.python filename=conftest.py}
"""Demonstrate scoping fixtures."""
import pandas as pd
import pytest


@pytest.fixture(scope="session")
def _mystery_machine():
    """Session-scoped fixture returning pandas dataframe."""
    return pd.DataFrame(
        {
            "name": ["Daphne", "Fred", "Scooby Doo", "Shaggy", "Velma"],
            "fave_food": [
                "carrots",
                "beans",
                "scooby snacks",
                "burgers",
                "hot dogs",
            ],
            "has_munchies": [True] * 5,
            "stomach_contents": ["empty"] * 5,
        }
    )


@pytest.fixture(scope="session")
def _mm_session_scoped(_mystery_machine):
    """Session-scoped fixture returning the _mystery_machine dataframe."""
    return _mystery_machine.copy(deep=True)


@pytest.fixture(scope="module")
def _mm_module_scoped(_mystery_machine):
    """Module-scoped _mystery_machine dataframe."""
    return _mystery_machine.copy(deep=True)


@pytest.fixture(scope="class")
def _mm_class_scoped(_mystery_machine):
    """Class-scoped _mystery_machine dataframe."""
    return _mystery_machine.copy(deep=True)


@pytest.fixture(scope="function")
def _mm_function_scoped(_mystery_machine):
    """Function-scoped _mystery_machine dataframe."""
    return _mystery_machine.copy(deep=True)

```

Fixtures can reference each other, if they're scoped correctly. More on this in
[the next section](#troubleshooting-fixture-scopes). This is useful for my toy
example as I intend the source functions to update the dataframes directly, if
I wasn't careful about deep copying the fixtures, my functions would update the
original `_mystery_machine` fixture's table. Those changes would then be
subsequently passed to the other fixtures, meaning I couldn't clearly
demonstrate how the different scopes persist.

##### Define the Source Functions



### Troubleshooting Fixture Scopes

## Temporary Files & Directories

## Parametrized Tests

## Marks

## Mocking


<p id=fin><i>fin!</i></p>
