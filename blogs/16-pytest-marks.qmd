---
title: "Custom Marks With Pytest in Plain English"
author: "Rich Leyshon"
date: "July 21 2024"
description: "Selectively running tests with marks"
categories:
    - Explanation
    - pytest
    - Unit tests
    - marks
    - custom marks
    - markers
    - pytest-in-plain-english
image: "https://i.imgur.com/dCJBh9w.jpeg"
image-alt: "Planet composed of croissants"
toc: true
jupyter: 
  kernelspec:
    name: "conda-env-mocking-env-py"
    language: "python"
    display_name: "blog-mocking-env"
css: /www/13-pytest-parametrize/styles.css
---

<figure class=center >
  <img class="shaded_box" width=400px src="https://i.imgur.com/dCJBh9w.jpeg"></img>
  <figcaption>Planet composed of croissants.</figcaption>
</figure>

> "The only flake I want is that of a croissant." Mariel Feldman.

## Introduction

`pytest` is a testing package for the python framework. It is broadly used to
quality assure code logic. This article discusses custom marks, use cases and
patterns for selecting and deselecting marked tests. This blog is the fifth and
final in a series of blogs called
[pytest in plain English](/blogs/index.qmd#category=pytest-in-plain-english),
favouring accessible language and simple examples to explain the more intricate
features of the `pytest` package.

For a wealth of documentation, guides and how-tos, please consult the
<a href="https://docs.pytest.org/en/8.0.x/" target="_blank">`pytest` documentation</a>.

### What are `pytest` Custom Marks?

Marks are a way to conditionally run specified tests. There are a few marks
that come with the `pytest` package. To view these, run `pytest --markers` from
the command line. This will print a list of the pre-registered marks available
within the `pytest` package. However, it is extremely easy to register your own
markers, allowing greater control over which tests get executed.

This article will cover:

* Reasons for marking tests
* Registering marks with `pytest`
* Marking tests
* Including or excluding markers from the command line

:::{.callout collapse="true"}

### A Note on the Purpose (Click to expand)

This article intends to discuss clearly. It doesn't aim to be clever or
impressive. Its aim is to extend understanding without overwhelming the reader.
The code may not always be optimal, favouring a simplistic approach wherever
possible.

:::

### Intended Audience

Programmers with a working knowledge of python and some familiarity with
`pytest` and packaging. The type of programmer who has wondered about how to
follow best practice in testing python code.

### What You'll Need:

- [ ] Preferred python environment manager (eg `conda`)
- [ ] `pip install pytest==8.1.1`
- [ ] Git
- [ ] GitHub account
- [ ] Command line access

### Preparation

This blog is accompanied by code in
[this repository](https://github.com/r-leyshon/pytest-fiddly-examples). The
main branch provides a template with the minimum structure and requirements
expected to run a `pytest` suite. The repo branches contain the code used in
the examples of the following sections.

Feel free to fork or clone the repo and checkout to the example branches as
needed.

The example code that accompanies this article is available in the
[marks branch](https://github.com/r-leyshon/pytest-fiddly-examples/tree/marks)
of the repo.

## Overview

Occasionally, we write tests that are a bit distinct to the rest of our test
suite. They could be integration tests, calling on elements of our code from
multiple modules. They could be end to end tests, executing a pipeline from
start to finish. Or they could be a flaky or brittle sort of test, a test that
is prone to failure on specific operating systems, architectures or when
external dependencies do not provide reliable inputs. 

There are multiple ways to handle these kinds of tests, including mocking, as
discussed in [my previous blog](/blogs/15-pytest-mocking.qmd). Mocking can
often take a bit of time, and developers don't always have that precious
commodity. So instead, they may mark the test as 'temperemental', ensuring that
it doesn't get run on continuous integration (CI) checks when merging changes
to the main branch. This would involve flagging the flaky test as
"technical debt" to be resolved later.

In fact, there are a number of reasons that we may want to selectively run
elements of a test suite. Here is a selection of scenarios that could benefit
from marking.

:::{.callout-note collapse="true"}

### Flaky Tests: Common Causes

| **Category**                | **Cause**                   | **Explanation**                                                         |
|-----------------------------|-----------------------------|-------------------------------------------------------------------------|
| External Dependencies       | Network                     | Network latency, outages, or Domain Name System (DNS) issues.           |
|                             | Web APIs                    | Breaking changes, rate limits, or outages.                              |
|                             | Databases                   | Concurrency issues, data changes, or connection problems.               |
|                             | Timeouts                    | Hardcoded or too-short timeouts cause failures.                         |
| Environment Dependencies    | Environment Variables       | Incorrectly set environment variables.                                  |
|                             | File System                 | File locks, permissions, or missing files.                              |
|                             | Resource Limits             | Insufficient CPU, memory, or disk space.                                |
| State Dependencies          | Shared State                | Interference between tests sharing state.                               |
|                             | Order Dependency            | Tests relying on execution order.                                       |
| Test Data                   | Random Data                 | Different results on each run due to random data and seed not set.      |
| Concurrency Issues          | Parallel Execution          | Tests not designed for parallel execution.                              |
|                             | Locks                       | Deadlocks or timeouts involving locks or semaphores.                    |
|                             | Race Conditions             | Tests depend on the order of execution of threads or processes.         |
|                             | Async Operations            | Improperly awaited asynchronous code.                                   |
| Hardware and System Issues  | Differences in Hardware     | Variations in performance across hardware or operating systems.         |
|                             | System Load                 | Failures under high system load due to resource contention.             |
| Non-deterministic Inputs    | Time                        | Variations in current time affecting test results.                      |
|                             | User Input                  | Non-deterministic user input causing flaky behavior.                    |
|                             | Filepaths                   | CI runner filepaths may be hard to predict.                             |
| Test Implementation Issues  | Assertions                  | Incorrect or overly strict assertions.                                  |
|                             | Setup and Teardown          | Inconsistent state due to improper setup or teardown.                   |
: {.light .hover .responsive-md}

:::

One approach may be to group all integration tests together and have them
execute within a dedicated CI workflow. This is common practice as developers
may want to stay alert to problems with external resources that their code
depends upon, while not failing 'core' CI checks about changes to the source
code. If your code relies on a web API; for instance; you're probably less
concerned about temporary outages in that service. However, a breaking change
to that service would require our source code to be adapted. Once more, life is
a compromise.

> "*Le mieux est l'ennemi du bien.*" (The best is the enemy of the good),
Voltaire

Marking allows us to have greater control over which of our tests are executed
when we invoke `pytest`. Conveniently, this is done in the following way
(presuming you have already written your source and test code):

1. Register a custom marker
2. Assign the new marker name to the target test
3. Invoke `pytest` with the `-m` (MARKEXPR) flag.

## Custom Marks in `pytest`

...

### Define the Source Code

...

### Let's Get Testing

...


## Summary

...

If you spot an error with this article, or have a suggested improvement then
feel free to
[raise an issue on GitHub](https://github.com/r-leyshon/blogging/issues).  

Happy testing!

## Acknowledgements

To past and present colleagues who have helped to discuss pros and cons,
establishing practice and firming-up some opinions. Particularly:

* Ethan
* Sergio

<p id=fin><i>fin!</i></p>
